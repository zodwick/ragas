{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\u2728 Introduction","text":"<p>Ragas dev  is a library that provides tools to supercharge the evaluation of Large Language Model (LLM) applications. It is designed to help you evaluate your LLM applications with ease and confidence. </p> <ul> <li> <p>\ud83d\ude80 Get Started</p> <p>Install with <code>pip</code> and get started with Ragas with these tutorials.</p> <p> Get Started</p> </li> <li> <p>\ud83d\udcda Core Concepts</p> <p>In depth explanation and discussion of the concepts and working of different features available in Ragas.</p> <p> Core Concepts</p> </li> <li> <p>\ud83d\udee0\ufe0f How-to Guides</p> <p>Practical guides to help you achieve a specific goals. Take a look at these guides to learn how to use Ragas to solve real-world problems.</p> <p> How-to Guides</p> </li> <li> <p>\ud83d\udcd6 References</p> <p>Technical descriptions of how Ragas classes and methods work.</p> <p> References</p> </li> </ul>"},{"location":"community/","title":"\u2764\ufe0f Community","text":"<p>\"Alone we can do so little; together we can do so much.\" - Helen Keller</p> <p>Our project thrives on the vibrant energy, diverse skills, and shared passion of our community. It's not just about code; it's about people coming together to create something extraordinary. This space celebrates every contribution, big or small, and features the amazing people who make it all happen.</p>"},{"location":"community/#contributors","title":"\ud83c\udf1f\u00a0 Contributors","text":"<p>Meet some of our outstanding members who made significant contributions !</p> <ul> <li>Tino Max Thayil</li> <li>Da Chen</li> <li>Yongtae Hwang</li> </ul>"},{"location":"community/#blog-insights","title":"\ud83d\udcda Blog &amp; Insights","text":"<p>Explore insightful articles, tutorials, and stories written by and for our community members.</p> <ul> <li>Shanthi Vardhan shares how his team at Atomicwork uses ragas to improve their AI system's ability to accurately identify and retrieve more precise information for enhanced service management.</li> <li>Pinecone's study on how RAGs can enhance capabilities of LLMs in \"RAG makes LLMs better and equal\" uses ragas to proves context retrieval makes LLMs provide significantly better results, even when increasing the data size to 1 billion.</li> <li>Aishwarya Prabhat shares her expertise on advanced RAG techniques in her comprehensive guide, \"Performing, Evaluating &amp; Tracking Advanced RAG (ft. AzureML, LlamaIndex &amp; Ragas)\".</li> <li>Leonie (aka @helloiamleonie)  offers her perspective in the detailed article, \"Evaluating RAG Applications with RAGAs\".</li> <li>The joint efforts of Erika Cardenas and Connor Shorten are showcased in their collaborative piece, \"An Overview on RAG Evaluation | Weaviate\", and their podcast with the Ragas team.</li> <li>Erika Cardenas further explores the \"RAG performance of hybrid search weightings (alpha)\" in her recent experiment to tune weaviate alpha score using Ragas.</li> <li>Langchain\u2019s work about RAG Evaluating RAG pipelines with RAGAs and Langsmith provides a complete tutorial on how to leverage both tools to evaluate RAG pipelines.</li> <li>Plaban Nayak shares his work Evaluate RAG Pipeline using RAGAS on building and evaluating a simple RAG using Langchain and RAGAS</li> <li>Stephen Kurniawan compares different RAG elements such as Chunk Size, Vector Stores: FAISS vs ChromaDB, Vector Stores 2: Multiple Documents, and Similarity Searches / Distance Metrics / Index Strategies. </li> <li>Discover Devanshu Brahmbhatt's insights on optimizing RAG systems in his article, Enhancing LLM's Accuracy with RAGAS. Learn about RAG architecture, key evaluation metrics, and how to use RAGAS scores to improve performance.</li> <li>Suzuki and Hwang conducted an experiment to investigate if Ragas' performance is language-dependent by comparing the performance (correlation coefficient between human labels and scores from Ragas) using datasets of the same content in Japanese and English. They wrote blog about the result of the experiment and basic algorithm of Ragas.<ul> <li>RAG Evaluation: Necessity and Challenge</li> <li>RAG Evaluation : Computational Metrics in RAG and Calculation Methods in Ragas</li> <li>RAG Evaluation: Assessing the Usefulness of Ragas</li> </ul> </li> <li>Atita Arora writes about Evaluating Retrieval Augmented Generation using RAGAS, an end-to-end tutorial on building RAG using Qdrant and Langchain and evaluating it with RAGAS. </li> <li>Bonus content : Learn how to create an evaluation dataset that serves as a reference point for evaluating our RAG pipeline, Understand the RAGAS evaluation metrics and how to make sense of them and putting them in action to test a Naive RAG pipeline and measure its performance using RAGAS metrics. </li> <li>Code walkthrough : https://github.com/qdrant/qdrant-rag-eval/tree/master/workshop-rag-eval-qdrant-ragas</li> <li>Code walkthrough using Deepset Haystack and Mixedbread.ai : https://github.com/qdrant/qdrant-rag-eval/tree/master/workshop-rag-eval-qdrant-ragas-haystack</li> </ul>"},{"location":"community/#events","title":"\ud83d\udcc5\u00a0Events","text":"<p>Stay updated with our latest gatherings, meetups, and online webinars.</p> <ul> <li>OpenAI Engineers shares their RAG tricks and features Ragas on DevDay.</li> <li>Langchain\u2019s a LangChain \"RAG Evaluation\u201d Webinar with the Ragas team</li> </ul>"},{"location":"concepts/","title":"\ud83d\udcda Core Concepts","text":"<ul> <li> <p> Components Guides</p> <p>Discover the various components used within Ragas.</p> <p>Components like Prompt Object, Evaluation Dataset and more..</p> </li> <li> <p>: Ragas Metrics</p> <p>Explore available metrics and understand how they work.</p> <p>Metrics for evaluating RAG, Agentic workflows and more...</p> </li> <li> <p> Test Data Generation</p> <p>Generate high-quality datasets for comprehensive testing.</p> <p>Algorithms for synthesizing data to test RAG, Agentic workflows </p> </li> <li> <p> Feedback Intelligence</p> <p>Leverage signals from production data to gain actionable insights.</p> <p>Learn about to leveraging implicit and explicit signals from production data.</p> </li> </ul>"},{"location":"concepts/prompt_adaptation/","title":"Automatic prompt Adaptation","text":"<p>All the prompts used in ragas are natively written fully in English language and hence ragas natively may not work as expected when using with languages other than English. Automatic prompt adaptation is built to overcome this problem.</p>"},{"location":"concepts/prompt_adaptation/#how-this-is-made-possible","title":"How this is made possible?","text":"<p>Note</p> <p>If you're unfamilar with Prompt object in ragas refer Prompt Object</p> <p>Each prompt in Ragas contains instructions and demonstrations. Through research and experiments, we found that by providing demonstrations in the target language can help LLMs adapt easily to any given target language. Leveraging this key insight we carefully translate all the relevant parts of the demonstrations into the target language. This is done using an LLM and once translated, the prompt can be saved to disk for reuse later.</p>"},{"location":"concepts/prompt_adaptation/#example","title":"Example","text":"<p>Native prompt <pre><code>Extract the noun from given sentence\n\nsentence: \"The sun sets over the mountains.\"\nnouns: [\"sun\", \"mountains\"]\n\nsentence: {sentence}\nnouns:\n</code></pre> Prompt adapted to Hindi <pre><code>Extract the noun from the given sentence\n\nsentence: \"\u0938\u0942\u0930\u091c \u092a\u0939\u093e\u0921\u093c\u094b\u0902 \u0915\u0947 \u092a\u093e\u0930 \u0921\u0942\u092c\u0924\u093e \u0939\u0948\u0964\"\nnouns: [\"\u0938\u0942\u0930\u091c\", \"\u092a\u0939\u093e\u0921\u093c\"]\n\nsentence: {sentence}\nnouns:\n</code></pre></p>"},{"location":"concepts/prompt_adaptation/#api-details","title":"API Details","text":"<p>Create a sample prompt using <code>Prompt</code> class.</p> <pre><code>from ragas.llms.prompt import Prompt\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom ragas.llms.base import LangchainLLMWrapper\n\nopenai_model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n\nopenai_model = LangchainLLMWrapper(openai_model)\nnoun_extractor = Prompt(\n    name=\"noun_extractor\",\n    instruction=\"Extract the noun from given sentence\",\n    examples=[{\n        \"sentence\":\"The sun sets over the mountains.\",\n        \"output\":{\"nouns\":[\"sun\", \"mountains\"]}\n    }],\n    input_keys=[\"sentence\"],\n    output_key=\"output\",\n    output_type=\"json\"\n)\n</code></pre> <p>Prompt adaption is done using the <code>.adapt</code> method:</p> <p><code>adapt(self, language, llm, cache_dir)</code></p> <p>The adapt method takes in a target language, LLM and adapts the prompts to the given target language. In case the adapted prompt is already present in <code>cache_dir</code> it is loaded.</p> <pre><code>adapted_prompt = noun_extractor.adapt(language=\"hindi\",llm=openai_model)\nprint(adapted_prompt.to_string())\n</code></pre> <pre><code>{'nouns': ['\u0938\u0942\u0930\u094d\u092f', '\u092a\u0939\u093e\u0921\u093c']}\nExtract the noun from given sentence\nOutput in only valid JSON format.\n\nsentence: \"\u0938\u0942\u0930\u091c \u092a\u0939\u093e\u0921\u093c\u094b\u0902 \u092a\u0930 \u0905\u0938\u094d\u0924 \u0939\u094b\u0924\u093e \u0939\u0948\u0964\"\noutput: {{\"nouns\": [\"\u0938\u0942\u0930\u094d\u092f\", \"\u092a\u0939\u093e\u0921\u093c\"]}}\n\nsentence: {sentence}\noutput: \n</code></pre> <p>The quality of the adapted prompt depends on the quality of LLM, so we advise you to use the best you have with prompt adaptation.</p>"},{"location":"concepts/prompts/","title":"Prompt Objects","text":"<p>Prompts play a crucial role in any language model-based framework and warrant more consideration than mere strings. A well-crafted prompt should include a clear task instruction, articulated in straightforward natural language, comprehensible to any language model. The objective is to compose prompts that are generalizable and do not overly specialize to a specific state of the language model. It's widely recognized that language models exhibit higher accuracy in few-shot scenarios as opposed to zero-shot contexts. To capitalize on this advantage, it is advisable to accompany each prompt with relevant examples.</p> <p>Prompts in ragas are defined using the <code>Prompt</code> class. Each prompt defined using this class will contain.</p> <ul> <li><code>name</code>: a name given to the prompt. Used to save and identify the prompt.</li> <li><code>instruction</code>: The natural language description of the task to be carried out by the LLM</li> <li><code>examples</code>: List one or more demonstrations of the task at hand. Using demonstrations converts the task from zero-shot to few-shot which can improve accuracy in most cases.</li> <li><code>input_keys</code>:  List of one or more variable names that are used to identify the inputs provided to the LLM.</li> <li><code>output_key</code>: Variable name that is used to identify the output</li> <li><code>output_type</code>: Output type of the prompt. Can be JSON or string.</li> <li><code>language</code>: the language in which demonstrations are written. The default is English.</li> </ul> <p>Let\u2019s create a simple prompt using <code>Prompt</code></p> <pre><code>from ragas.llms.prompt import Prompt\n\nqa_prompt = Prompt(\n    name=\"question_generation\",\n    instruction=\"Generate a question for the given answer\",\n    examples=[\n        {\n            \"answer\": \"The last Olympics was held in Tokyo, Japan.\",\n            \"context\": \"The last Olympics was held in Tokyo, Japan. It is held every 4 years\",\n            \"output\": {\"question\":\"Where was the last Olympics held?\"},\n        },\n        {\n            \"answer\": \"It can change its skin color based on the temperature of its environment.\",\n            \"context\": \"A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.\",\n            \"output\": {\"question\":\"What unique ability does the newly discovered species of frog have?\"},\n        }\n    ],\n    input_keys=[\"answer\", \"context\"],\n    output_key=\"output\",\n    output_type=\"json\",\n)\n</code></pre> <p>This will create a Prompt class object with the given instruction, examples, and keys. The <code>output_type</code> is given as JSON here which will process the example values as JSON strings. This object when created will undergo validations to check if the prompt class criteria are met.</p> <ul> <li><code>instruction</code> is mandatory and cannot be an empty string.</li> <li><code>input_keys</code> and <code>output_key</code> are mandatory fields. Multiple <code>input_keys</code> can be used but a single <code>output_key</code> is accepted.</li> <li><code>examples</code> are optional but if provided should contain the input_key and output_keys in the example keys. The example values should match the output_type (dict or json or str).</li> </ul> <p>Prompt objects have the following methods that can be used when evaluating or formatting a prompt object.</p> <ul> <li> <p><code>to_string(self)</code></p> <p>This method will generate a prompt string from the given object. This string can be directly used as a formatted string with the metrics in the evaluation task.</p> <pre><code>print(qa_prompt.to_string())\n</code></pre> <pre><code>Generate a question for the given answer\n\nanswer: \"The last Olympics was held in Tokyo, Japan.\"\ncontext: \"The last Olympics was held in Tokyo, Japan. It is held every 4 years\"\noutput: {{\"question\": \"Where was the last Olympics held?\"}}\n\nanswer: \"It can change its skin color based on the temperature of its environment.\"\ncontext: \"A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.\"\noutput: {{\"question\": \"What unique ability does the newly discovered species of frog have?\"}}\n\nanswer: {answer}\ncontext: {context}\noutput:\n</code></pre> </li> <li> <p><code>format(self, **kwargs)</code></p> <p>This method will use the parameters passed as keyword arguments to format the prompt object and return a Langchain <code>PromptValue</code> object that can be directly used in the evaluation tasks.</p> <pre><code>qa_prompt.format(answer=\"This is an answer\", context=\"This is a context\")\n</code></pre> <pre><code>PromptValue(prompt_str='Generate a question for the given answer\\n\\nanswer: \"The last Olympics was held in Tokyo, Japan.\"\\ncontext: \"The last Olympics was held in Tokyo, Japan. It is held every 4 years\"\\noutput: {\"question\": \"Where was the last Olympics held?\"}\\n\\nanswer: \"It can change its skin color based on the temperature of its environment.\"\\ncontext: \"A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.\"\\noutput: {\"question\": \"What unique ability does the newly discovered species of frog have?\"}\\n\\nanswer: This is an answer\\ncontext: This is a context\\noutput: \\n')\n</code></pre> </li> <li> <p><code>save(self, cache_dir)</code></p> <p>This method will save the prompt to the given cache_dir (default <code>~/.cache</code>) directory using the value in the <code>name</code> variable.</p> <pre><code>qa_prompt.save()\n</code></pre> <p>The prompts are saved in JSON format to <code>~/.cache/ragas</code> by default. One can change this by setting the <code>RAGAS_CACHE_HOME</code> environment variable to the desired path. In this example,  the prompt will be saved in <code>~/.cache/ragas/english/question_generation.json</code></p> </li> <li> <p><code>_load(self, language, name, cache_dir)</code></p> <p>This method will load the appropriate prompt from the saved directory.</p> <pre><code>from ragas.utils import RAGAS_CACHE_HOME\nPrompt._load(name=\"question_generation\",language=\"english\",cache_dir=RAGAS_CACHE_HOME)\n</code></pre> <pre><code>Prompt(name='question_generation', instruction='Generate a question for the given answer', examples=[{'answer': 'The last Olympics was held in Tokyo, Japan.', 'context': 'The last Olympics was held in Tokyo, Japan. It is held every 4 years', 'output': {'question': 'Where was the last Olympics held?'}}, {'answer': 'It can change its skin color based on the temperature of its environment.', 'context': 'A recent scientific study has discovered a new species of frog in the Amazon rainforest that has the unique ability to change its skin color based on the temperature of its environment.', 'output': {'question': 'What unique ability does the newly discovered species of frog have?'}}], input_keys=['answer', 'context'], output_key='output', output_type='JSON')\n</code></pre> <p>The prompt was loaded from <code>.cache/ragas/english/question_generation.json</code></p> </li> </ul>"},{"location":"concepts/testset_generation/","title":"Synthetic Test Data generation","text":""},{"location":"concepts/testset_generation/#why-synthetic-test-data","title":"Why synthetic test data?","text":"<p>Evaluating RAG (Retrieval-Augmented Generation) augmented pipelines is crucial for assessing their performance. However, manually creating hundreds of QA (Question-Context-Answer) samples from documents can be time-consuming and labor-intensive. Additionally, human-generated questions may struggle to reach the level of complexity required for a thorough evaluation, ultimately impacting the quality of the assessment. By using synthetic data generation developer time in data aggregation process can be reduced by 90%. </p>"},{"location":"concepts/testset_generation/#how-does-ragas-differ-in-test-data-generation","title":"How does Ragas differ in test data generation?","text":"<p>Ragas takes a novel approach to evaluation data generation. An ideal evaluation dataset should encompass various types of questions encountered in production, including questions of varying difficulty levels. LLMs by default are not good at creating diverse samples as it tends to follow common paths. Inspired by works like Evol-Instruct, Ragas achieves this by employing an evolutionary generation paradigm, where questions with different characteristics such as reasoning, conditioning, multi-context, and more are systematically crafted from the provided set of documents. This approach ensures comprehensive coverage of the performance of various components within your pipeline, resulting in a more robust evaluation process.</p> Component-wise Evaluation"},{"location":"concepts/testset_generation/#in-depth-evolution","title":"In-Depth Evolution","text":"<p>Large Language Models (LLMs) possess the capability to transform simple questions into more complex ones effectively. To generate medium to hard samples from the provided documents, we employ the following methods:</p> <ul> <li> <p>Reasoning: Rewrite the question in a way that enhances the need for reasoning to answer it effectively.</p> </li> <li> <p>Conditioning: Modify the question to introduce a conditional element, which adds complexity to the question.</p> </li> <li> <p>Multi-Context: Rephrase the question in a manner that necessitates information from multiple related sections or chunks to formulate an answer.</p> </li> </ul> <p>Moreover, our paradigm extends its capabilities to create conversational questions from the given documents:</p> <ul> <li>Conversational: A portion of the questions, following the evolution process, can be transformed into conversational samples. These questions simulate a chat-based question-and-follow-up interaction, mimicking a chat-Q&amp;A pipeline.</li> </ul> <p>Note</p> <p>Moving forward, we will be expanding the range of evolution techniques to offer even more diverse evaluation possibilities</p>"},{"location":"concepts/testset_generation/#example","title":"Example","text":"<p><pre><code>from langchain_community.document_loaders import PubMedLoader\n\nloader = PubMedLoader(\"liver\", load_max_docs=10)\ndocuments = loader.load()\n</code></pre> Checkout langchain document loaders to see more examples</p> <p><pre><code>from llama_index import download_loader\n\nSemanticScholarReader = download_loader(\"SemanticScholarReader\")\nloader = SemanticScholarReader()\nquery_space = \"large language models\"\ndocuments = loader.load_data(query=query_space,full_text=True,limit=10)\n</code></pre> Checkout llama-index document loaders to see more examples</p> <pre><code>from ragas.testset.generator import TestsetGenerator\nfrom ragas.testset.evolutions import simple, reasoning, multi_context\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\n# documents = load your documents\n\n# generator with openai models\ngenerator_llm = ChatOpenAI(model=\"gpt-4o-mini\")\ncritic_llm = ChatOpenAI(model=\"gpt-4o\")\nembeddings = OpenAIEmbeddings()\n\ngenerator = TestsetGenerator.from_langchain(\n    generator_llm,\n    critic_llm,\n    embeddings\n)\n\n# Change resulting question type distribution\ndistributions = {\n    simple: 0.5,\n    multi_context: 0.4,\n    reasoning: 0.1\n}\n\n# use generator.generate_with_llamaindex_docs if you use llama-index as document loader\ntestset = generator.generate_with_langchain_docs(documents, 10, distributions) \ntestset.to_pandas()\n</code></pre> <pre><code>test_df = testset.to_pandas()\ntest_df.head()\n</code></pre> Testset Output"},{"location":"concepts/testset_generation/#analyze-question-types","title":"Analyze question types","text":"<p>Analyze the frequency of different question types in the created dataset</p> Testset Output"},{"location":"concepts/components/","title":"Components Guide","text":"<p>This guide provides an overview of the different components used inside Ragas.</p> <ul> <li>Prompt Object</li> <li>Evaluation Sample</li> <li>Evaluation Dataset</li> </ul>"},{"location":"concepts/components/eval_dataset/","title":"Evaluation Dataset","text":"<p>An evaluation dataset is a homogeneous collection of data samples designed to assess the performance and capabilities of an AI application. In Ragas, evaluation datasets are represented using the <code>EvaluationDataset</code> class, which provides a structured way to organize and manage data samples for evaluation purposes. </p>"},{"location":"concepts/components/eval_dataset/#structure-of-an-evaluation-dataset","title":"Structure of an Evaluation Dataset","text":"<p>An evaluation dataset consists of:</p> <ul> <li>Samples: A collection of SingleTurnSample or MultiTurnSample instances. Each sample represents a unique interaction or scenario.</li> <li>Consistency: All samples within the dataset should be of the same type (either all single-turn or all multi-turn samples) to maintain consistency in evaluation.</li> </ul>"},{"location":"concepts/components/eval_dataset/#guidelines-for-curating-an-effective-evaluation-dataset","title":"Guidelines for Curating an Effective Evaluation Dataset","text":"<ul> <li> <p>Define Clear Objectives: Identify the specific aspects of the AI application that you want to evaluate and the scenarios you want to test. Collect data samples that reflect these objectives.</p> </li> <li> <p>Collect Representative Data: Ensure that the dataset covers a diverse range of scenarios, user inputs, and expected responses to provide a comprehensive evaluation of the AI application. This can be achieved by collecting data from various sources or generating synthetic data.</p> </li> <li> <p>Quality and Size: Aim for a dataset that is large enough to provide meaningful insights but not so large that it becomes unwieldy. Ensure that the data is of high quality and accurately reflects the real-world scenarios you want to evaluate.</p> </li> </ul>"},{"location":"concepts/components/eval_dataset/#example","title":"Example","text":"<p>In this example, we\u2019ll demonstrate how to create an EvaluationDataset using multiple <code>SingleTurnSample</code> instances. We\u2019ll walk through the process step by step, including creating individual samples, assembling them into a dataset, and performing basic operations on the dataset.</p> <p>Step 1: Import Necessary Classes</p> <p>First, import the SingleTurnSample and EvaluationDataset classes from your module. <pre><code>from ragas import SingleTurnSample, EvaluationDataset\n</code></pre></p> <p>Step 2: Create Individual Samples</p> <p>Create several SingleTurnSample instances that represent individual evaluation samples.</p> <pre><code># Sample 1\nsample1 = SingleTurnSample(\n    user_input=\"What is the capital of Germany?\",\n    retrieved_contexts=[\"Berlin is the capital and largest city of Germany.\"],\n    response=\"The capital of Germany is Berlin.\",\n    reference=\"Berlin\",\n)\n\n# Sample 2\nsample2 = SingleTurnSample(\n    user_input=\"Who wrote 'Pride and Prejudice'?\",\n    retrieved_contexts=[\"'Pride and Prejudice' is a novel by Jane Austen.\"],\n    response=\"'Pride and Prejudice' was written by Jane Austen.\",\n    reference=\"Jane Austen\",\n)\n\n# Sample 3\nsample3 = SingleTurnSample(\n    user_input=\"What's the chemical formula for water?\",\n    retrieved_contexts=[\"Water has the chemical formula H2O.\"],\n    response=\"The chemical formula for water is H2O.\",\n    reference=\"H2O\",\n)\n</code></pre> <p>Step 3: Create the EvaluationDataset Create an EvaluationDataset by passing a list of SingleTurnSample instances.</p> <pre><code>dataset = EvaluationDataset(samples=[sample1, sample2, sample3])\n</code></pre> <p>EvaluationDataset API Reference</p>"},{"location":"concepts/components/eval_sample/","title":"Evaluation Sample","text":"<p>An evaluation sample is a single structured data instance that is used to asses and measure the performance of your LLM application in specific scenarios. It represents a single unit of interaction or a specific use case that the AI application is expected to handle. In Ragas, evaluation samples are represented using the <code>SingleTurnSample</code> and <code>MultiTurnSample</code> classes.</p>"},{"location":"concepts/components/eval_sample/#singleturnsample","title":"SingleTurnSample","text":"<p>SingleTurnSample represents a single-turn interaction between a user, LLM, and expected results for evaluation. It is suitable for evaluations that involve a single question and answer pair, possibly with additional context or reference information.</p> <p>SingleTurnSample API Reference</p>"},{"location":"concepts/components/eval_sample/#example","title":"Example","text":"<p>The following example demonstrates how to create a <code>SingleTurnSample</code> instance for evaluating a single-turn interaction in a RAG-based application. In this scenario, a user asks a question, and the AI provides an answer. We\u2019ll create a SingleTurnSample instance to represent this interaction, including any retrieved contexts, reference answers, and evaluation rubrics. <pre><code>from ragas import SingleTurnSample\n\n# User's question\nuser_input = \"What is the capital of France?\"\n\n# Retrieved contexts (e.g., from a knowledge base or search engine)\nretrieved_contexts = [\"Paris is the capital and most populous city of France.\"]\n\n# AI's response\nresponse = \"The capital of France is Paris.\"\n\n# Reference answer (ground truth)\nreference = \"Paris\"\n\n# Evaluation rubric\nrubric = {\n    \"accuracy\": \"Correct\",\n    \"completeness\": \"High\",\n    \"fluency\": \"Excellent\"\n}\n\n# Create the SingleTurnSample instance\nsample = SingleTurnSample(\n    user_input=user_input,\n    retrieved_contexts=retrieved_contexts,\n    response=response,\n    reference=reference,\n    rubric=rubric\n)\n</code></pre></p>"},{"location":"concepts/components/eval_sample/#multiturnsample","title":"MultiTurnSample","text":"<p>MultiTurnSample represents a multi-turn interaction between Human, AI and optionally a Tool and expected results for evaluation. It is suitable for representing conversational agents in more complex interactions for evaluation. In <code>MultiTurnSample</code>, the <code>user_input</code> attribute represents a sequence of messages that collectively form a multi-turn conversation between a human user and an AI system. These messages are instances of the classes  HumanMessage, AIMessage, and ToolMessage</p> <p>MultTurnSample API Reference</p>"},{"location":"concepts/components/eval_sample/#example_1","title":"Example","text":"<p>The following example demonstrates how to create a <code>MultiTurnSample</code> instance for evaluating a multi-turn interaction. In this scenario, a user wants to know the current weather in New York City. The AI assistant will use a weather API tool to fetch the information and respond to the user.</p> <pre><code>from ragas.messages import HumanMessage, AIMessage, ToolMessage, ToolCall\n\n# User asks about the weather in New York City\nuser_message = HumanMessage(content=\"What's the weather like in New York City today?\")\n\n# AI decides to use a weather API tool to fetch the information\nai_initial_response = AIMessage(\n    content=\"Let me check the current weather in New York City for you.\",\n    tool_calls=[ToolCall(name=\"WeatherAPI\", args={\"location\": \"New York City\"})]\n)\n\n# Tool provides the weather information\ntool_response = ToolMessage(content=\"It's sunny with a temperature of 75\u00b0F in New York City.\")\n\n# AI delivers the final response to the user\nai_final_response = AIMessage(content=\"It's sunny and 75 degrees Fahrenheit in New York City today.\")\n\n# Combine all messages into a list to represent the conversation\nconversation = [\n    user_message,\n    ai_initial_response,\n    tool_response,\n    ai_final_response\n]\n</code></pre> <p>Now, use the conversation to create a MultiTurnSample object, including any reference responses and evaluation rubrics. <pre><code>from ragas import MultiTurnSample\n# Reference response for evaluation purposes\nreference_response = \"Provide the current weather in New York City to the user.\"\n\n\n# Create the MultiTurnSample instance\nsample = MultiTurnSample(\n    user_input=conversation,\n    reference=reference_response,\n)\n</code></pre></p>"},{"location":"concepts/components/prompt/","title":"Prompt Object","text":"<p>Prompts in Ragas are used inside various metrics and synthetic data generation tasks. In each of these tasks, Ragas also provides a way for the user to modify or replace the default prompt with a custom prompt. This guide provides an overview of the Prompt Object in Ragas. </p>"},{"location":"concepts/components/prompt/#components-of-a-prompt-object","title":"Components of a Prompt Object","text":"<p>In Ragas, a prompt object is composed of the following key components:</p> <ol> <li> <p>Instruction: A fundamental element of any prompt, the instruction is a natural language directive that clearly describes the task the Language Model (LLM) should perform. This is specified using the <code>instruction</code> variable within the prompt object.</p> </li> <li> <p>Few-Shot Examples: LLMs are known to perform better when provided with few-shot examples, as they help the model understand the task context and generate more accurate responses. These examples are specified using the <code>examples</code> variable in the prompt object. Each example consists of an input and its corresponding output, which the LLM uses to learn the task.</p> </li> <li> <p>Input Model: Every prompt expects an input to produce an output. In Ragas, the expected format of this input is defined using the <code>input_model</code> variable. This is a Pydantic model that outlines the structure of the input, enabling validation and parsing of the data provided to the prompt.</p> </li> <li> <p>Output Model: Upon execution, a prompt generates an output. The format of this output is specified using the <code>output_model</code> variable in the prompt object. Like the input model, the output model is a Pydantic model that defines the structure of the output, facilitating validation and parsing of the data produced by the LLM.</p> </li> </ol>"},{"location":"concepts/components/prompt/#example","title":"Example","text":"<p>Here's an example of a prompt object that defines a prompt for a text generation task:</p> <pre><code>from ragas.experimental.prompt import PydanticPrompt\nfrom pydantic import BaseModel, Field\n\nclass MyInput(BaseModel):\n    question: str = Field(description=\"The question to answer\")\n\nclass MyOutput(BaseModel):\n    answer: str = Field(description=\"The answer to the question\")\n\nclass MyPrompt(PydanticPrompt[MyInput,MyInput]):\n    instruction = \"Answer the given question\"\n    input_model = MyInput\n    output_model = MyOutput\n    examples = [\n        (\n            MyInput(question=\"Who's building the opensource standard for LLM app evals?\"),\n            MyOutput(answer=\"Ragas\")\n        )\n    ]\n</code></pre> <p>Prompt Object API Reference</p>"},{"location":"concepts/components/prompt/#guidelines-for-creating-effective-prompts","title":"Guidelines for Creating Effective Prompts","text":"<p>When creating prompts in Ragas, consider the following guidelines to ensure that your prompts are effective and aligned with the task requirements:</p> <ol> <li>Clear and Concise Instructions: Provide clear and concise instructions that clearly define the task the LLM should perform. Ambiguity in instructions can lead to inaccurate responses.</li> <li>Relevant Few-Shot Examples: Include relevant few-shot examples that cover a diverse range of scenarios related to the task (ideally 3-5). These examples help the LLM understand the context and generate accurate responses.</li> <li>Simple Input and Output Models: Define simple and intuitive input and output models that accurately represent the data format expected by the LLM and the output generated by the LLM. If the models are complex, try to break the task into smaller sub-tasks with separate prompts.</li> </ol>"},{"location":"concepts/feedback/","title":"Utilizing User Feedback","text":"<p>User feedback can often be noisy and challenging to harness effectively. However, within the feedback, valuable signals exist that can be leveraged to iteratively enhance your LLM and RAG applications. These signals have the potential to be amplified effectively, aiding in the detection of specific issues within the pipeline and preventing recurring errors. Ragas is equipped to assist you in the analysis of user feedback data, enabling the discovery of patterns and making it a valuable resource for continual improvement.</p> <p>Talk to founders to add this to your LLM app building cycle.</p>"},{"location":"concepts/metrics/","title":"Metrics","text":"<ul> <li>Overview Learn more about overview and design principles</li> <li> Available Metrics Learn about available metrics and their inner workings</li> </ul>"},{"location":"concepts/metrics/available_metrics/","title":"List of available metrics","text":"<p>Ragas provides a set of evaluation metrics that can be used to measure the performance of your LLM application. These metrics are designed to help you objectively measure the performance of your application. Metrics are available for different applications and tasks, such as RAG and Agentic workflows. </p> <p>Each metric are essentially paradigms that is designed to evaluate a particular aspect of the application. LLM Based metrics might use one or more LLM calls to arrive at the score or result. One can also modify or write your own metrics using ragas.</p>"},{"location":"concepts/metrics/available_metrics/#retrieval-augmented-generation","title":"Retrieval Augmented Generation","text":"<ul> <li>Context Precision</li> <li>Context Recall</li> <li>Context Entities Recall</li> <li>Noise Sensitivity</li> <li>Response Relevancy</li> <li>Faithfulness</li> </ul>"},{"location":"concepts/metrics/available_metrics/#agents-or-tool-use-cases","title":"Agents or Tool use cases","text":"<ul> <li>Topic adherence</li> <li>Tool call Accuracy</li> <li>Agent Goal Accuracy</li> </ul>"},{"location":"concepts/metrics/available_metrics/#natural-language-comparison","title":"Natural Language Comparison","text":"<ul> <li>Factual Correctness</li> <li>Semantic Similarity</li> <li>Non LLM String Similarity</li> <li>BLEU Score</li> <li>ROUGE Score</li> <li>String Presence</li> <li>Exact Match</li> </ul>"},{"location":"concepts/metrics/available_metrics/#sql","title":"SQL","text":"<ul> <li>Execution based Datacompy Score</li> <li>SQL query Equivalence</li> </ul>"},{"location":"concepts/metrics/available_metrics/#general-purpose","title":"General purpose","text":"<ul> <li>Aspect critic</li> <li>Simple Criteria Scoring</li> <li>Rubrics based scoring</li> <li>Instance specific rubrics scoring</li> </ul>"},{"location":"concepts/metrics/available_metrics/#other-tasks","title":"Other tasks","text":"<ul> <li>Summarization</li> </ul>"},{"location":"concepts/metrics/available_metrics/agents/","title":"Agentic or Tool use","text":"<p>Agentic or tool use workflows can be evaluated in multiple dimensions. Here are some of the metrics that can be used to evaluate the performance of agents or tools in a given task.</p>"},{"location":"concepts/metrics/available_metrics/agents/#tool-call-accuracy","title":"Tool call Accuracy","text":"<p>Tool call accuracy is a metric that can be used to evaluate the performance of the LLM in identifying and calling the required tools to complete a given task. This metric needs <code>user_input</code> and <code>reference_tool_calls</code> to evaluate the performance of the LLM in identifying and calling the required tools to complete a given task. The metric is computed by comparing the <code>reference_tool_calls</code> with the Tool calls made by the AI. The values range between 0 and 1, with higher values indicating better performance. </p> <pre><code>from ragas.dataset_schema import  MultiTurnSample\nfrom ragas.messages import HumanMessage,AIMessage,ToolMessage,ToolCall\nfrom ragas.metrics._tool_call_accuracy import ToolCallAccuracy\n\n\nsample = [\n    HumanMessage(content=\"What's the weather like in New York right now?\"),\n    AIMessage(content=\"The current temperature in New York is 75\u00b0F and it's partly cloudy.\", tool_calls=[\n        ToolCall(name=\"weather_check\", args={\"location\": \"New York\"})\n    ]),\n    HumanMessage(content=\"Can you translate that to Celsius?\"),\n    AIMessage(content=\"Let me convert that to Celsius for you.\", tool_calls=[\n        ToolCall(name=\"temperature_conversion\", args={\"temperature_fahrenheit\": 75})\n    ]),\n    ToolMessage(content=\"75\u00b0F is approximately 23.9\u00b0C.\"),\n    AIMessage(content=\"75\u00b0F is approximately 23.9\u00b0C.\")\n]\n\nsampl2 = MultiTurnSample(\n    user_input=sample,\n    reference_tool_calls=[\n        ToolCall(name=\"weather_check\", args={\"location\": \"New York\"}),\n        ToolCall(name=\"temperature_conversion\", args={\"temperature_fahrenheit\": 75})\n    ]\n)\n\nscorer = ToolCallAccuracy()\nawait metric.multi_turn_ascore(sample)\n</code></pre> <p>The tool call sequence specified in <code>reference_tool_calls</code> is used as the ideal outcome. If the tool calls made by the AI does not the the order or sequence of the <code>reference_tool_calls</code>, the metric will return a score of 0. This helps to ensure that the AI is able to identify and call the required tools in the correct order to complete a given task.</p> <p>By default the tool names and arguments are compared using exact string matching. But sometimes this might not be optimal, for example if the args are natural language strings. You can also use any ragas metrics (values between 0 and 1) as distance measure to identify if a retrieved context is relevant or not. For example,</p> <pre><code>from ragas.metrics._string import NonLLMStringSimilarity\nfrom ragas.metrics._tool_call_accuracy import ToolCallAccuracy\n\nmetric = ToolCallAccuracy()\nmetric.arg_comparison_metric = NonLLMStringSimilarity()\n</code></pre>"},{"location":"concepts/metrics/available_metrics/agents/#agent-goal-accuracy","title":"Agent Goal accuracy","text":"<p>Agent goal accuracy is a metric that can be used to evaluate the performance of the LLM in identifying and achieving the goals of the user. This is a binary metric, with 1 indicating that the AI has achieved the goal and 0 indicating that the AI has not achieved the goal.</p>"},{"location":"concepts/metrics/available_metrics/agents/#with-reference","title":"With reference","text":"<p>Calculating agent goal accuracy with reference needs <code>user_input</code> and <code>reference</code> to evaluate the performance of the LLM in identifying and achieving the goals of the user. The annotated <code>reference</code> will be used as ideal outcome. The metric is computed by comparing the <code>reference</code> with the goal achieved by the end of workflow.</p> <pre><code>from ragas.dataset_schema import  MultiTurnSample\nfrom ragas.messages import HumanMessage,AIMessage,ToolMessage,ToolCall\nfrom ragas.metrics._agent_goal_accuracy import AgentGoalAccuracyWithReference\n\n\nsample = MultiTurnSample(user_input=[\n    HumanMessage(content=\"Hey, book a table at the nearest best Chinese restaurant for 8:00pm\"),\n    AIMessage(content=\"Sure, let me find the best options for you.\", tool_calls=[\n        ToolCall(name=\"restaurant_search\", args={\"cuisine\": \"Chinese\", \"time\": \"8:00pm\"})\n    ]),\n    ToolMessage(content=\"Found a few options: 1. Golden Dragon, 2. Jade Palace\"),\n    AIMessage(content=\"I found some great options: Golden Dragon and Jade Palace. Which one would you prefer?\"),\n    HumanMessage(content=\"Let's go with Golden Dragon.\"),\n    AIMessage(content=\"Great choice! I'll book a table for 8:00pm at Golden Dragon.\", tool_calls=[\n        ToolCall(name=\"restaurant_book\", args={\"name\": \"Golden Dragon\", \"time\": \"8:00pm\"})\n    ]),\n    ToolMessage(content=\"Table booked at Golden Dragon for 8:00pm.\"),\n    AIMessage(content=\"Your table at Golden Dragon is booked for 8:00pm. Enjoy your meal!\"),\n    HumanMessage(content=\"thanks\"),\n],\n    reference=\"Table booked at one of the chinese restaurants at 8 pm\")\n\nscorer = AgentGoalAccuracyWithReference()\nawait metric.multi_turn_ascore(sample)\n</code></pre>"},{"location":"concepts/metrics/available_metrics/agents/#without-reference","title":"Without reference","text":"<p>In without reference mode, the metric will evaluate the performance of the LLM in identifying and achieving the goals of the user without any reference. Here the desired outcome is inferred from the human interactions in the workflow.</p>"},{"location":"concepts/metrics/available_metrics/agents/#example","title":"Example","text":"<pre><code>from ragas.dataset_schema import  MultiTurnSample\nfrom ragas.messages import HumanMessage,AIMessage,ToolMessage,ToolCall\nfrom ragas.metrics._agent_goal_accuracy import AgentGoalAccuracyWithoutReference\n\n\nsample = MultiTurnSample(user_input=[\n    HumanMessage(content=\"Hey, book a table at the nearest best Chinese restaurant for 8:00pm\"),\n    AIMessage(content=\"Sure, let me find the best options for you.\", tool_calls=[\n        ToolCall(name=\"restaurant_search\", args={\"cuisine\": \"Chinese\", \"time\": \"8:00pm\"})\n    ]),\n    ToolMessage(content=\"Found a few options: 1. Golden Dragon, 2. Jade Palace\"),\n    AIMessage(content=\"I found some great options: Golden Dragon and Jade Palace. Which one would you prefer?\"),\n    HumanMessage(content=\"Let's go with Golden Dragon.\"),\n    AIMessage(content=\"Great choice! I'll book a table for 8:00pm at Golden Dragon.\", tool_calls=[\n        ToolCall(name=\"restaurant_book\", args={\"name\": \"Golden Dragon\", \"time\": \"8:00pm\"})\n    ]),\n    ToolMessage(content=\"Table booked at Golden Dragon for 8:00pm.\"),\n    AIMessage(content=\"Your table at Golden Dragon is booked for 8:00pm. Enjoy your meal!\"),\n    HumanMessage(content=\"thanks\"),\n])\n\nscorer = AgentGoalAccuracyWithoutReference()\nawait metric.multi_turn_ascore(sample)\n</code></pre>"},{"location":"concepts/metrics/available_metrics/answer_correctness/","title":"Answer correctness","text":""},{"location":"concepts/metrics/available_metrics/answer_correctness/#answer-correctness","title":"Answer Correctness","text":"<p>The assessment of Answer Correctness involves gauging the accuracy of the generated answer when compared to the ground truth. This evaluation relies on the <code>ground truth</code> and the <code>answer</code>, with scores ranging from 0 to 1. A higher score indicates a closer alignment between the generated answer and the ground truth, signifying better correctness.</p> <p>Answer correctness encompasses two critical aspects: semantic similarity between the generated answer and the ground truth, as well as factual similarity. These aspects are combined using a weighted scheme to formulate the answer correctness score. Users also have the option to employ a 'threshold' value to round the resulting score to binary, if desired.</p> <p>Example</p> <p>Ground truth: Einstein was born in 1879 in Germany.</p> <p>High answer correctness: In 1879, Einstein was born in Germany.</p> <p>Low answer correctness: Einstein was born in Spain in 1879.</p>"},{"location":"concepts/metrics/available_metrics/answer_correctness/#example","title":"Example","text":"<pre><code>from datasets import Dataset \nfrom ragas.metrics import answer_correctness\nfrom ragas import evaluate\n\ndata_samples = {\n    'question': ['When was the first super bowl?', 'Who won the most super bowls?'],\n    'answer': ['The first superbowl was held on Jan 15, 1967', 'The most super bowls have been won by The New England Patriots'],\n    'ground_truth': ['The first superbowl was held on January 15, 1967', 'The New England Patriots have won the Super Bowl a record six times']\n}\ndataset = Dataset.from_dict(data_samples)\nscore = evaluate(dataset,metrics=[answer_correctness])\nscore.to_pandas()\n</code></pre>"},{"location":"concepts/metrics/available_metrics/answer_correctness/#calculation","title":"Calculation","text":"<p>Let's calculate the answer correctness for the answer with low answer correctness. It is computed as the sum of factual correctness and the semantic similarity between the given answer and the ground truth.</p> <p>Factual correctness quantifies the factual overlap between the generated answer and the ground truth answer. This is done using the concepts of: - TP (True Positive): Facts or statements that are present in both the ground truth and the generated answer. - FP (False Positive): Facts or statements that are present in the generated answer but not in the ground truth. - FN (False Negative): Facts or statements that are present in the ground truth but not in the generated answer.</p> <p>In the second example: - TP: <code>[Einstein was born in 1879]</code> - FP: <code>[Einstein was born in Spain]</code> - FN: <code>[Einstein was born in Germany]</code></p> <p>Now, we can use the formula for the F1 score to quantify correctness based on the number of statements in each of these lists:</p> \\[ \\text{F1 Score} = {|\\text{TP} \\over {(|\\text{TP}| + 0.5 \\times (|\\text{FP}| + |\\text{FN}|))}} \\] <p>Next, we calculate the semantic similarity between the generated answer and the ground truth. Read more about it here.</p> <p>Once we have the semantic similarity, we take a weighted average of the semantic similarity and the factual similarity calculated above to arrive at the final score. You can adjust this weightage by modifying the <code>weights</code> parameter.</p>"},{"location":"concepts/metrics/available_metrics/answer_relevance/","title":"Response Relevancy","text":""},{"location":"concepts/metrics/available_metrics/answer_relevance/#response-relevancy","title":"Response Relevancy","text":"<p>The evaluation metric, Answer Relevancy, focuses on assessing how pertinent the generated answer is to the given prompt. A lower score is assigned to answers that are incomplete or contain redundant information and higher scores indicate better relevancy. This metric is computed using the <code>user_input</code>, the <code>retrived_contexts</code> and the <code>response</code>. </p> <p>The Answer Relevancy is defined as the mean cosine similarity of the original <code>user_input</code> to a number of artificial questions, which where generated (reverse engineered) based on the <code>response</code>: </p> \\[ \\text{answer relevancy} = \\frac{1}{N} \\sum_{i=1}^{N} cos(E_{g_i}, E_o) \\] \\[ \\text{answer relevancy} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{E_{g_i} \\cdot E_o}{\\|E_{g_i}\\|\\|E_o\\|} \\] <p>Where: </p> <ul> <li>\\(E_{g_i}\\) is the embedding of the generated question \\(i\\).</li> <li>\\(E_o\\) is the embedding of the original question.</li> <li>\\(N\\) is the number of generated questions, which is 3 default.</li> </ul> <p>Please note, that eventhough in practice the score will range between 0 and 1 most of the time, this is not mathematically guaranteed, due to the nature of the cosine similarity ranging from -1 to 1.</p> <p>An answer is deemed relevant when it directly and appropriately addresses the original question. Importantly, our assessment of answer relevance does not consider factuality but instead penalizes cases where the answer lacks completeness or contains redundant details. To calculate this score, the LLM is prompted to generate an appropriate question for the generated answer multiple times, and the mean cosine similarity between these generated questions and the original question is measured. The underlying idea is that if the generated answer accurately addresses the initial question, the LLM should be able to generate questions from the answer that align with the original question.</p>"},{"location":"concepts/metrics/available_metrics/answer_relevance/#example","title":"Example","text":"<pre><code>from ragas import SingleTurnSample \nfrom ragas.metrics import ResponseRelevancy\n\nsample = SingleTurnSample(\n        user_input=\"When was the first super bowl?\",\n        response=\"The first superbowl was held on Jan 15, 1967\",\n        retrieved_contexts=[\n            \"The First AFL\u2013NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles.\"\n        ]\n    )\n\nscorer = ResponseRelevancy()\nawait scorer.single_turn_ascore(sample)\n</code></pre>"},{"location":"concepts/metrics/available_metrics/answer_relevance/#how-its-calculated","title":"How It\u2019s Calculated","text":"<p>Example</p> <p>Question: Where is France and what is it's capital?</p> <p>Low relevance answer: France is in western Europe.</p> <p>High relevance answer: France is in western Europe and Paris is its capital.</p> <p>To calculate the relevance of the answer to the given question, we follow two steps:</p> <ul> <li> <p>Step 1: Reverse-engineer 'n' variants of the question from the generated answer using a Large Language Model (LLM).    For instance, for the first answer, the LLM might generate the following possible questions:</p> <ul> <li>Question 1: \"In which part of Europe is France located?\"</li> <li>Question 2: \"What is the geographical location of France within Europe?\"</li> <li>Question 3: \"Can you identify the region of Europe where France is situated?\"</li> </ul> </li> <li> <p>Step 2: Calculate the mean cosine similarity between the generated questions and the actual question.</p> </li> </ul> <p>The underlying concept is that if the answer correctly addresses the question, it is highly probable that the original question can be reconstructed solely from the answer.</p>"},{"location":"concepts/metrics/available_metrics/aspect_critic/","title":"Aspect Critique","text":"<p>This is designed to assess submissions based on predefined aspects such as <code>harmlessness</code> and <code>correctness</code>. Additionally, users have the flexibility to define their own aspects for evaluating submissions according to their specific criteria. The output of aspect critiques is binary, indicating whether the submission aligns with the defined aspect or not. This evaluation is performed using the 'answer' as input.</p> <p>Critiques within the LLM evaluators evaluate submissions based on the provided aspect. Ragas Critiques offers a range of predefined aspects like correctness, harmfulness, etc. (Please refer to <code>SUPPORTED_ASPECTS</code> for a complete list). If you prefer, you can also create custom aspects to evaluate submissions according to your unique requirements.</p> <p>The <code>strictness</code> parameter plays a crucial role in maintaining a certain level of self-consistency in predictions, with an ideal range typically falling between 2 to 4.</p> <pre><code>SUPPORTED_ASPECTS = [\n    harmfulness,\n    maliciousness,\n    coherence,\n    correctness,\n    conciseness,\n]\n</code></pre>"},{"location":"concepts/metrics/available_metrics/aspect_critic/#example","title":"Example","text":"<pre><code>:caption: Answer critique\nfrom datasets import Dataset \nfrom ragas.metrics import AspectCritic\nfrom ragas import evaluate\n\ndata_samples = {\n    'question': ['When was the first super bowl?', 'Who won the most super bowls?'],\n    'answer': ['The first superbowl was held on Jan 15, 1967', 'The most super bowls have been won by The New England Patriots'],\n    'contexts' : [['The First AFL\u2013NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles,'], \n    ['The Green Bay Packers...Green Bay, Wisconsin.','The Packers compete...Football Conference']],\n}\ncritic = AspectCritic(\n    name=\"correctness\",\n    definition=\"Is the submission factually correct?\",\n)\ndataset = Dataset.from_dict(data_samples)\nscore = evaluate(dataset,metrics=[critic])\nscore.to_pandas()\n</code></pre>"},{"location":"concepts/metrics/available_metrics/aspect_critic/#calculation","title":"Calculation","text":"<p>Critics are essentially basic LLM calls using the defined criteria. For example, let's see how the harmfulness critic works:</p> <ul> <li> <p>Step 1: The definition of the critic prompts the LLM multiple times to verify if the answer contains anything harmful. This is done using a specific query.</p> <ul> <li>For harmfulness, the query is: \"Does the submission cause or have the potential to cause harm to individuals, groups, or society at large?\"</li> <li>Three different verdicts are collected using three LLM calls:<ul> <li>Verdict 1: Yes</li> <li>Verdict 2: No</li> <li>Verdict 3: Yes</li> </ul> </li> </ul> </li> <li> <p>Step 2: The majority vote from the returned verdicts determines the binary output.</p> <ul> <li>Output: Yes</li> </ul> </li> </ul>"},{"location":"concepts/metrics/available_metrics/context_entities_recall/","title":"Context Entities Recall","text":""},{"location":"concepts/metrics/available_metrics/context_entities_recall/#context-entities-recall","title":"Context Entities Recall","text":"<p>This metric gives the measure of recall of the retrieved context, based on the number of entities present in both <code>reference</code> and <code>retrieved_contexts</code> relative to the number of entities present in the <code>reference</code> alone. Simply put, it is a measure of what fraction of entities are recalled from <code>reference</code>. This metric is useful in fact-based use cases like tourism help desk, historical QA, etc. This metric can help evaluate the retrieval mechanism for entities, based on comparison with entities present in <code>reference</code>, because in cases where entities matter, we need the <code>retrieved_contexts</code> which cover them.</p> <p>To compute this metric, we use two sets, \\(GE\\) and \\(CE\\), as set of entities present in <code>reference</code> and set of entities present in <code>retrieved_contexts</code> respectively. We then take the number of elements in intersection of these sets and divide it by the number of elements present in the \\(GE\\), given by the formula:</p> \\[ \\text{context entity recall} = \\frac{| CE \\cap GE |}{| GE |} \\]"},{"location":"concepts/metrics/available_metrics/context_entities_recall/#example","title":"Example","text":"<pre><code>from ragas import SingleTurnSample\nfrom ragas.metrics import ContextEntityRecall\n\nsample = SingleTurnSample(\n    reference=\"The Eiffel Tower is located in Paris.\",\n    retrieved_contexts=[\"The Eiffel Tower is located in Paris.\"], \n)\n\nscorer = ContextEntityRecall()\n\nawait scorer.single_turn_ascore(sample)\n</code></pre>"},{"location":"concepts/metrics/available_metrics/context_entities_recall/#how-its-calculated","title":"How It\u2019s Calculated","text":"<p>Example</p> <p>reference: The Taj Mahal is an ivory-white marble mausoleum on the right bank of the river Yamuna in the Indian city of Agra. It was commissioned in 1631 by the Mughal emperor Shah Jahan to house the tomb of his favorite wife, Mumtaz Mahal. High entity recall context: The Taj Mahal is a symbol of love and architectural marvel located in Agra, India. It was built by the Mughal emperor Shah Jahan in memory of his beloved wife, Mumtaz Mahal. The structure is renowned for its intricate marble work and beautiful gardens surrounding it. Low entity recall context: The Taj Mahal is an iconic monument in India. It is a UNESCO World Heritage Site and attracts millions of visitors annually. The intricate carvings and stunning architecture make it a must-visit destination.</p> <p>Let us consider the ground truth and the contexts given above.</p> <ul> <li>Step-1: Find entities present in the ground truths.<ul> <li>Entities in ground truth (GE) - ['Taj Mahal', 'Yamuna', 'Agra', '1631', 'Shah Jahan', 'Mumtaz Mahal']</li> </ul> </li> <li>Step-2: Find entities present in the context.<ul> <li>Entities in context (CE1) - ['Taj Mahal', 'Agra', 'Shah Jahan', 'Mumtaz Mahal', 'India']</li> <li>Entities in context (CE2) - ['Taj Mahal', 'UNESCO', 'India']</li> </ul> </li> <li> <p>Step-3: Use the formula given above to calculate entity-recall</p> \\[ \\text{context entity recall 1} = \\frac{| CE1 \\cap GE |}{| GE |}                              = 4/6                              = 0.666 \\] \\[ \\text{context entity recall 2} = \\frac{| CE2 \\cap GE |}{| GE |}                              = 1/6 \\] <p>We can see that the first context had a high entity recall, because it has a better entity coverage given the ground truth. If these two contexts were fetched by two retrieval mechanisms on same set of documents, we could say that the first mechanism was better than the other in use-cases where entities are of importance.</p> </li> </ul>"},{"location":"concepts/metrics/available_metrics/context_precision/","title":"Context Precision","text":"<p>Context Precision is a metric that measures the proportion of relevant chunks in the <code>retrieved_contexts</code>. It is calculated as the mean of the precision@k for each chunk in the context. Precision@k is the ratio of the number of relevant chunks at rank k to the total number of chunks at rank k.</p> \\[ \\text{Context Precision@K} = \\frac{\\sum_{k=1}^{K} \\left( \\text{Precision@k} \\times v_k \\right)}{\\text{Total number of relevant items in the top } K \\text{ results}} \\] \\[ \\text{Precision@k} = {\\text{true positives@k} \\over  (\\text{true positives@k} + \\text{false positives@k})} \\] <p>Where \\(K\\) is the total number of chunks in <code>retrieved_contexts</code> and \\(v_k \\in \\{0, 1\\}\\) is the relevance indicator at rank \\(k\\).</p>"},{"location":"concepts/metrics/available_metrics/context_precision/#llm-based-context-precision","title":"LLM Based Context Precision","text":"<p>The following metrics uses LLM to identify if a retrieved context is relevant or not.</p>"},{"location":"concepts/metrics/available_metrics/context_precision/#context-precision-without-reference","title":"Context Precision without reference","text":"<p>This metric is can be used when you have both retrieved contexts and also reference contexts associated with a <code>user_input</code>. To estimate if a retrieved contexts is relevant or not this method uses the LLM to compare each of the retrieved context or chunk present in <code>retrieved_contexts</code> with <code>response</code>. </p>"},{"location":"concepts/metrics/available_metrics/context_precision/#example","title":"Example","text":"<pre><code>from ragas import SingleTurnSample\nfrom ragas.metrics import LLMContextPrecisionWithoutReference\n\ncontext_precision = LLMContextPrecisionWithoutReference()\n\nsample = SingleTurnSample(\n    user_input=\"Where is the Eiffel Tower located?\",\n    response=\"The Eiffel Tower is located in Paris.\",\n    retrieved_contexts=[\"The Eiffel Tower is located in Paris.\"], \n)\n\n\nawait context_precision.single_turn_ascore(sample)\n</code></pre>"},{"location":"concepts/metrics/available_metrics/context_precision/#context-precision-with-reference","title":"Context Precision with reference","text":"<p>This metric is can be used when you have both retrieved contexts and also reference answer associated with a <code>user_input</code>. To estimate if a retrieved contexts is relevant or not this method uses the LLM to compare each of the retrieved context or chunk present in <code>retrieved_contexts</code> with <code>reference</code>. </p>"},{"location":"concepts/metrics/available_metrics/context_precision/#example_1","title":"Example","text":"<pre><code>from ragas import SingleTurnSample\nfrom ragas.metrics import LLMContextPrecisionWithReference\n\ncontext_precision = LLMContextPrecisionWithReference()\n\nsample = SingleTurnSample(\n    user_input=\"Where is the Eiffel Tower located?\",\n    reference=\"The Eiffel Tower is located in Paris.\",\n    retrieved_contexts=[\"The Eiffel Tower is located in Paris.\"], \n)\n\nawait context_precision.single_turn_ascore(sample)\n</code></pre>"},{"location":"concepts/metrics/available_metrics/context_precision/#non-llm-based-context-precision","title":"Non LLM Based Context Precision","text":"<p>The following metrics uses traditional methods to identify if a retrieved context is relevant or not. You can use any non LLM based metrics as distance measure to identify if a retrieved context is relevant or not.</p>"},{"location":"concepts/metrics/available_metrics/context_precision/#context-precision-with-reference-contexts","title":"Context Precision with reference contexts","text":"<p>This metric is can be used when you have both retrieved contexts and also reference contexts associated with a <code>user_input</code>. To estimate if a retrieved contexts is relevant or not this method uses the LLM to compare each of the retrieved context or chunk present in <code>retrieved_contexts</code> with each ones present in <code>reference_contexts</code>. </p>"},{"location":"concepts/metrics/available_metrics/context_precision/#example_2","title":"Example","text":"<pre><code>from ragas import SingleTurnSample\nfrom ragas.metrics import NonLLMContextPrecisionWithReference\n\ncontext_precision = NonLLMContextPrecisionWithReference()\n\nsample = SingleTurnSample(\n    retrieved_contexts=[\"The Eiffel Tower is located in Paris.\"], \n    reference_contexts=[\"Paris is the capital of France.\", \"The Eiffel Tower is one of the most famous landmarks in Paris.\"]\n)\n\nawait context_precision.single_turn_ascore(sample)\n</code></pre>"},{"location":"concepts/metrics/available_metrics/context_recall/","title":"Context Recall","text":"<p>Context Recall measures how many of the relevant documents (or pieces of information) were successfully retrieved. It focuses on not missing important results. Higher recall means fewer relevant documents were left out. In short, recall is about not missing anything important. Since it is about not missing anything, calculating context recall always requires a reference to compare against.</p>"},{"location":"concepts/metrics/available_metrics/context_recall/#llm-based-context-recall","title":"LLM Based Context Recall","text":"<p>Computed using <code>user_input</code>, <code>reference</code> and the  <code>retrieved_contexts</code>, and the values range between 0 and 1, with higher values indicating better performance. This metric uses <code>reference</code> as a proxy to <code>reference_contexts</code> which also makes it easier to use as annotating reference contexts can be very time consuming. To estimate context recall from the <code>reference</code>, the reference is broken down into claims each claim in the <code>reference</code> answer is analyzed to determine whether it can be attributed to the retrieved context or not. In an ideal scenario, all claims in the reference answer should be attributable to the retrieved context.</p> <p>The formula for calculating context recall is as follows:</p> \\[ \\text{context recall} = {|\\text{GT claims that can be attributed to context}| \\over |\\text{Number of claims in GT}|} \\]"},{"location":"concepts/metrics/available_metrics/context_recall/#example","title":"Example","text":"<pre><code>from ragas.dataset_schema import SingleTurnSample\nfrom ragas.metrics import LLMContextRecall\n\nsample = SingleTurnSample(\n    user_input=\"Where is the Eiffel Tower located?\",\n    response=\"The Eiffel Tower is located in Paris.\",\n    reference=\"The Eiffel Tower is located in Paris.\",\n    retrieved_contexts=[\"Paris is the capital of France.\"], \n)\n\ncontext_recall = LLMContextRecall()\nawait context_recall.single_turn_ascore(sample)\n</code></pre>"},{"location":"concepts/metrics/available_metrics/context_recall/#non-llm-based-context-recall","title":"Non LLM Based Context Recall","text":"<p>Computed using <code>retrieved_contexts</code> and <code>reference_contexts</code>, and the values range between 0 and 1, with higher values indicating better performance. This metrics uses non llm string comparison metrics to identify if a retrieved context is relevant or not. You can use any non LLM based metrics as distance measure to identify if a retrieved context is relevant or not.</p> <p>The formula for calculating context recall is as follows:</p> \\[ \\text{context recall} = {|\\text{Number of relevant contexts retrieved}| \\over |\\text{Total number of reference contexts}|} \\]"},{"location":"concepts/metrics/available_metrics/context_recall/#example_1","title":"Example","text":"<pre><code>from ragas.dataset_schema import SingleTurnSample\nfrom ragas.metrics import NonLLMContextRecall\n\nsample = SingleTurnSample(\n    retrieved_contexts=[\"Paris is the capital of France.\"], \n    reference_contexts=[\"Paris is the capital of France.\", \"The Eiffel Tower is one of the most famous landmarks in Paris.\"]\n)\n\ncontext_recall = NonLLMContextRecall()\nawait context_recall.single_turn_ascore(sample)\n</code></pre>"},{"location":"concepts/metrics/available_metrics/factual_correctness/","title":"Factual Correctness","text":""},{"location":"concepts/metrics/available_metrics/factual_correctness/#factual-correctness","title":"Factual Correctness","text":"<p>Factual correctness is a metric that compares and evaluates the factual accuracy of the generated <code>response</code> with the <code>reference</code>. This metric is used to determine the extent to which the generated response aligns with the reference. The factual correctness score ranges from 0 to 1, with higher values indicating better performance. To measure the alignment between the response and the reference, the metric uses the LLM for first break down the response and reference into claims and then uses natural language inference to determine the factual overlap between the response and the reference. Factual overlap is quantified using precision, recall, and F1 score, which can be controlled using the <code>mode</code> parameter.</p> <p>The formula for calculating True Positive (TP), False Positive (FP), and False Negative (FN) is as follows:</p> \\[ \\text{True Positive (TP)} = \\text{Number of claims in response that are present in reference} \\] \\[ \\text{False Positive (FP)} = \\text{Number of claims in response that are not present in reference} \\] \\[ \\text{False Negative (FN)} = \\text{Number of claims in reference that are not present in response} \\] <p>The formula for calculating precision, recall, and F1 score is as follows:</p> \\[ \\text{Precision} = {TP \\over (TP + FP)} \\] \\[ \\text{Recall} = {TP \\over (TP + FN)} \\] \\[ \\text{F1 Score} = {2 \\times \\text{Precision} \\times \\text{Recall} \\over (\\text{Precision} + \\text{Recall})} \\]"},{"location":"concepts/metrics/available_metrics/factual_correctness/#example","title":"Example","text":"<pre><code>from ragas.dataset_schema import SingleTurnSample\nfrom ragas.metrics._factual_correctness import FactualCorrectness\n\n\nsample = SingleTurnSample(\n    response=\"The Eiffel Tower is located in Paris.\",\n    reference=\"The Eiffel Tower is located in Paris. I has a height of 1000ft.\"\n)\n\nscorer = FactualCorrectness()\nscorer.llm = openai_model\nawait scorer.single_turn_ascore(sample)\n</code></pre> <p>By default, the mode is set to <code>F1</code>, you can change the mode to <code>precision</code> or <code>recall</code> by setting the <code>mode</code> parameter.</p> <pre><code>scorer = FactualCorrectness(mode=\"precision\")\n</code></pre>"},{"location":"concepts/metrics/available_metrics/factual_correctness/#controlling-the-number-of-claims","title":"Controlling the Number of Claims","text":"<p>Each sentence in the response and reference can be broken down into one or more claims. The number of claims that are generated from a single sentence is determined by the level of <code>atomicity</code> and <code>coverage</code> required for your application.</p>"},{"location":"concepts/metrics/available_metrics/factual_correctness/#example_1","title":"Example","text":"<pre><code>scorer = FactualCorrectness(mode=\"precision\",atomicity=\"low\")\n</code></pre>"},{"location":"concepts/metrics/available_metrics/factual_correctness/#understanding-atomicity-and-coverage","title":"Understanding Atomicity and Coverage","text":"<p>In claim decomposition, two important parameters influence the output:</p> <ol> <li>Atomicity</li> <li>Coverage</li> </ol> <p>These parameters help control the granularity and completeness of the generated claims.</p>"},{"location":"concepts/metrics/available_metrics/factual_correctness/#atomicity","title":"Atomicity","text":"<p>Atomicity refers to how much a sentence is broken down into its smallest, meaningful components. It can be adjusted based on whether you need highly detailed claims or a more consolidated view.</p> <ul> <li>High Atomicity: The sentence is broken down into its fundamental, indivisible claims. This results in multiple, smaller claims, each representing a distinct piece of information.</li> </ul> <p>Example:   - Original Sentence:      - \"Albert Einstein was a German theoretical physicist who developed the theory of relativity and contributed to quantum mechanics.\"   - Decomposed Claims:     - \"Albert Einstein was a German theoretical physicist.\"     - \"Albert Einstein developed the theory of relativity.\"     - \"Albert Einstein contributed to quantum mechanics.\"</p> <ul> <li>Low Atomicity: The sentence is kept more intact, resulting in fewer claims that may contain multiple pieces of information.</li> </ul> <p>Example:   - Original Sentence:     - \"Albert Einstein was a German theoretical physicist who developed the theory of relativity and contributed to quantum mechanics.\"   - Decomposed Claims:     - \"Albert Einstein was a German theoretical physicist who developed the theory of relativity and contributed to quantum mechanics.\"</p>"},{"location":"concepts/metrics/available_metrics/factual_correctness/#coverage","title":"Coverage","text":"<p>Coverage refers to how comprehensively the claims represent the information in the original sentence. It can be adjusted to either include all details or to generalize the content.</p> <ul> <li>High Coverage: The decomposed claims capture all the information present in the original sentence, preserving every detail.</li> </ul> <p>Example:   - Original Sentence:      - \"Marie Curie was a Polish and naturalized-French physicist and chemist who conducted pioneering research on radioactivity.\"   - Decomposed Claims:     - \"Marie Curie was a Polish physicist.\"     - \"Marie Curie was a naturalized-French physicist.\"     - \"Marie Curie was a chemist.\"     - \"Marie Curie conducted pioneering research on radioactivity.\"</p> <ul> <li>Low Coverage: The decomposed claims cover only the main points, omitting some details to provide a more generalized view.</li> </ul> <p>Example:   - Original Sentence:     - \"Marie Curie was a Polish and naturalized-French physicist and chemist who conducted pioneering research on radioactivity.\"   - Decomposed Claims:     - \"Marie Curie was a physicist.\"     - \"Marie Curie conducted research on radioactivity.\"</p>"},{"location":"concepts/metrics/available_metrics/factual_correctness/#combining-atomicity-and-coverage","title":"Combining Atomicity and Coverage","text":"<p>By adjusting both atomicity and coverage, you can customize the level of detail and completeness to meet the needs of your specific use case.</p> <ul> <li>High Atomicity &amp; High Coverage: Produces highly detailed and comprehensive claims that cover all aspects of the original sentence.</li> </ul> <p>Example:   - Original Sentence:     - \"Charles Babbage was an English mathematician, philosopher, inventor, and mechanical engineer.\"   - Decomposed Claims:     - \"Charles Babbage was an English mathematician.\"     - \"Charles Babbage was a philosopher.\"     - \"Charles Babbage was an inventor.\"     - \"Charles Babbage was a mechanical engineer.\"</p> <ul> <li>Low Atomicity &amp; Low Coverage: Produces fewer claims with less detail, summarizing the main idea without going into specifics.</li> </ul> <p>Example:   - Original Sentence:     - \"Charles Babbage was an English mathematician, philosopher, inventor, and mechanical engineer.\"   - Decomposed Claims:     - \"Charles Babbage was an English mathematician.\"     - \"Charles Babbage was an inventor.\"</p>"},{"location":"concepts/metrics/available_metrics/factual_correctness/#practical-application","title":"Practical Application","text":"<ul> <li>Use High Atomicity and High Coverage when you need a detailed and comprehensive breakdown for in-depth analysis or information extraction.</li> <li>Use Low Atomicity and Low Coverage when only the key information is necessary, such as for summarization.</li> </ul> <p>This flexibility in controlling the number of claims helps ensure that the information is presented at the right level of granularity for your application's requirements.</p>"},{"location":"concepts/metrics/available_metrics/faithfulness/","title":"Faithfulness","text":""},{"location":"concepts/metrics/available_metrics/faithfulness/#faithfulness","title":"Faithfulness","text":"<p>This measures the factual consistency of the generated answer against the given context. It is calculated from answer and retrieved context. The answer is scaled to (0,1) range. Higher the better.</p> <p>The generated answer is regarded as faithful if all the claims made in the answer can be inferred from the given context. To calculate this, a set of claims from the generated answer is first identified. Then each of these claims is cross-checked with the given context to determine if it can be inferred from the context. The faithfulness score is given by:</p> \\[ \\text{Faithfulness score} = {|\\text{Number of claims in the generated answer that can be inferred from given context}| \\over |\\text{Total number of claims in the generated answer}|} \\]"},{"location":"concepts/metrics/available_metrics/faithfulness/#example","title":"Example","text":"<pre><code>from ragas.database_schema import SingleTurnSample \nfrom ragas.metrics import Faithfulness\n\nsample = SingleTurnSample(\n        user_input=\"When was the first super bowl?\",\n        response=\"The first superbowl was held on Jan 15, 1967\",\n        retrieved_contexts=[\n            \"The First AFL\u2013NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles.\"\n        ]\n    )\nscorer = Faithfulness()\nawait scorer.single_turn_ascore(sample)\n</code></pre>"},{"location":"concepts/metrics/available_metrics/faithfulness/#faithfullness-with-hhem-21-open","title":"Faithfullness with HHEM-2.1-Open","text":"<p>Vectara's HHEM-2.1-Open is a classifier model (T5) that is trained to detect hallucinations from LLM generated text. This model can be used in the second step of calculating faithfulness, i.e. when claims are cross-checked with the given context to determine if it can be inferred from the context. The model is free, small, and open-source, making it very efficient in production use cases. To use the model to calculate faithfulness, you can use the following code snippet:</p> <pre><code>from ragas.database_schema import SingleTurnSample \nfrom ragas.metrics import FaithfulnesswithHHEM\n\n\nsample = SingleTurnSample(\n        user_input=\"When was the first super bowl?\",\n        response=\"The first superbowl was held on Jan 15, 1967\",\n        retrieved_contexts=[\n            \"The First AFL\u2013NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles.\"\n        ]\n    )\nscorer = FaithfulnesswithHHEM()\nawait scorer.single_turn_ascore(sample)\n</code></pre> <p>You can load the model onto a specified device by setting the <code>device</code> argument and adjust the batch size for inference using the <code>batch_size</code> parameter. By default, the model is loaded on the CPU with a batch size of 10</p> <pre><code>my_device = \"cuda:0\"\nmy_batch_size = 10\n\nscorer = FaithfulnesswithHHEM(device=my_device, batch_size=my_batch_size)\nawait scorer.single_turn_ascore(sample)\n</code></pre>"},{"location":"concepts/metrics/available_metrics/faithfulness/#how-its-calculated","title":"How It\u2019s Calculated","text":"<p>Example</p> <p>Question: Where and when was Einstein born?</p> <p>Context: Albert Einstein (born 14 March 1879) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time</p> <p>High faithfulness answer: Einstein was born in Germany on 14th March 1879.</p> <p>Low faithfulness answer:  Einstein was born in Germany on 20th March 1879.</p> <p>Let's examine how faithfulness was calculated using the low faithfulness answer:</p> <ul> <li> <p>Step 1: Break the generated answer into individual statements.</p> <ul> <li>Statements:<ul> <li>Statement 1: \"Einstein was born in Germany.\"</li> <li>Statement 2: \"Einstein was born on 20th March 1879.\"</li> </ul> </li> </ul> </li> <li> <p>Step 2: For each of the generated statements, verify if it can be inferred from the given context.</p> <ul> <li>Statement 1: Yes</li> <li>Statement 2: No</li> </ul> </li> <li> <p>Step 3: Use the formula depicted above to calculate faithfulness.</p> \\[ \\text{Faithfulness} = { \\text{1} \\over \\text{2} } = 0.5 \\] </li> </ul>"},{"location":"concepts/metrics/available_metrics/general_purpose/","title":"General Purpose Metrics","text":"<p>General purpose evaluation metrics are used to evaluate any given task. </p>"},{"location":"concepts/metrics/available_metrics/general_purpose/#aspect-critic","title":"Aspect Critic","text":"<p>Aspect critic is an evaluation metric that can be used to evaluate responses based on predefined aspects in free form natural language. The output of aspect critiques is binary, indicating whether the submission aligns with the defined aspect or not. </p>"},{"location":"concepts/metrics/available_metrics/general_purpose/#example","title":"Example","text":"<pre><code>from ragas.dataset_schema import SingleTurnSample\nfrom ragas.metrics import AspectCritic\n\nsample = SingleTurnSample(\n    user_input=\"Where is the Eiffel Tower located?\",\n    response=\"The Eiffel Tower is located in Paris.\",\n    reference=\"The Eiffel Tower is located in Paris.\",\n)\n\ncritic =  AspectCritic(\n    name=\"maliciousness\",\n    definition=\"Is the submission intended to harm, deceive, or exploit users?\",\n)\nawait critic.single_turn_ascore(sample)\n</code></pre>"},{"location":"concepts/metrics/available_metrics/general_purpose/#how-it-works","title":"How it works","text":"<p>Critics are essentially basic LLM calls using the defined criteria. For example, let's see how the harmfulness critic works:</p> <ul> <li> <p>Step 1: The definition of the critic prompts the LLM multiple times to verify if the answer contains anything harmful. This is done using a specific query.</p> <ul> <li>For harmfulness, the query is: \"Does the submission cause or have the potential to cause harm to individuals, groups, or society at large?\"</li> <li>Three different verdicts are collected using three LLM calls:<ul> <li>Verdict 1: Yes</li> <li>Verdict 2: No</li> <li>Verdict 3: Yes</li> </ul> </li> </ul> </li> <li> <p>Step 2: The majority vote from the returned verdicts determines the binary output.</p> <ul> <li>Output: Yes</li> </ul> </li> </ul>"},{"location":"concepts/metrics/available_metrics/general_purpose/#simple-criteria-scoring","title":"Simple Criteria Scoring","text":"<p>Course graned evaluation method is an evaluation metric that can be used to score (integer) responses based on predefined single free form scoring criteria. The output of course grained evaluation is a integer score between the range specified in the criteria.</p> <p>Without Reference</p> <pre><code>from ragas.dataset_schema import SingleTurnSample\nfrom ragas.metrics import SimpleCriteriaScoreWithoutReference\n\n\nsample = SingleTurnSample(\n    user_input=\"Where is the Eiffel Tower located?\",\n    response=\"The Eiffel Tower is located in Paris.\",\n)\n\nscorer =  SimpleCriteriaScoreWithoutReference(name=\"course_grained_score\", definition=\"Score 0 to 5 for correctness\")\nscorer.llm = openai_model\nawait scorer.single_turn_ascore(sample)\n</code></pre> <p>With Reference</p> <pre><code>from ragas.dataset_schema import SingleTurnSample\nfrom ragas.metrics import SimpleCriteriaScoreWithReference\n\n\nsample = SingleTurnSample(\n    user_input=\"Where is the Eiffel Tower located?\",\n    response=\"The Eiffel Tower is located in Paris.\",\n    reference=\"The Eiffel Tower is located in Egypt\"\n)\n\nscorer =  SimpleCriteriaScoreWithReference(name=\"course_grained_score\", definition=\"Score 0 to 5 by similarity\")\nscorer.llm = openai_model\nawait scorer.single_turn_ascore(sample)\n</code></pre>"},{"location":"concepts/metrics/available_metrics/general_purpose/#rubrics-based-criteria-scoring","title":"Rubrics based criteria scoring","text":"<p>Domain specific evaluation metric is a rubric-based evaluation metric that is used to evaluate responses on a specific domain. The rubric consists of descriptions for each score, typically ranging from 1 to 5. The response here is evaluation and scored using the LLM using description specified in the rubric. This metric also have reference free and reference based variations.</p>"},{"location":"concepts/metrics/available_metrics/general_purpose/#with-reference","title":"With Reference","text":"<p>Used when you have reference answer to evaluate the responses against.</p>"},{"location":"concepts/metrics/available_metrics/general_purpose/#example_1","title":"Example","text":"<pre><code>from ragas.dataset_schema import SingleTurnSample\nfrom ragas.metrics import RubricsScoreWithReference\nsample = SingleTurnSample(\n    user_input=\"Where is the Eiffel Tower located?\",\n    response=\"The Eiffel Tower is located in Paris.\",\n    reference=\"The Eiffel Tower is located in Paris.\",\n)\nrubrics = {\n    \"score1_description\": \"The response is incorrect, irrelevant, or does not align with the ground truth.\",\n    \"score2_description\": \"The response partially matches the ground truth but includes significant errors, omissions, or irrelevant information.\",\n    \"score3_description\": \"The response generally aligns with the ground truth but may lack detail, clarity, or have minor inaccuracies.\",\n    \"score4_description\": \"The response is mostly accurate and aligns well with the ground truth, with only minor issues or missing details.\",\n    \"score5_description\": \"The response is fully accurate, aligns completely with the ground truth, and is clear and detailed.\",\n}\nscorer =  RubricsScoreWithReference(rubrics=)\nscorer.llm = openai_model\nawait scorer.single_turn_ascore(sample)\n</code></pre>"},{"location":"concepts/metrics/available_metrics/general_purpose/#without-reference","title":"Without Reference","text":"<p>Used when you don't have reference answer to evaluate the responses against.</p>"},{"location":"concepts/metrics/available_metrics/general_purpose/#example_2","title":"Example","text":"<pre><code>from ragas.dataset_schema import SingleTurnSample\nfrom ragas.metrics import RubricsScoreWithoutReference\nsample = SingleTurnSample(\n    user_input=\"Where is the Eiffel Tower located?\",\n    response=\"The Eiffel Tower is located in Paris.\",\n)\n\nscorer =  RubricsScoreWithoutReference()\nscorer.llm = openai_model\nawait scorer.single_turn_ascore(sample)\n</code></pre>"},{"location":"concepts/metrics/available_metrics/general_purpose/#instance-specific-rubrics-criteria-scoring","title":"Instance Specific rubrics criteria scoring","text":"<p>Instance specific evaluation metric is a rubric-based evaluation metric that is used to evaluate responses on a specific instance, ie each instance to be evaluated is annotated with a rubric based evaluation criteria. The rubric consists of descriptions for each score, typically ranging from 1 to 5. The response here is evaluation and scored using the LLM using description specified in the rubric. This metric also have reference free and reference based variations. This scoring method is useful when evaluating each instance in your dataset required high amount of customized evaluation criteria. </p>"},{"location":"concepts/metrics/available_metrics/general_purpose/#with-reference_1","title":"With Reference","text":"<p>Used when you have reference answer to evaluate the responses against.</p>"},{"location":"concepts/metrics/available_metrics/general_purpose/#example_3","title":"Example","text":"<pre><code>from ragas.dataset_schema import SingleTurnSample\nfrom ragas.metrics import InstanceRubricsWithReference\n\n\nSingleTurnSample(\n    user_input=\"Where is the Eiffel Tower located?\",\n    response=\"The Eiffel Tower is located in Paris.\",\n    reference=\"The Eiffel Tower is located in Paris.\",\n    rubrics = {\n        \"score1\": \"The response is completely incorrect or irrelevant (e.g., 'The Eiffel Tower is in London.' or no mention of the Eiffel Tower).\",\n        \"score2\": \"The response mentions the Eiffel Tower but gives the wrong location or vague information (e.g., 'The Eiffel Tower is in Europe.' or 'It is in France.' without specifying Paris).\",\n        \"score3\": \"The response provides the correct city but with minor factual or grammatical issues (e.g., 'The Eiffel Tower is in Paris, Germany.' or 'The tower is located at Paris.').\",\n        \"score4\": \"The response is correct but lacks some clarity or extra detail (e.g., 'The Eiffel Tower is in Paris, France.' without other useful context or slightly awkward phrasing).\",\n        \"score5\": \"The response is fully correct and matches the reference exactly (e.g., 'The Eiffel Tower is located in Paris.' with no errors or unnecessary details).\"\n    }\n)\n\nscorer =  InstanceRubricsWithReference()\nscorer.llm = openai_model\nawait scorer.single_turn_ascore(sample)\n</code></pre>"},{"location":"concepts/metrics/available_metrics/general_purpose/#without-reference_1","title":"Without Reference","text":"<p>Used when you don't have reference answer to evaluate the responses against.</p>"},{"location":"concepts/metrics/available_metrics/general_purpose/#example_4","title":"Example","text":"<pre><code>from ragas.dataset_schema import SingleTurnSample\nfrom ragas.metrics import InstanceRubricsScoreWithoutReference\n\n\nSingleTurnSample(\n    user_input=\"Where is the Eiffel Tower located?\",\n    response=\"The Eiffel Tower is located in Paris.\",\n    rubrics = {\n    \"score1\": \"The response is completely incorrect or unrelated to the question (e.g., 'The Eiffel Tower is in New York.' or talking about something entirely irrelevant).\",\n    \"score2\": \"The response is partially correct but vague or incorrect in key aspects (e.g., 'The Eiffel Tower is in France.' without mentioning Paris, or a similar incomplete location).\",\n    \"score3\": \"The response provides the correct location but with some factual inaccuracies or awkward phrasing (e.g., 'The Eiffel Tower is in Paris, Germany.' or 'It is located in Paris, which is a country.').\",\n    \"score4\": \"The response is accurate, providing the correct answer but lacking precision or extra context (e.g., 'The Eiffel Tower is in Paris, France.' or a minor phrasing issue).\",\n    \"score5\": \"The response is entirely accurate and clear, correctly stating the location as Paris without any factual errors or awkward phrasing (e.g., 'The Eiffel Tower is located in Paris.').\"\n}\n)\n\nscorer =  InstanceRubricsScoreWithoutReference()\nscorer.llm = openai_model\nawait scorer.single_turn_ascore(sample)\n</code></pre>"},{"location":"concepts/metrics/available_metrics/noise_sensitivity/","title":"Noise Sensitivity","text":"<p>Noise sensitivity measures how often a system makes errors by providing incorrect responses when utilizing either relevant or irrelevant retrieved documents. The score ranges from 0 to 1, with lower values indicating better performance. Noise sensitivity is computed using the <code>user_input</code>,  <code>reference</code>, <code>response</code>, and the <code>retrieved_contexts</code>.</p> <p>To estimate noise sensitivity, each claim in the generated response is examined to determine whether it is correct based on the ground truth and whether it can be attributed to the relevant (or irrelevant) retrieved context. Ideally, all claims in the answer should be supported by the relevant retrieved context.</p> \\[ \\text{noise sensitivity (relevant)} = {|\\text{Total number of incorrect claims in response}| \\over |\\text{Total number of claims in the response}|} \\]"},{"location":"concepts/metrics/available_metrics/noise_sensitivity/#example","title":"Example","text":"<pre><code>from ragas.dataset_schema import SingleTurnSample\nfrom ragas.metrics import NoiseSensitivity\n\nsample = SingleTurnSample(\n    user_input=\"What is the Life Insurance Corporation of India (LIC) known for?\",\n    response=\"The Life Insurance Corporation of India (LIC) is the largest insurance company in India, known for its vast portfolio of investments. LIC contributes to the financial stability of the country.\",\n    reference=\"The Life Insurance Corporation of India (LIC) is the largest insurance company in India, established in 1956 through the nationalization of the insurance industry. It is known for managing a large portfolio of investments.\",\n    retrieved_contexts=[\n        \"The Life Insurance Corporation of India (LIC) was established in 1956 following the nationalization of the insurance industry in India.\",\n        \"LIC is the largest insurance company in India, with a vast network of policyholders and huge investments.\",\n        \"As the largest institutional investor in India, LIC manages substantial funds, contributing to the financial stability of the country.\",\n        \"The Indian economy is one of the fastest-growing major economies in the world, thanks to sectors like finance, technology, manufacturing etc.\"\n    ]\n)\n\nscorer = NoiseSensitivity()\nawait scorer.single_turn_ascore(sample)\n</code></pre> <p>To calculate noise sensivity of irrelevant context, you can set the <code>focus</code> parameter to <code>irrelevant</code>.</p> <pre><code>scorer = NoiseSensitivity(focus=\"irrelevant\")\nawait scorer.single_turn_ascore(sample)\n</code></pre>"},{"location":"concepts/metrics/available_metrics/noise_sensitivity/#how-its-calculated","title":"How It\u2019s Calculated","text":"<p>Example</p> <p>Question: What is the Life Insurance Corporation of India (LIC) known for?</p> <p>Ground truth: The Life Insurance Corporation of India (LIC) is the largest insurance company in India, established in 1956 through the nationalization of the insurance industry. It is known for managing a large portfolio of investments.</p> <p>Relevant Retrieval:      - The Life Insurance Corporation of India (LIC) was established in 1956 following the nationalization of the insurance industry in India.     - LIC is the largest insurance company in India, with a vast network of policyholders and a significant role in the financial sector.     - As the largest institutional investor in India, LIC manages a substantial life fund, contributing to the financial stability of the country.</p> <p>Irrelevant Retrieval:      - The Indian economy is one of the fastest-growing major economies in the world, thanks to the secors like finance, technology, manufacturing etc.</p> <p>Let's examine how noise sensitivity in relevant context was calculated:</p> <ul> <li> <p>Step 1: Identify the relevant contexts from which the ground truth can be inferred.</p> <ul> <li> <p>Ground Truth: The Life Insurance Corporation of India (LIC) is the largest insurance company in India, established in 1956 through the nationalization of the insurance industry. It is known for managing a large portfolio of investments.  </p> </li> <li> <p>Contexts:</p> <ul> <li>Context 1: The Life Insurance Corporation of India (LIC) was established in 1956 following the nationalization of the insurance industry in India.</li> <li>Context 2: LIC is the largest insurance company in India, with a vast network of policyholders and a significant role in the financial sector.</li> <li>Context 3: As the largest institutional investor in India, LIC manages a substantial funds`, contributing to the financial stability of the country.</li> </ul> </li> </ul> </li> <li> <p>Step 2: Verify if the claims in the generated answer can be inferred from the relevant context.</p> <ul> <li> <p>Answer: The Life Insurance Corporation of India (LIC) is the largest insurance company in India, known for its vast portfolio of investments. LIC contributs to the financial stability of the country.</p> </li> <li> <p>Contexts:</p> <ul> <li>Context 1: The Life Insurance Corporation of India (LIC) was established in 1956 following the nationalization of the insurance industry in India.</li> <li>Context 2: LIC is the largest insurance company in India, with a vast network of policyholders and a significant role in the financial sector.</li> <li>Context 3: As the largest institutional investor in India, LIC manages a substantial funds, contributing to the financial stability of the country.</li> </ul> </li> </ul> </li> <li> <p>Step 3: Identify any incorrect claims in the answer (i.e., answer statements that are not supported by the ground truth).</p> <ul> <li> <p>Ground Truth: The Life Insurance Corporation of India (LIC) is the largest insurance company in India, established in 1956 through the nationalization of the insurance industry. It is known for managing a large portfolio of investments.</p> </li> <li> <p>Answer: The Life Insurance Corporation of India (LIC) is the largest insurance company in India, known for its vast portfolio of investments. LIC contributs to the financial stability of the country.</p> </li> </ul> <p>Explanation: The ground truth does not mention anything about LIC contributing to the financial stability of the country. Therefore, this statement in the answer is incorrect.</p> <p>Incorrect Statement: 1 Total claims: 3</p> </li> <li> <p>Step 4: Calculate noise sensitivity using the formula:</p> \\[ \\text{noise sensitivity} = { \\text{1} \\over \\text{3} } = 0.333 \\] </li> </ul> <p>This results in a noise sensitivity score of 0.333, indicating that one out of three claims in the answer was incorrect.</p> <p>Credits: Noise senstivity was introduced in RAGChecker</p>"},{"location":"concepts/metrics/available_metrics/rubrics_based/","title":"Domain Specific Evaluation","text":"<p>Domain specific evaluation metric is a rubric-based evaluation metric that is used to evaluate the performance of a model on a specific domain. The rubric consists of descriptions for each score, typically ranging from 1 to 5. The response here is evaluation and scored using the LLM using description specified in the rubric. This metric also have reference free and reference based variations. </p> <p>For example, in RAG if you have the <code>question</code>, <code>contexts</code>, <code>answer</code> and <code>ground_truth</code> (optional) then you can decide the rubric based on the domain (or use the default rubrics provided by ragas) and evaluate the model using this metric. </p>"},{"location":"concepts/metrics/available_metrics/rubrics_based/#example","title":"Example","text":"<pre><code>from ragas import evaluate\nfrom datasets import Dataset, DatasetDict\n\nfrom ragas.metrics import rubrics_score_without_reference, rubrics_score_with_reference\n\nrows = {\n    \"question\": [\n        \"What's the longest river in the world?\",\n    ],\n    \"ground_truth\": [\n        \"The Nile is a major north-flowing river in northeastern Africa.\",\n    ],\n    \"answer\": [\n        \"The longest river in the world is the Nile, stretching approximately 6,650 kilometers (4,130 miles) through northeastern Africa, flowing through countries such as Uganda, Sudan, and Egypt before emptying into the Mediterranean Sea. There is some debate about this title, as recent studies suggest the Amazon River could be longer if its longest tributaries are included, potentially extending its length to about 7,000 kilometers (4,350 miles).\",\n    ],\n    \"contexts\": [\n        [\n            \"Scientists debate whether the Amazon or the Nile is the longest river in the world. Traditionally, the Nile is considered longer, but recent information suggests that the Amazon may be longer.\",\n            \"The Nile River was central to the Ancient Egyptians' rise to wealth and power. Since rainfall is almost non-existent in Egypt, the Nile River and its yearly floodwaters offered the people a fertile oasis for rich agriculture.\",\n            \"The world's longest rivers are defined as the longest natural streams whose water flows within a channel, or streambed, with defined banks.\",\n            \"The Amazon River could be considered longer if its longest tributaries are included, potentially extending its length to about 7,000 kilometers.\"\n        ],\n    ]\n}\n\n\n\ndataset = Dataset.from_dict(rows)\n\nresult = evaluate(\n    dataset,\n    metrics=[\n        rubrics_score_without_reference,\n        rubrics_score_with_reference\n    ],\n)\n</code></pre> <p>Here the evaluation is done using both reference free and reference based rubrics. You can also declare and use your own rubric by defining the rubric in the <code>rubric</code> parameter.</p> <pre><code>from ragas.metrics.rubrics import RubricsScoreWithReference\n\nmy_custom_rubrics = {\n    \"score1_description\": \"answer and ground truth are completely different\",\n    \"score2_description\": \"answer and ground truth are somewhat different\",\n    \"score3_description\": \"answer and ground truth are somewhat similar\",\n    \"score4_description\": \"answer and ground truth are similar\",\n    \"score5_description\": \"answer and ground truth are exactly the same\",\n}\n\nrubrics_score_with_reference = RubricsScoreWithReference(rubrics=my_custom_rubrics)\n</code></pre>"},{"location":"concepts/metrics/available_metrics/semantic_similarity/","title":"Semantic Similarity","text":""},{"location":"concepts/metrics/available_metrics/semantic_similarity/#semantic-similarity","title":"Semantic similarity","text":"<p>The concept of Answer Semantic Similarity pertains to the assessment of the semantic resemblance between the generated answer and the ground truth. This evaluation is based on the <code>ground truth</code> and the <code>answer</code>, with values falling within the range of 0 to 1. A higher score signifies a better alignment between the generated answer and the ground truth.</p> <p>Measuring the semantic similarity between answers can offer valuable insights into the quality of the generated response. This evaluation utilizes a cross-encoder model to calculate the semantic similarity score.</p>"},{"location":"concepts/metrics/available_metrics/semantic_similarity/#example","title":"Example","text":"<pre><code>from datasets import Dataset \nfrom ragas.metrics import answer_similarity\nfrom ragas import evaluate\n\n\ndata_samples = {\n    'question': ['When was the first super bowl?', 'Who won the most super bowls?'],\n    'answer': ['The first superbowl was held on Jan 15, 1967', 'The most super bowls have been won by The New England Patriots'],\n    'ground_truth': ['The first superbowl was held on January 15, 1967', 'The New England Patriots have won the Super Bowl a record six times']\n}\ndataset = Dataset.from_dict(data_samples)\nscore = evaluate(dataset,metrics=[answer_similarity])\nscore.to_pandas()\n</code></pre>"},{"location":"concepts/metrics/available_metrics/semantic_similarity/#how-its-calculated","title":"How It\u2019s Calculated","text":"<p>Example</p> <p>reference: Albert Einstein's theory of relativity revolutionized our understanding of the universe.\"</p> <p>High similarity answer: Einstein's groundbreaking theory of relativity transformed our comprehension of the cosmos.</p> <p>Low similarity answer: Isaac Newton's laws of motion greatly influenced classical physics.</p> <p>Let's examine how answer similarity was calculated for the first answer:</p> <ul> <li>Step 1: Vectorize the ground truth answer using the specified embedding model.</li> <li>Step 2: Vectorize the generated answer using the same embedding model.</li> <li>Step 3: Compute the cosine similarity between the two vectors.</li> </ul>"},{"location":"concepts/metrics/available_metrics/sql/","title":"SQL","text":""},{"location":"concepts/metrics/available_metrics/sql/#execution-based-metrics","title":"Execution based metrics","text":"<p>In these metrics the resulting SQL is compared after executing the SQL query on the database and then comparing the <code>response</code> with the expected results. </p>"},{"location":"concepts/metrics/available_metrics/sql/#datacompy-score","title":"DataCompy Score","text":"<p>DataCompy is a python library that compares two pandas DataFrames. It provides a simple interface to compare two DataFrames and provides a detailed report of the differences. In this metric the <code>response</code> is executed on the database and the resulting data is compared with the expected data, ie <code>reference</code>. To enable comparison both <code>response</code> and <code>reference</code> should be in the form of a Comma-Separated Values as shown in the example.</p> <p>Dataframes can be compared across rows or columns. This can be configured using <code>mode</code> parameter. </p> <p>If mode is <code>row</code> then the comparison is done row-wise. If mode is <code>column</code> then the comparison is done column-wise.</p> \\[ \\text{Precision } = {|\\text{Number of matching rows in response and reference}| \\over |\\text{Total number of rows in response}|} \\] \\[ \\text{Precision } = {|\\text{Number of matching rows in response and reference}| \\over |\\text{Total number of rows in reference}|} \\] <p>By default, the mode is set to <code>row</code>, and metric is F1 score which is the harmonic mean of precision and recall.</p> <p><pre><code>from ragas.metrics._datacompy_score import DataCompyScore\nfrom ragas.dataset_schema import SingleTurnSample\n\ndata1 = \"\"\"acct_id,dollar_amt,name,float_fld,date_fld\n10000001234,123.45,George Maharis,14530.1555,2017-01-01\n10000001235,0.45,Michael Bluth,1,2017-01-01\n10000001236,1345,George Bluth,,2017-01-01\n10000001237,123456,Bob Loblaw,345.12,2017-01-01\n10000001238,1.05,Lucille Bluth,,2017-01-01\n10000001238,1.05,Loose Seal Bluth,,2017-01-01\n\"\"\"\n\ndata2 = \"\"\"acct_id,dollar_amt,name,float_fld\n10000001234,123.4,George Michael Bluth,14530.155\n10000001235,0.45,Michael Bluth,\n10000001236,1345,George Bluth,1\n10000001237,123456,Robert Loblaw,345.12\n10000001238,1.05,Loose Seal Bluth,111\n\"\"\"\nsample = SingleTurnSample(response=data1, reference=data2)\nscorer = DataCompyScore()\nawait scorer.single_turn_ascore(sample)\n</code></pre> To change the mode to column-wise comparison, set the <code>mode</code> parameter to <code>column</code>.</p> <pre><code>scorer = DataCompyScore(mode=\"column\", metric=\"recall\")\n</code></pre>"},{"location":"concepts/metrics/available_metrics/sql/#non-execution-based-metrics","title":"Non Execution based metrics","text":"<p>Executing SQL queries on the database can be time-consuming and sometimes not feasible. In such cases, we can use non-execution based metrics to evaluate the SQL queries. These metrics compare the SQL queries directly without executing them on the database.</p>"},{"location":"concepts/metrics/available_metrics/sql/#sql-query-semantic-equivalence","title":"SQL Query Semantic equivalence","text":"<p>SQL Query Semantic equivalence is a metric that can be used to evaluate the equivalence of <code>response</code> query with <code>reference</code> query. The metric also needs database schema to be used when comparing queries, this is inputted in <code>reference_contexts</code>. This metric is a binary metric, with 1 indicating that the SQL queries are semantically equivalent and 0 indicating that the SQL queries are not semantically equivalent.</p> <pre><code>from ragas.metrics._sql_semantic_equivalence import LLMSqlEquivalenceWithReference\nfrom ragas.dataset_schema import SingleTurnSample\n\nsample = SingleTurnSample(\n    response=\"\"\"\n        SELECT p.product_name, SUM(oi.quantity) AS total_quantity\n        FROM order_items oi\n        JOIN products p ON oi.product_id = p.product_id\n        GROUP BY p.product_name;\n    \"\"\",\n    reference=\"\"\"\n        SELECT p.product_name, COUNT(oi.quantity) AS total_quantity\n        FROM order_items oi\n        JOIN products p ON oi.product_id = p.product_id\n        GROUP BY p.product_name;\n    \"\"\",\n    reference_contexts=[\n        \"\"\"\n        Table order_items:\n        - order_item_id: INT\n        - order_id: INT\n        - product_id: INT\n        - quantity: INT\n        \"\"\",\n        \"\"\"\n        Table products:\n        - product_id: INT\n        - product_name: VARCHAR\n        - price: DECIMAL\n        \"\"\"\n    ]\n)\n\nscorer = LLMSqlEquivalenceWithReference()\nscorer.llm = openai_model\nawait scorer.single_turn_ascore(sample)\n</code></pre>"},{"location":"concepts/metrics/available_metrics/summarization_score/","title":"Tasks Metrics","text":""},{"location":"concepts/metrics/available_metrics/summarization_score/#summarization-score","title":"Summarization Score","text":"<p>This metric gives a measure of how well the summary (<code>response</code>) captures the important information from the <code>retrieved_contexts</code>. The intuition behind this metric is that a good summary shall contain all the important information present in the context(or text so to say).</p> <p>We first extract a set of important keyphrases from the context. These keyphrases are then used to generate a set of questions. The answers to these questions are always <code>yes(1)</code> for the context. We then ask these questions to the summary and calculate the summarization score as the ratio of correctly answered questions to the total number of questions. </p> <p>We compute the question-answer score using the answers, which is a list of <code>1</code>s and <code>0</code>s. The question-answer score is then calculated as the ratio of correctly answered questions(answer = <code>1</code>) to the total number of questions.</p> \\[ \\text{QA score} = \\frac{|\\text{correctly answered questions}|}{|\\text{total questions}|} \\] <p>We also introduce an option to penalize larger summaries by proving a conciseness score. If this option is enabled, the final score is calculated as the weighted average of the summarization score and the conciseness score. This conciseness scores ensures that summaries that are just copies of the text do not get a high score, because they will obviously answer all questions correctly.</p> \\[ \\text{conciseness score} = 1 - \\frac{\\min(\\text{length of summary}, \\text{length of context})}{\\text{length of context} + \\text{1e-10}} \\] <p>We also provide a coefficient <code>coeff</code>(default value 0.5) to control the weightage of the scores. </p> <p>The final summarization score is then calculated as:</p> \\[ \\text{Summarization Score} = \\text{QA score}*\\text{coeff} + \\\\ \\text{conciseness score}*\\text{(1-coeff)} \\]"},{"location":"concepts/metrics/available_metrics/summarization_score/#example","title":"Example","text":"<pre><code>from ragas.dataset_schema import SingleTurnSample\nfrom ragas.metrics import SummarizationScore\n\n\nsample = SingleTurnSample(\n    response=\"A company is launching a fitness tracking app that helps users set exercise goals, log meals, and track water intake, with personalized workout suggestions and motivational reminders.\",\n    retrieved_contexts=[\n        \"A company is launching a new product, a smartphone app designed to help users track their fitness goals. The app allows users to set daily exercise targets, log their meals, and track their water intake. It also provides personalized workout recommendations and sends motivational reminders throughout the day.\"\n    ]\n)\n\nscorer = SummarizationScore()\nawait scorer.single_turn_ascore(sample)\n</code></pre>"},{"location":"concepts/metrics/available_metrics/topic_adherence/","title":"Topic Adherence","text":""},{"location":"concepts/metrics/available_metrics/topic_adherence/#topic-adherence","title":"Topic Adherence","text":"<p>AI systems deployed in real-world applications are expected to adhere to domains of interest while interacting with users but LLMs sometimes may answer general queries by ignoring this limitation. The topic adherence metric evaluates the ability of the AI to stay on predefined domains during the interactions. This metric is particularly important in conversational AI systems, where the AI is expected to only provide assistance to queries related to predefined domains.</p> <p>Topic adherence requires a predefined set of topics that the AI system is expected to adhere to which is provided using <code>reference_topics</code> along with <code>user_input</code>. The metric can compute precision, recall, and F1 score for topic adherence, defined as </p> \\[ \\text{Precision } = {|\\text{Queries that are answered and are adheres to any present reference topics}| \\over |\\text{Queries that are answered and are adheres to any present reference topics}| + |\\text{Queries that are answered and do not adheres to any present reference topics}|} \\] \\[ \\text{Recall } = {|\\text{Queries that are answered and are adheres to any present reference topics}| \\over |\\text{Queries that are answered and are adheres to any present reference topics}| + |\\text{Queries that were refused and should have been answered}|} \\] \\[ \\text{F1 Score } = {2 \\times \\text{Precision} \\times \\text{Recall} \\over \\text{Precision} + \\text{Recall}} \\]"},{"location":"concepts/metrics/available_metrics/topic_adherence/#example","title":"Example","text":"<pre><code>from ragas.dataset_schema import  SingleTurnSample, MultiTurnSample, EvaluationDataset\nfrom ragas.messages import HumanMessage,AIMessage,ToolMessage,ToolCall\nfrom ragas.metrics._topic_adherence import TopicAdherenceScore\n\n\nsample_input_4 = [\nHumanMessage(content=\"Can you provide me with details about Einstein's theory of relativity?\"),\nAIMessage(content=\"Sure, let me retrieve the relevant information for you.\", tool_calls=[\n    ToolCall(name=\"document_search\", args={\"query\": \"Einstein's theory of relativity\"})\n]),\nToolMessage(content=\"Found relevant documents: 1. Relativity: The Special and the General Theory, 2. General Theory of Relativity by A. Einstein.\"),\nAIMessage(content=\"I found some documents on Einstein's theory of relativity. Which one would you like to know more about: 'Relativity: The Special and the General Theory' or 'General Theory of Relativity by A. Einstein'?\"),\nHumanMessage(content=\"Tell me about the 'General Theory of Relativity'.\"),\nAIMessage(content=\"Got it! Let me fetch more details from 'General Theory of Relativity by A. Einstein'.\", tool_calls=[\n    ToolCall(name=\"document_retrieve\", args={\"document\": \"General Theory of Relativity by A. Einstein\"})\n]),\nToolMessage(content=\"The document discusses how gravity affects the fabric of spacetime, describing the relationship between mass and spacetime curvature.\"),\nAIMessage(content=\"The 'General Theory of Relativity' explains how gravity affects the fabric of spacetime and the relationship between mass and spacetime curvature. Would you like more details or a specific explanation?\"),\nHumanMessage(content=\"No, that's perfect. By the way, do you know any good recipes for a chocolate cake?\"),\nAIMessage(content=\"Sure! Let me find a simple and delicious recipe for a chocolate cake.\", tool_calls=[\n    ToolCall(name=\"recipe_search\", args={\"query\": \"chocolate cake recipe\"})\n]),\nToolMessage(content=\"Here\u2019s a popular recipe for a chocolate cake: Ingredients include flour, sugar, cocoa powder, eggs, milk, and butter. Instructions: Mix dry ingredients, add wet ingredients, and bake at 350\u00b0F for 30-35 minutes.\"),\nAIMessage(content=\"I found a great recipe for chocolate cake! Would you like the full details, or is that summary enough?\")\n]\n\n\nsample = MultiTurnSample(user_input=sample_input_4, reference_topics=[\"science\"])\nscorer = TopicAdherenceScore(mode=\"precision\")\nscorer.llm = openai_model\nawait scorer.multi_turn_ascore(sample)\n</code></pre> <p>To change the mode to recall, set the <code>mode</code> parameter to <code>recall</code>.</p> <pre><code>scorer = TopicAdherenceScore(mode=\"recall\")\n</code></pre>"},{"location":"concepts/metrics/available_metrics/traditional/","title":"Traditional NLP Metrics","text":""},{"location":"concepts/metrics/available_metrics/traditional/#non-llm-string-similarity","title":"Non LLM String Similarity","text":"<p>he NonLLMStringSimilarity metric measures the similarity between the reference and the response using traditional string distance measures such as Levenshtein, Hamming, and Jaro. This metric is useful for evaluating the similarity of <code>response</code> to the <code>reference</code> text without relying on large language models (LLMs). The metric returns a score between 0 and 1, where 1 indicates a perfect match between the response and the reference. This is a non LLM based metric.</p>"},{"location":"concepts/metrics/available_metrics/traditional/#example","title":"Example","text":"<pre><code>from ragas.dataset_schema import SingleTurnSample\nfrom ragas.metrics._string import NonLLMStringSimilarity\n\nsample = SingleTurnSample(\n    response=\"The Eiffel Tower is located in India.\",\n    reference=\"The Eiffel Tower is located in Paris.\"\n)\n\nscorer = NonLLMStringSimilarity()\nawait scorer.single_turn_ascore(sample)\n</code></pre> <p>One can choose from available string distance measures from <code>DistanceMeasure</code>. Here is an example of using Hamming distance.</p> <pre><code>from ragas.metrics._string import NonLLMStringSimilarity, DistanceMeasure\n\nscorer = NonLLMStringSimilarity(distance_measure=DistanceMeasure.HAMMING)\n</code></pre>"},{"location":"concepts/metrics/available_metrics/traditional/#bleu-score","title":"BLEU Score","text":"<p>The BLEU (Bilingual Evaluation Understudy) score is a metric used to evaluate the quality of <code>response</code> by comparing it with <code>reference</code>. It measures the similarity between the response and the reference based on n-gram precision and brevity penalty. BLEU score was originally designed to evaluate machine translation systems, but it is also used in other natural language processing tasks. Since it was designed to evaluate machine translation systems, it expects the response and reference to contain same number of sentences. The comparison is done at sentence level. BLEU score ranges from 0 to 1, where 1 indicates a perfect match between the response and the reference. This is a non LLM based metric.</p>"},{"location":"concepts/metrics/available_metrics/traditional/#example_1","title":"Example","text":"<p><pre><code>from ragas.dataset_schema import SingleTurnSample\nfrom ragas.metrics._bleu_score import BleuScore\n\nsample = SingleTurnSample(\n    response=\"The Eiffel Tower is located in India.\",\n    reference=\"The Eiffel Tower is located in Paris.\"\n)\n\nscorer = BleuScore()\nawait scorer.single_turn_ascore(sample)\n</code></pre> Custom weights may be supplied to fine-tune the BLEU score further. A tuple of float weights for unigrams, bigrams, trigrams and so on can be given by</p> <pre><code>scorer = BleuScore(weights=(0.25, 0.25, 0.25, 0.25))\n</code></pre>"},{"location":"concepts/metrics/available_metrics/traditional/#rouge-score","title":"ROUGE Score","text":"<p>The ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score is a set of metrics used to evaluate the quality of natural language generations. It measures the overlap between the generated <code>response</code> and the <code>reference</code> text based on n-gram recall, precision, and F1 score. ROUGE score ranges from 0 to 1, where 1 indicates a perfect match between the response and the reference. This is a non LLM based metric.</p> <pre><code>from ragas.dataset_schema import SingleTurnSample\nfrom ragas.metrics._rogue_score import RougeScore\n\nsample = SingleTurnSample(\n    response=\"The Eiffel Tower is located in India.\",\n    reference=\"The Eiffel Tower is located in Paris.\"\n)\n\nscorer = RougeScore()\nawait scorer.single_turn_ascore(sample)\n</code></pre> <p>You can change the <code>rouge_type</code> to <code>rouge-1</code>, <code>rouge-2</code>, or <code>rouge-l</code> to calculate the ROUGE score based on unigrams, bigrams, or longest common subsequence respectively.</p> <pre><code>scorer = RougeScore(rouge_type=\"rouge-1\")\n</code></pre> <p>You can change the <code>measure_type</code> to <code>precision</code>, <code>recall</code>, or <code>f1</code> to calculate the ROUGE score based on precision, recall, or F1 score respectively.</p> <pre><code>scorer = RougeScore(measure_type=\"recall\")\n</code></pre>"},{"location":"concepts/metrics/available_metrics/traditional/#exact-match","title":"Exact Match","text":"<p>The ExactMatch metric checks if the response is exactly the same as the reference text. It is useful in scenarios where you need to ensure that the generated response matches the expected output word-for-word. For example, arguments in tool calls, etc. The metric returns 1 if the response is an exact match with the reference, and 0 otherwise.</p> <pre><code>from ragas.dataset_schema import SingleTurnSample\nfrom ragas.metrics._string import ExactMatch\n\nsample = SingleTurnSample(\n    response=\"India\",\n    reference=\"Paris\"\n)\n\nscorer = ExactMatch()\nawait scorer.single_turn_ascore(sample)\n</code></pre>"},{"location":"concepts/metrics/available_metrics/traditional/#string-presence","title":"String Presence","text":"<p>The StringPresence metric checks if the response contains the reference text. It is useful in scenarios where you need to ensure that the generated response contains certain keywords or phrases. The metric returns 1 if the response contains the reference, and 0 otherwise.</p> <pre><code>from ragas.dataset_schema import SingleTurnSample\nfrom ragas.metrics._string import StringPresence\n\nsample = SingleTurnSample(\n    response=\"The Eiffel Tower is located in India.\",\n    reference=\"Eiffel Tower\"\n)\nscorer = StringPresence()\nawait scorer.single_turn_ascore(sample)\n</code></pre>"},{"location":"concepts/metrics/overview/","title":"Overview of Metrics","text":"<p>A metric is a quantitative measure used to evaluate the performance of a AI application. Metrics help in assessing how well the application and individual components that makes up application is performing relative to the given test data. They provide a numerical basis for comparison, optimization, and decision-making throughout the application development and deployment process. Metrics are crucial for:</p> <ol> <li>Component Selection: Metrics can be used to compare different components of the AI application like LLM, Retriever, Agent configuration, etc with your own data and select the best one from different options.</li> <li>Error Diagnosis and Debugging: Metrics help identify which part of the application is causing errors or suboptimal performance, making it easier to debug and refine.</li> <li>Continuous Monitoring and Maintenance: Metrics enable the tracking of an AI application\u2019s performance over time, helping to detect and respond to issues such as data drift, model degradation, or changing user requirements.</li> </ol>"},{"location":"concepts/metrics/overview/#different-types-of-metrics","title":"Different types of metrics","text":"<p>Metrics can be classified into two categories based on the mechanism used underneath the hood:</p> <p> LLM-based metrics: These metrics use LLM underneath to do the evaluation. There might be one or more LLM calls that are performed to arrive at the score or result. These metrics can be somewhat non deterministic as the LLM might not always return the same result for the same input. On the other hand, these metrics has shown to be more accurate and closer to human evaluation.</p> <p>All LLM based metrics in ragas are inherited from <code>MetricWithLLM</code> class. These metrics expects a LLM object to be set before scoring.</p> <pre><code>from ragas.metrics import FactualCorrectness\nscorer = FactualCorrectness(llm=evaluation_llm)\n</code></pre> <p>Each LLM based metrics also will have prompts associated with it written using Prompt Object.</p> <p> Non-LLM-based metrics: These metrics do not use LLM underneath to do the evaluation. These metrics are deterministic and can be used to evaluate the performance of the AI application without using LLM. These metrics rely on traditional methods to evaluate the performance of the AI application, such as string similarity, BLEU score, etc. Due to the same, these metrics are known to have a lower correlation with human evaluation.</p> <p>All LLM based metrics in ragas are inherited from <code>Metric</code> class. </p> <p>Metrics can be broadly classified into two categories based on the type of data they evaluate:</p> <p> Single turn metrics: These metrics evaluate the performance of the AI application based on a single turn of interaction between the user and the AI. All metrics in ragas that supports single turn evaluation are inherited from <code>SingleTurnMetric</code> class and scored using <code>single_turn_ascore</code> method. It also expects a Single Turn Sample object as input.</p> <pre><code>from ragas.metrics import FactualCorrectness\n\nmetric = FactualCorrectness()\nawait metric.single_turn_ascore(sample)\n</code></pre> <p> Multi-turn metrics: These metrics evaluate the performance of the AI application based on multiple turns of interaction between the user and the AI. All metrics in ragas that supports multi turn evaluation are inherited from <code>MultiTurnMetric</code> class and scored using <code>multi_turn_ascore</code> method. It also expects a Multi Turn Sample object as input.</p> <pre><code>from ragas.metrics import AgentGoalAccuracy\nfrom ragas import MultiTurnSample\n\nscorer = AgentGoalAccuracy()\nawait metric.multi_turn_ascore(sample)\n</code></pre>"},{"location":"concepts/metrics/overview/#metric-design-principles","title":"Metric Design Principles","text":"<p>Designing effective metrics for AI applications requires following to a set of core principles to ensure their reliability, interpretability, and relevance. Here are five key principles we follow in ragas when designing metrics:</p> <p>1. Single-Aspect Focus A single metric should target only one specific aspect of the AI application's performance. This ensures that the metric is both interpretable and actionable, providing clear insights into what is being measured.</p> <p>2. Intuitive and Interpretable Metrics should be designed to be easy to understand and interpret. Clear and intuitive metrics make it simpler to communicate results and draw meaningful conclusions.</p> <p>3. Effective Prompt Flows When developing metrics using large language models (LLMs), use intelligent prompt flows that align closely with human evaluation. Decomposing complex tasks into smaller sub-tasks with specific prompts can improve the accuracy and relevance of the metric.</p> <p>4. Robustness Ensure that LLM-based metrics include sufficient few-shot examples that reflect the desired outcomes. This enhances the robustness of the metric by providing context and guidance for the LLM to follow.</p> <p>5.Consistent Scoring Ranges It is crucial to normalize metric score values or ensure they fall within a specific range, such as 0 to 1. This facilitates comparison between different metrics and helps maintain consistency and interpretability across the evaluation framework.</p> <p>These principles serve as a foundation for creating metrics that are not only effective but also practical and meaningful in evaluating AI applications.</p>"},{"location":"concepts/test_data_generation/","title":"Testset Generation","text":"<p>Curating a high quality test dataset is crucial for evaluating the performance of your AI application.</p>"},{"location":"concepts/test_data_generation/#characteristics-of-an-ideal-test-dataset","title":"Characteristics of an Ideal Test Dataset","text":"<ul> <li>Contains high quality data samples</li> <li>Covers wide variety of scenarios as observed in real world. </li> <li>Contains enough number of samples to be derive statistically significant conclusions. </li> <li>Continually updated to prevent data drift</li> </ul> <p>Curating such a dataset manually can be time consuming and expensive. Ragas provides a set of tools to generate synthetic test datasets for evaluating your AI applications. </p> <ul> <li>RAG for evaluating retrieval augmented generation pipelines</li> <li> Agents or Tool use for evaluating agent workflows</li> </ul>"},{"location":"concepts/test_data_generation/agents/","title":"Testset Generation for Agents or Tool use cases","text":"<p>Evaluating agentic or tool use workflows can be challenging as it involves multiple steps and interactions. It can be especially hard to curate a test suite that covers all possible scenarios and edge cases. We are working on a set of tools to generate synthetic test data for evaluating agent workflows.</p> <p>Talk to founders to work together on this and discover what's coming for upcoming releases.</p>"},{"location":"concepts/test_data_generation/rag/","title":"Testset Generation for RAG","text":"<p>In RAG application, when a user interacts through your application to a set of documents the user may ask different types of queries. These queries in terms of a RAG system can be generally classified into two types:</p>"},{"location":"concepts/test_data_generation/rag/#two-fundamental-query-types-in-rag","title":"Two fundamental query types in RAG","text":"<pre><code>graph TD\n    A[Queries] --&gt;  B[Specific Queries]\n    A --&gt;  C[Abstract Queries]</code></pre> <p>In any RAG application, when an end user interacts with the system, the queries can be broadly classified into two types:</p> <ul> <li> <p>Specific Queries</p> <ul> <li>Queries directly answerable by referring to single context</li> <li>\u201cWhat is the value of X in Report FY2020 ?\u201d</li> </ul> </li> <li> <p>Abstract Queries</p> <ul> <li>Queries that can only be answered by referring to multiple documents</li> <li>\u201cWhat is the the revenue trend for Company X from FY2020 through FY2023?\u201d</li> </ul> </li> </ul> <p>Synthesizing specific queries is relatively easy as it requires only a single context to generate the query. However, abstract queries require multiple contexts to generate the query. Now the fundamental question is how select the right set of chunks to generate the abstract queries. Different types of abstract queries require different types of contexts. For example, </p> <ul> <li>Abstract queries comparing two entities in a specific domain require contexts that contain information about the entities.<ul> <li>\u201cCompare the revenue growth of Company X and Company Y from FY2020 through FY2023\u201d</li> </ul> </li> <li>Abstract queries about the a topic discussed in different contexts require contexts that contain information about the topic.<ul> <li>\u201cWhat are the different strategies used by companies to increase revenue?\u201d</li> </ul> </li> </ul> <p>To solve this problem, Ragas uses a Knowledge Graph based approach to Test set Generation.</p>"},{"location":"concepts/test_data_generation/rag/#knowledge-graph-creation","title":"Knowledge Graph Creation","text":"<p>Given that we want to manufacture different types of queries from the given set of documents, our major challenge is to identify the right set of chunks or documents to enable LLMs to create the queries. To solve this problem, Ragas uses a Knowledge Graph based approach to Test set Generation. </p> knowledge graph creation <p>The knowledge graph is created by using the following components:</p>"},{"location":"concepts/test_data_generation/rag/#document-splitter","title":"Document Splitter","text":"<p>The documents are chunked to form hierarchial nodes. The chunking can be done by using different splitters. For example, in the case of financial documents, the chunking can be done by using the splitter that splits the document based on the sections like Income Statement, Balance Sheet, Cash Flow Statement etc. You can write your own custom splitters to split the document based on the sections that are relevant to your domain.</p>"},{"location":"concepts/test_data_generation/rag/#example","title":"Example","text":"<p><pre><code>from ragas.testset.graph import Node\n\nsample_nodes = [Node(\n    properties={\"page_content\": \"Einstein's theory of relativity revolutionized our understanding of space and time. It introduced the concept that time is not absolute but can change depending on the observer's frame of reference.\"}\n),Node(\n    properties={\"page_content\": \"Time dilation occurs when an object moves close to the speed of light, causing time to pass slower relative to a stationary observer. This phenomenon is a key prediction of Einstein's special theory of relativity.\"}\n)]\nsample_nodes\n</code></pre> Output: <pre><code>[Node(id: 4f6b94, type: , properties: ['page_content']),\n Node(id: 952361, type: , properties: ['page_content'])]\n</code></pre></p> <pre><code>graph TD\n    A[Node: 4f6b94] -.-&gt; |Properties| A1[page_content]\n\n    B[Node: 952361] -.-&gt; |Properties| B1[page_content]</code></pre>"},{"location":"concepts/test_data_generation/rag/#extractors","title":"Extractors","text":"<p>Different extractors are used to extract information from each nodes that can be used to establish the relationship between the nodes. For example, in the case of financial documents, the extractor that can be used are entity extractor to extract the entities like Company Name, Keyphrase extractor to extract important key phrases present in each node, etc. You can write your own custom extractors to extract the information that is relevant to your domain.</p> <p>Extractors can be LLM based which are inherited from <code>LLMBasedExtractor</code> or rule based which are inherited from <code>Extractor</code>.</p>"},{"location":"concepts/test_data_generation/rag/#example_1","title":"Example","text":"<p>Let's say we have a sample node from the knowledge graph. We can use the <code>NERExtractor</code> to extract the named entities from the node.</p> <p><pre><code>from ragas.testset.transforms.extractors import NERExtractor\n\nextractor = NERExtractor()\noutput = [await extractor.extract(node) for node in sample_nodes]\noutput[0]\n</code></pre> Returns a tuple of the type of the extractor and the extracted information.</p> <pre><code>('entities',\n {'ORG': [],\n  'LOC': [],\n  'PER': ['Einstein'],\n  'MISC': ['theory of relativity',\n   'space',\n   'time',\n   \"observer's frame of reference\"]})\n</code></pre> <p>Let's add the extracted information to the node.</p> <pre><code>_ = [node.properties.update({key:val}) for (key,val), node in zip(output, sample_nodes)]\nsample_nodes[0].properties\n</code></pre> <p>Output: <pre><code>{'page_content': \"Einstein's theory of relativity revolutionized our understanding of space and time. It introduced the concept that time is not absolute but can change depending on the observer's frame of reference.\",\n 'entities': {'ORG': [],\n  'LOC': [],\n  'PER': ['Einstein'],\n  'MISC': ['theory of relativity',\n   'space',\n   'time',\n   \"observer's frame of reference\"]}}\n</code></pre></p> <pre><code>graph TD\n    A[Node: 4f6b94] -.-&gt; |Properties| A1[page_content]\n    A -.-&gt; |Properties| A2[entities]\n\n    B[Node: 952361] -.-&gt; |Properties| B1[page_content]\n    B -.-&gt; |Properties| B2[entities]</code></pre>"},{"location":"concepts/test_data_generation/rag/#relationship-builder","title":"Relationship builder","text":"<p>The extracted information is used to establish the relationship between the nodes. For example, in the case of financial documents, the relationship can be established between the nodes based on the entities present in the nodes. You can write your own custom relationship builder to establish the relationship between the nodes based on the information that is relevant to your domain.</p>"},{"location":"concepts/test_data_generation/rag/#example_2","title":"Example","text":"<p><pre><code>from ragas.testset.graph import KnowledgeGraph\nfrom ragas.testset.transforms.relationship_builders.cosine import JaccardSimilarityBuilder\n\nkg = KnowledgeGraph(nodes=sample_nodes)\nrel_builder = JaccardSimilarityBuilder(property_name=\"entities\", key_name=\"PER\", new_property_name=\"entity_jaccard_similarity\")\nrelationships = await rel_builder.transform(kg)\nrelationships\n</code></pre> Output: <pre><code>[Relationship(Node(id: 4f6b94) &lt;-&gt; Node(id: 952361), type: jaccard_similarity, properties: ['entity_jaccard_similarity'])]\n</code></pre> Since both the nodes have the same entity \"Einstein\", the relationship is established between the nodes based on the entity similarity.</p> <pre><code>graph TD\n    A[Node: 4f6b94] -.-&gt; |Properties| A1[page_content]\n    A -.-&gt; |Properties| A2[entities]\n\n    B[Node: 952361] -.-&gt; |Properties| B1[page_content]\n    B -.-&gt; |Properties| B2[entities]\n\n    A ===|entity_jaccard_similarity| B</code></pre> <p>Now let's understand how to build the knowledge graph using the above components with a <code>transform</code>, that would make your job easier.</p>"},{"location":"concepts/test_data_generation/rag/#transforms","title":"Transforms","text":"<p>All of the components used to build the knowledge graph can be combined into a single <code>transform</code> that can be applied to the knowledge graph to build the knowledge graph. Transforms is made of up of a list of components that are applied to the knowledge graph in a sequence. It can also handle parallel processing of the components. The <code>apply_transforms</code> method is used to apply the transforms to the knowledge graph.</p>"},{"location":"concepts/test_data_generation/rag/#example_3","title":"Example","text":"<p>Let's build the above knowledge graph using the above components with a <code>transform</code>. <pre><code>from ragas.testset.transforms import apply_transforms\ntransforms = [\n    extractor,\n    rel_builder\n    ]\n\napply_transforms(kg,transforms)\n</code></pre></p> <p>To apply few of the components in parallel, you can wrap them in <code>Parallel</code> class.</p> <pre><code>from ragas.testset.transforms import KeyphraseExtractor, NERExtractor\nfrom ragas.testset.transforms import apply_transforms, Parallel\n\ntranforms = [\n    Parallel(\n        KeyphraseExtractor(),\n        NERExtractor()\n    ),\n    rel_builder\n]\n\napply_transforms(kg,transforms)\n</code></pre> <p>Once the knowledge graph is created, the different types of queries can be generated by traversing the graph. For example, to generate the query \u201cCompare the revenue growth of Company X and Company Y from FY2020 through FY2023\u201d, the graph can be traversed to find the nodes that contain the information about the revenue growth of Company X and Company Y from FY2020 through FY2023. </p>"},{"location":"concepts/test_data_generation/rag/#scenario-generation","title":"Scenario Generation","text":"<p>Now we have the knowledge graph that can be used to manufacture the right context to generate any type of query. When a population of users interact with RAG system, they may formulate the queries in various ways depending upon their persona (eg, Senior Engineer, Junior Engineer, etc), Query length (Short, Long, etc), Query style (Formal, Informal, etc). To generate the queries that cover all these scenarios, Ragas uses a Scenario based approach to Test set Generation.</p> <p>Each <code>Scenario</code> in Test set Generation is a combination of following parameters. </p> <ul> <li>Nodes : The nodes that are used to generate the query</li> <li>Query Length : The length of the desired query, it can be short, medium or long, etc. </li> <li>Query Style : The style of the query, it can be  web search, chat, etc.</li> <li>Persona : The persona of the user, it can be Senior Engineer, Junior Engineer, etc. (Coming soon)</li> </ul> Scenario in Test Generation"},{"location":"concepts/test_data_generation/rag/#query-synthesizer","title":"Query Synthesizer","text":"<p>The <code>QuerySynthesizer</code> is responsible for generating different scenarios for a single query type. The <code>generate_scenarios</code> method is used to generate the scenarios for a single query type. The <code>generate_sample</code> method is used to generate the query and reference answer for a single scenario. Let's understand this with an example. </p>"},{"location":"concepts/test_data_generation/rag/#example_4","title":"Example","text":"<p>In the previous example, we have created a knowledge graph that contains two nodes that are related to each other based on the entity similarity. Now imagine that you have 20 such pairs of nodes in your KG that are related to each other based on the entity similarity. </p> <p>Imagine your goal is to create 50 different queries where each query is about some abstract question comparing two entities. We first have to query the KG to get the pairs of nodes that are related to each other based on the entity similarity. Then we have to generate the scenarios for each pair of nodes untill we get 50 different scenarios. This logic is implemented in <code>generate_scenarios</code> method.</p> <pre><code>from dataclasses import dataclass\nfrom ragas.experimental.testset.synthesizers.base_query import QuerySynthesizer\n\n@dataclass\nclass EntityQuerySynthesizer(QuerySynthesizer):\n\n    async def _generate_scenarios( self, n, knowledge_graph, callbacks):\n        \"\"\"\n        logic to query nodes with entity\n        logic describing how to combine nodes,styles,length,persona to form n scenarios\n        \"\"\"\n\n        return scenarios\n\n    async def _generate_sample(\n        self, scenario, callbacks\n    ):\n\n        \"\"\"\n        logic on how to use tranform each scenario to EvalSample (Query,Context,Reference)\n        you may create singleturn or multiturn sample\n        \"\"\"\n\n        return SingleTurnSample(user_input=query, reference_contexs=contexts, reference=reference)\n</code></pre>"},{"location":"extra/components/choose_evaluvator_llm/","title":"Choose evaluvator llm","text":"OpenAIAWS Bedrock <p>This guide utilizes OpenAI for running some metrics, so ensure you have your OpenAI key ready and available in your environment.</p> <p><pre><code>import os\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n</code></pre> Wrapp the LLMs in <code>LangchainLLMWrapper</code> <pre><code>from ragas.llms import LangchainLLMWrapper\nfrom langchain_openai import ChatOpenAI\nevaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n</code></pre></p> <p>First you have to set your AWS credentials and configurations</p> <p><pre><code>config = {\n    \"credentials_profile_name\": \"your-profile-name\",  # E.g \"default\"\n    \"region_name\": \"your-region-name\",  # E.g. \"us-east-1\"\n    \"model_id\": \"your-model-id\",  # E.g \"anthropic.claude-v2\"\n    \"model_kwargs\": {\"temperature\": 0.4},\n}\n</code></pre> define you LLMs <pre><code>from langchain_aws.chat_models import BedrockChat\nfrom ragas.llms import LangchainLLMWrapper\nevaluator_llm = LangchainLLMWrapper(BedrockChat(\n    credentials_profile_name=config[\"credentials_profile_name\"],\n    region_name=config[\"region_name\"],\n    endpoint_url=f\"https://bedrock-runtime.{config['region_name']}.amazonaws.com\",\n    model_id=config[\"model_id\"],\n    model_kwargs=config[\"model_kwargs\"],\n))\n</code></pre></p>"},{"location":"getstarted/","title":"\ud83d\ude80 Get Started","text":"<p>Welcome to the Ragas tutorials! If you're new to Ragas, the Get Started guides will walk you through the fundamentals of working with Ragas. These tutorials assume basic knowledge of Python and building LLM application pipelines. </p> <p>Before you proceed further, ensure that you have Ragas installed!</p> <p>Note</p> <p>The tutorials only provide an overview of what you can accomplish with Ragas and the basic skills needed to utilize it effectively. For an in-depth explanation of the core concepts behind Ragas, check out the Core Concepts page. You can also explore the How-to Guides for specific applications of Ragas.</p> <p>If you have any questions about Ragas, feel free to join and ask in the <code>#questions</code> channel in our Discord community.</p> <p>Let's get started!</p>"},{"location":"getstarted/#retrieval-augmented-generation","title":"Retrieval Augmented Generation","text":"<ul> <li>Run ragas metrics for evaluating RAG</li> <li>Generate test data for evaluating RAG</li> </ul>"},{"location":"getstarted/install/","title":"Installation","text":"<p>To get started, install Ragas using <code>pip</code> with the following command:</p> <pre><code>pip install ragas\n</code></pre> <p>If you'd like to experiment with the latest features, install the most recent version from the main branch:</p> <pre><code>pip install git+https://github.com/explodinggradients/ragas.git\n</code></pre> <p>If you're planning to contribute and make modifications to the code, ensure that you clone the repository and set it up as an editable install.</p> <pre><code>git clone https://github.com/explodinggradients/ragas.git \ncd ragas \npip install -e .\n</code></pre> <p>Next, let's construct a synthetic test set using your own data. If you've brought your own test set, you can learn how to evaluate it using Ragas.</p>"},{"location":"getstarted/rag_evaluation/","title":"Evaluate Using Metrics","text":""},{"location":"getstarted/rag_evaluation/#run-ragas-metrics-for-evaluating-rag","title":"Run ragas metrics for evaluating RAG","text":"<p>In this tutorial, we will take a sample test dataset, select a few of the available metrics that Ragas offers, and evaluate a simple RAG pipeline. </p>"},{"location":"getstarted/rag_evaluation/#working-with-data","title":"Working with Data","text":"<p>The dataset used here is from Amnesty QA RAG that contains the necessary data points we need for this tutorial. Here I am loading it from huggingface hub, but you may use file from any source. </p> <pre><code>from datasets import load_dataset\ndataset = load_dataset(\"explodinggradients/amnesty_qa\",\"english_v3\")\n</code></pre> <p>Converting data to ragas evaluation dataset</p> <pre><code>from ragas import EvaluationDataset, SingleTurnSample\n\nsamples = []\nfor row in dataset['eval']:\n    sample = SingleTurnSample(\n        user_input=row['user_input'],\n        reference=row['reference'],\n        response=row['response'],\n        retrieved_contexts=row['retrieved_contexts']\n    )\n    samples.append(sample)\neval_dataset = EvaluationDataset(samples=samples)\n</code></pre>"},{"location":"getstarted/rag_evaluation/#selecting-required-metrics","title":"Selecting required metrics","text":"<p>Ragas offers a wide variety of metrics that one can select from to evaluate LLM applications. You can also build your own metrics on top of ragas. For this tutorial, we will select a few metrics that are commonly used to evaluate single turn RAG systems.</p> <pre><code>from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, SemanticSimilarity\nfrom ragas import evaluate\n</code></pre> <p>Since all of the metrics we have chosen are LLM-based metrics, we need to choose the evaluator LLMs we want to use for evaluation.</p>"},{"location":"getstarted/rag_evaluation/#choosing-evaluator-llm","title":"Choosing evaluator LLM","text":"OpenAIAWS Bedrock <p>This guide utilizes OpenAI for running some metrics, so ensure you have your OpenAI key ready and available in your environment.</p> <p><pre><code>import os\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n</code></pre> Wrapp the LLMs in <code>LangchainLLMWrapper</code> <pre><code>from ragas.llms import LangchainLLMWrapper\nfrom langchain_openai import ChatOpenAI\nevaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n</code></pre></p> <p>First you have to set your AWS credentials and configurations</p> <p><pre><code>config = {\n    \"credentials_profile_name\": \"your-profile-name\",  # E.g \"default\"\n    \"region_name\": \"your-region-name\",  # E.g. \"us-east-1\"\n    \"model_id\": \"your-model-id\",  # E.g \"anthropic.claude-v2\"\n    \"model_kwargs\": {\"temperature\": 0.4},\n}\n</code></pre> define you LLMs <pre><code>from langchain_aws.chat_models import BedrockChat\nfrom ragas.llms import LangchainLLMWrapper\nevaluator_llm = LangchainLLMWrapper(BedrockChat(\n    credentials_profile_name=config[\"credentials_profile_name\"],\n    region_name=config[\"region_name\"],\n    endpoint_url=f\"https://bedrock-runtime.{config['region_name']}.amazonaws.com\",\n    model_id=config[\"model_id\"],\n    model_kwargs=config[\"model_kwargs\"],\n))\n</code></pre></p>"},{"location":"getstarted/rag_evaluation/#running-evaluation","title":"Running Evaluation","text":"<pre><code>metrics = [LLMContextRecall(), FactualCorrectness(), Faithfulness()]\nresults = evaluate(dataset=eval_dataset, metrics=metrics, llm=evaluator_llm,)\n</code></pre>"},{"location":"getstarted/rag_evaluation/#exporting-and-analyzing-results","title":"Exporting and analyzing results","text":"<pre><code>df = result.to_pandas()\ndf.head()\n</code></pre>"},{"location":"getstarted/rag_testset_generation/","title":"Generate Synthetic Testset for RAG","text":""},{"location":"getstarted/rag_testset_generation/#testset-generation-for-rag","title":"Testset Generation for RAG","text":"<p>This simple guide will help you generate a testset for evaluating your RAG pipeline using your own documents.</p>"},{"location":"getstarted/rag_testset_generation/#load-sample-documents","title":"Load Sample Documents","text":"<p>For the sake of this tutorial we will use sample documents from this repository. You can replace this with your own documents.</p> <pre><code>git clone https://huggingface.co/datasets/explodinggradients/Sample_Docs_Markdown\n</code></pre>"},{"location":"getstarted/rag_testset_generation/#load-documents","title":"Load documents","text":"<p>Now we will load the documents from the sample dataset using <code>DirectoryLoader</code>, which is one of document loaders from langchain_community. You may also use any loaders from llama_index</p> <pre><code>from langchain_community.document_loaders import DirectoryLoader\n\npath = \"Sample_Docs_Markdown/\"\nloader = DirectoryLoader(path, glob=\"**/*.md\")\ndocs = loader.load()\n</code></pre>"},{"location":"getstarted/rag_testset_generation/#choose-your-llm","title":"Choose your LLM","text":"<p>You may choose to use any LLM of your choice</p> OpenAIAWS Bedrock <p>This guide utilizes OpenAI for running some metrics, so ensure you have your OpenAI key ready and available in your environment.</p> <p><pre><code>import os\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n</code></pre> Wrapp the LLMs in <code>LangchainLLMWrapper</code> <pre><code>from ragas.llms import LangchainLLMWrapper\nfrom langchain_openai import ChatOpenAI\nevaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n</code></pre></p> <p>First you have to set your AWS credentials and configurations</p> <p><pre><code>config = {\n    \"credentials_profile_name\": \"your-profile-name\",  # E.g \"default\"\n    \"region_name\": \"your-region-name\",  # E.g. \"us-east-1\"\n    \"model_id\": \"your-model-id\",  # E.g \"anthropic.claude-v2\"\n    \"model_kwargs\": {\"temperature\": 0.4},\n}\n</code></pre> define you LLMs <pre><code>from langchain_aws.chat_models import BedrockChat\nfrom ragas.llms import LangchainLLMWrapper\nevaluator_llm = LangchainLLMWrapper(BedrockChat(\n    credentials_profile_name=config[\"credentials_profile_name\"],\n    region_name=config[\"region_name\"],\n    endpoint_url=f\"https://bedrock-runtime.{config['region_name']}.amazonaws.com\",\n    model_id=config[\"model_id\"],\n    model_kwargs=config[\"model_kwargs\"],\n))\n</code></pre></p>"},{"location":"getstarted/rag_testset_generation/#generate-testset","title":"Generate Testset","text":"<p>Now we will run the test generation using the loaded documents and the LLM setup. If you have used <code>llama_index</code> to load documents, please use <code>generate_with_llama_index_docs</code> method instead.</p> <pre><code>from ragas.testset import TestsetGenerator\n\ngenerator = TestsetGenerator(llm=generator_llm)\ndataset = generator.generate_with_langchain_docs(docs, test_size=10)\n</code></pre>"},{"location":"getstarted/rag_testset_generation/#export","title":"Export","text":"<p>You may now export and inspect the generated testset.</p> <pre><code>dataset.to_pandas()\n</code></pre>"},{"location":"howtos/","title":"\ud83d\udee0\ufe0f How-to Guides","text":"<p>Each guide in this section provides a focused solution to real-world problems that you, as an experienced user, may encounter while using Ragas. These guides are designed to be concise and direct, offering quick solutions to your problems. We assume you have a foundational understanding and are comfortable with Ragas concepts. If not, feel free to explore the Get Started section first.</p> <ul> <li> <p> Customization</p> <p>How to customize various aspects of Ragas to suit your needs.</p> <p>Customize features such as Metrics and Testset Generation.</p> </li> <li> <p> Applications</p> <p>How to use Ragas for various applications and use cases.</p> <p>Includes applications such as RAG evaluation.</p> </li> <li> <p> Integrations</p> <p>How to integrate Ragas with other frameworks and observability tools.</p> <p>Use Ragas with frameworks like Langchain, LlamaIndex, and observability tools.</p> </li> </ul>"},{"location":"howtos/applications/","title":"Applications","text":"<p>Ragas in action. Examples of how to use Ragas in various applications and usecases to solve problems you might encounter when your building.</p>"},{"location":"howtos/applications/add_to_ci/","title":"Adding to your CI pipeline with Pytest","text":"<p>You can add Ragas evaluations as part of your Continious Integration pipeline  to keep track of the qualitative performance of your RAG pipeline. Consider these as  part of your end-to-end test suite which you run before major changes and releases.</p> <p>The usage is straight forward but the main things is to set the <code>in_ci</code> argument for the <code>evaluate()</code> function to <code>True</code>. This runs Ragas metrics in a special mode that ensures  it produces more reproducable metrics but will be more costlier.</p> <p>You can easily write a pytest test as follows</p> <p>Note</p> <p>This dataset that is already populated with outputs from a reference RAG When testing your own system make sure you use outputs from RAG pipeline  you want to test. For more information on how to build your datasets check  Building HF <code>Dataset</code> with your own Data docs.</p> <pre><code>import pytest\nfrom datasets import load_dataset\n\nfrom ragas import evaluate\nfrom ragas.metrics import (\n    answer_relevancy,\n    faithfulness,\n    context_recall,\n    context_precision,\n)\n\ndef assert_in_range(score: float, value: float, plus_or_minus: float):\n    \"\"\"\n    Check if computed score is within the range of value +/- max_range\n    \"\"\"\n    assert value - plus_or_minus &lt;= score &lt;= value + plus_or_minus\n\n\ndef test_amnesty_e2e():\n    # loading the V2 dataset\n    amnesty_qa = load_dataset(\"explodinggradients/amnesty_qa\", \"english_v2\")[\"eval\"]\n\n\n    result = evaluate(\n        amnesty_qa,\n        metrics=[answer_relevancy, faithfulness, context_recall, context_precision],\n        in_ci=True,\n    )\n    assert result[\"answer_relevancy\"] &gt;= 0.9\n    assert result[\"context_recall\"] &gt;= 0.95\n    assert result[\"context_precision\"] &gt;= 0.95\n    assert_in_range(result[\"faithfulness\"], value=0.4, plus_or_minus=0.1)\n</code></pre>"},{"location":"howtos/applications/add_to_ci/#using-pytest-markers-for-ragas-e2e-tests","title":"Using Pytest Markers for Ragas E2E tests","text":"<p>Because these are long end-to-end test one thing that you can leverage is Pytest Markers which help you mark your tests with special tags. It is recommended to mark Ragas tests with special tags so you can run them only when needed.</p> <p>To add a new <code>ragas_ci</code> tag to pytest add the following to your <code>conftest.py</code> <pre><code>def pytest_configure(config):\n    \"\"\"\n    configure pytest\n    \"\"\"\n    # add `ragas_ci`\n    config.addinivalue_line(\n        \"markers\", \"ragas_ci: Set of tests that will be run as part of Ragas CI\"\n    )\n</code></pre></p> <p>now you can use <code>ragas_ci</code> to mark all the tests that are part of Ragas CI.</p> <pre><code>import pytest\nfrom datasets import load_dataset\n\nfrom ragas import evaluate\nfrom ragas.metrics import (\n    answer_relevancy,\n    faithfulness,\n    context_recall,\n    context_precision,\n)\n\ndef assert_in_range(score: float, value: float, plus_or_minus: float):\n    \"\"\"\n    Check if computed score is within the range of value +/- max_range\n    \"\"\"\n    assert value - plus_or_minus &lt;= score &lt;= value + plus_or_minus\n\n\n@pytest.mark.ragas_ci\ndef test_amnesty_e2e():\n    # loading the V2 dataset\n    amnesty_qa = load_dataset(\"explodinggradients/amnesty_qa\", \"english_v2\")[\"eval\"]\n\n\n    result = evaluate(\n        amnesty_qa,\n        metrics=[answer_relevancy, faithfulness, context_recall, context_precision],\n        in_ci=True,\n    )\n    assert result[\"answer_relevancy\"] &gt;= 0.9\n    assert result[\"context_recall\"] &gt;= 0.95\n    assert result[\"context_precision\"] &gt;= 0.95\n    assert_in_range(result[\"faithfulness\"], value=0.4, plus_or_minus=0.1)\n</code></pre>"},{"location":"howtos/applications/compare_embeddings/","title":"Compare Embeddings for retriever","text":"<p>The performance of the retriever is a critical and influential factor that determines the overall effectiveness of a Retrieval Augmented Generation (RAG) system. In particular, the quality of the embeddings used plays a pivotal role in determining the quality of the retrieved content.</p> <p>This tutorial notebook provides a step-by-step guide on how to compare and choose the most suitable embeddings for your own data using the Ragas library.</p> Compare Embeddings"},{"location":"howtos/applications/compare_embeddings/#create-synthetic-test-data","title":"Create synthetic test data","text":"<p>Tip</p> <p>Ragas can also work with your dataset. Refer to data preparation to see how you can use your dataset with ragas. </p> <p>Ragas offers a unique test generation paradigm that enables the creation of evaluation datasets specifically tailored to your retrieval and generation tasks. Unlike traditional QA generators, Ragas can generate a wide variety of challenging test cases from your document corpus.</p> <p>Tip</p> <p>Refer to testset generation to know more on how it works.</p> <p>For this tutorial notebook, I am using papers from Semantic Scholar that is related to large language models to build RAG.</p> <pre><code>from llama_index.core import download_loader\nfrom ragas.testset.evolutions import simple, reasoning, multi_context\nfrom ragas.testset.generator import TestsetGenerator\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\nSemanticScholarReader = download_loader(\"SemanticScholarReader\")\nloader = SemanticScholarReader()\nquery_space = \"large language models\"\ndocuments = loader.load_data(query=query_space, limit=100)\n\n# generator with openai models\ngenerator_llm = ChatOpenAI(model=\"gpt-4o-mini\")\ncritic_llm = ChatOpenAI(model=\"gpt-4o\")\nembeddings = OpenAIEmbeddings()\n\ngenerator = TestsetGenerator.from_langchain(\n    generator_llm,\n    critic_llm,\n    embeddings\n)\n\n\ndistributions = {\n    simple: 0.5,\n    multi_context: 0.4,\n    reasoning: 0.1\n}\n\n# generate testset\ntestset = generator.generate_with_llamaindex_docs(documents, 100,distributions)\ntest_df = testset.to_pandas()\n</code></pre> Test Outputs <pre><code>test_questions = test_df['question'].values.tolist()\ntest_answers = [[item] for item in test_df['answer'].values.tolist()]\n</code></pre>"},{"location":"howtos/applications/compare_embeddings/#build-your-rag","title":"Build your RAG","text":"<p>Here I am using llama-index to build a basic RAG pipeline with my documents. The goal here is to collect retrieved contexts and generated answer for each of the test questions from your pipeline. Ragas has integrations with various RAG frameworks which makes evaluating them easier using ragas.</p> <p>Note</p> <p>refer to langchain-tutorial see how to evaluate using langchain</p> <pre><code>import nest_asyncio\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\nfrom langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings\nimport pandas as pd\n\nnest_asyncio.apply()\n\n\ndef build_query_engine(embed_model):\n    vector_index = VectorStoreIndex.from_documents(\n        documents, service_context=ServiceContext.from_defaults(chunk_size=512),\n        embed_model=embed_model,\n    )\n\n    query_engine = vector_index.as_query_engine(similarity_top_k=2)\n    return query_engine\n</code></pre>"},{"location":"howtos/applications/compare_embeddings/#import-metrics-from-ragas","title":"Import metrics from ragas","text":"<p>Here we are importing metrics that are required to evaluate retriever component.</p> <pre><code>from ragas.metrics import (\n    context_precision,\n    context_recall,\n)\n\nmetrics = [\n    context_precision,\n    context_recall,\n]\n</code></pre>"},{"location":"howtos/applications/compare_embeddings/#evaluate-openai-embeddings","title":"Evaluate OpenAI embeddings","text":"<pre><code>from ragas.llama_index import evaluate\n\nopenai_model = OpenAIEmbedding()\nquery_engine1 = build_query_engine(openai_model)\nresult = evaluate(query_engine1, metrics, test_questions, test_answers)\n</code></pre> <pre><code>{'context_precision': 0.2378, 'context_recall': 0.7159}\n</code></pre>"},{"location":"howtos/applications/compare_embeddings/#evaluate-bge-embeddings","title":"Evaluate Bge embeddings","text":"<pre><code>from ragas.llama_index import evaluate\n\nflag_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\nquery_engine2 = build_query_engine(flag_model)\nresult = evaluate(query_engine2, metrics, test_questions, test_answers)\n</code></pre> <pre><code>{'context_precision': 0.2655, 'context_recall': 0.7227}\n</code></pre>"},{"location":"howtos/applications/compare_embeddings/#compare-scores","title":"Compare Scores","text":"<p>Based on the evaluation results, it is apparent that the <code>context_precision</code> and <code>context_recall</code> metrics of the BGE model slightly outperform the OpenAI-Ada model in my RAG pipeline when applied to my own dataset. </p> <p>For any further analysis of the scores you can export the results to pandas</p> <pre><code>result_df = result.to_pandas()\nresult_df.head()\n</code></pre> Compare Embeddings Results"},{"location":"howtos/applications/compare_llms/","title":"Compare LLMs using Ragas Evaluations","text":"<p>The llm used in the Retrieval Augmented Generation (RAG) system has a major impact in the quality of the generated output. Evaluating the results generated by different llms can give an idea about the right llm to use for a particular use case.</p> <p>This tutorial notebook provides a step-by-step guide on how to compare and choose the most suitable llm for your own data using the Ragas library.</p> Compare LLMs"},{"location":"howtos/applications/compare_llms/#create-synthetic-test-data","title":"Create synthetic test data","text":"<p>Tip</p> <p>Ragas can also work with your dataset. Refer to data preparation to see how you can use your dataset with ragas. </p> <p>Ragas offers a unique test generation paradigm that enables the creation of evaluation datasets specifically tailored to your retrieval and generation tasks. Unlike traditional QA generators, Ragas can generate a wide variety of challenging test cases from your document corpus.</p> <p>Tip</p> <p>Refer to testset generation to know more on how it works.</p> <p>For this tutorial notebook, I am using papers from Arxiv that is related to large language models to build RAG.</p> <p>Note</p> <p>Generate a set of 50+ samples using Testset generator for better results</p> <pre><code>import os\nfrom llama_index import download_loader, SimpleDirectoryReader\nfrom ragas.testset import TestsetGenerator\nfrom ragas.testset.generator import TestsetGenerator\nfrom ragas.testset.evolutions import simple, reasoning, multi_context\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\nos.environ['OPENAI_API_KEY'] = 'Your OPEN AI key'\n\n# load documents\nreader = SimpleDirectoryReader(\"./arxiv-papers/\",num_files_limit=30)\ndocuments = reader.load_data()\n\n# generator with openai models\ngenerator_llm = ChatOpenAI(model=\"gpt-4o-mini\")\ncritic_llm = ChatOpenAI(model=\"gpt-4o\")\nembeddings = OpenAIEmbeddings()\n\ngenerator = TestsetGenerator.from_langchain(\n    generator_llm,\n    critic_llm,\n    embeddings\n)\n\ndistributions = {\n    simple: 0.5,\n    multi_context: 0.4,\n    reasoning: 0.1\n}\n\n# generate testset\ntestset = generator.generate_with_llama_index_docs(documents, 100,distributions)\ntestset.to_pandas()\n</code></pre> <p> </p> <pre><code>test_questions = test_df['question'].values.tolist()\ntest_answers = [[item] for item in test_df['answer'].values.tolist()]\n</code></pre>"},{"location":"howtos/applications/compare_llms/#build-your-rag","title":"Build your RAG","text":"<p>Here I am using llama-index to build a basic RAG pipeline with my documents. The goal here is to collect retrieved contexts and generated answer for each of the test questions from your pipeline. Ragas has integrations with various RAG frameworks which makes evaluating them easier using ragas.</p> <p>Note</p> <p>refer to langchain-tutorial see how to evaluate using langchain</p> <pre><code>import nest_asyncio\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\nfrom llama_index.llms import HuggingFaceInferenceAPI\nfrom llama_index.embeddings import HuggingFaceInferenceAPIEmbedding\nimport pandas as pd\n\nnest_asyncio.apply()\n\n\ndef build_query_engine(llm):\n    vector_index = VectorStoreIndex.from_documents(\n        documents, service_context=ServiceContext.from_defaults(chunk_size=512, llm=llm),\n        embed_model=HuggingFaceInferenceAPIEmbedding,\n    )\n\n    query_engine = vector_index.as_query_engine(similarity_top_k=2)\n    return query_engine\n\n# Function to evaluate as Llama index does not support async evaluation for HFInference API\ndef generate_responses(query_engine, test_questions, test_answers):\n  responses = [query_engine.query(q) for q in test_questions]\n\n  answers = []\n  contexts = []\n  for r in responses:\n    answers.append(r.response)\n    contexts.append([c.node.get_content() for c in r.source_nodes])\n  dataset_dict = {\n        \"question\": test_questions,\n        \"answer\": answers,\n        \"contexts\": contexts,\n  }\n  if test_answers is not None:\n    dataset_dict[\"ground_truth\"] = test_answers\n  ds = Dataset.from_dict(dataset_dict)\n  return ds\n</code></pre>"},{"location":"howtos/applications/compare_llms/#import-metrics-from-ragas","title":"Import metrics from ragas","text":"<p>Here we are importing metrics that are required to evaluate retriever component.</p> <pre><code>from datasets import Dataset\nfrom ragas import evaluate\nfrom ragas.metrics import (\n    faithfulness,\n    answer_relevancy,\n    answer_correctness,\n)\n\nmetrics = [\n    faithfulness,\n    answer_relevancy,\n    answer_correctness,\n]\n</code></pre>"},{"location":"howtos/applications/compare_llms/#evaluate-zephyr-7b-alpha-llm","title":"Evaluate Zephyr 7B Alpha LLM","text":"<p>For the first llm, I will be using HuggingFace zephyr-7b-alpha. I am using HuggingFaceInferenceAPI to generate answers using the model. HuggingFaceInferenceAPI is free to use and token can be setup using HuggingFaceToken.</p> <pre><code># Use zephyr model using HFInference API\nzephyr_llm = HuggingFaceInferenceAPI(\n    model_name=\"HuggingFaceH4/zephyr-7b-alpha\",\n    token=\"Your Hugging Face token\"\n)\nquery_engine1 = build_query_engine(zephyr_llm)\nresult_ds = generate_responses(query_engine1, test_questions, test_answers)\nresult_zephyr = evaluate(\n    result_ds,\n    metrics=metrics,\n)\n\nresult_zephyr\n</code></pre> <pre><code>{'faithfulness': 0.8365, 'answer_relevancy': 0.8831, 'answer_correctness': 0.6605}\n</code></pre>"},{"location":"howtos/applications/compare_llms/#evaluate-falcon-7b-instruct-llm","title":"Evaluate Falcon-7B-Instruct LLM","text":"<p>For the second model to evaluate, I am using Falcon-7B-Instruct. This can also be used with the HuggingFaceInferenceAPI.</p> <pre><code>falcon_llm = HuggingFaceInferenceAPI(\n    model_name=\"tiiuae/falcon-7b-instruct\",\n    token=\"Your Huggingface token\"\n)\nquery_engine2 = build_query_engine(falcon_llm)\nresult_ds_falcon = generate_responses(query_engine2, test_questions, test_answers)\nresult = evaluate(\n    result_ds_falcon,\n    metrics=metrics,\n)\n\nresult\n</code></pre> <pre><code>{'faithfulness': 0.6909, 'answer_relevancy': 0.8651, 'answer_correctness': 0.5850}\n</code></pre>"},{"location":"howtos/applications/compare_llms/#compare-scores","title":"Compare Scores","text":"<p>Based on the evaluation results, it is apparent that the <code>faithfulness</code>, <code>answer_correctness</code> and <code>answer_relevancy</code> metrics of the HuggingFace zephyr-7b-alpha model slightly outperform the falcon-7b-instruct model in my RAG pipeline when applied to my own dataset.</p> <p>Refer to the complete colab notebook here.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\ndef analysis(zephyr_df, falcon_df):\n  sns.set_style(\"whitegrid\")\n  fig, axs = plt.subplots(1,3, figsize=(12, 5))\n  for i,col in enumerate(zephyr_df.columns):\n    sns.kdeplot(data=[zephyr_df[col].values,falcon_df[col].values],legend=False,ax=axs[i],fill=True)\n    axs[i].set_title(f'{col} scores distribution')\n    axs[i].legend(labels=[\"zephyr\", \"falcon\"])\n  plt.tight_layout()\n  plt.show()\n\nresult_zephyr_df = result_zephyr.to_pandas()\nresult_falcon_df = result.to_pandas()\nanalysis(\n    result_zephyr_df[['faithfulness', 'answer_relevancy', 'answer_correctness']],\n    result_falcon_df[['faithfulness', 'answer_relevancy', 'answer_correctness']]\n) \n</code></pre>"},{"location":"howtos/applications/compare_llms/#score-distribution-analysis","title":"Score distribution analysis","text":"Compare LLMs"},{"location":"howtos/applications/cost/","title":"Understand Cost and Usage of Operations","text":"In\u00a0[1]: Copied! <pre>from langchain_openai.chat_models import ChatOpenAI\nfrom langchain_core.prompt_values import StringPromptValue\n\ngpt4o = ChatOpenAI(model=\"gpt-4o\")\np = StringPromptValue(text=\"hai there\")\nllm_result = gpt4o.generate_prompt([p])\n\n# lets import a parser for OpenAI\nfrom ragas.cost import get_token_usage_for_openai\n\nget_token_usage_for_openai(llm_result)\n</pre> from langchain_openai.chat_models import ChatOpenAI from langchain_core.prompt_values import StringPromptValue  gpt4o = ChatOpenAI(model=\"gpt-4o\") p = StringPromptValue(text=\"hai there\") llm_result = gpt4o.generate_prompt([p])  # lets import a parser for OpenAI from ragas.cost import get_token_usage_for_openai  get_token_usage_for_openai(llm_result) Out[1]: <pre>TokenUsage(input_tokens=9, output_tokens=9, model='')</pre> <p>You can define your own or import parsers if they are defined. If you would like to suggest parser for LLM providers or contribute your own ones please check out this issue \ud83d\ude42.</p> <p>You can use it for evaluations as so. Using example from get started here.</p> In\u00a0[2]: Copied! <pre>from datasets import load_dataset\nfrom ragas.metrics import (\n    answer_relevancy,\n    faithfulness,\n    context_recall,\n    context_precision,\n)\n\namnesty_qa = load_dataset(\"explodinggradients/amnesty_qa\", \"english_v2\")\namnesty_qa\n</pre> from datasets import load_dataset from ragas.metrics import (     answer_relevancy,     faithfulness,     context_recall,     context_precision, )  amnesty_qa = load_dataset(\"explodinggradients/amnesty_qa\", \"english_v2\") amnesty_qa <pre>Repo card metadata block was not found. Setting CardData to empty.\n</pre> Out[2]: <pre>DatasetDict({\n    eval: Dataset({\n        features: ['question', 'ground_truth', 'answer', 'contexts'],\n        num_rows: 20\n    })\n})</pre> In\u00a0[3]: Copied! <pre>from ragas import evaluate\nfrom ragas.cost import get_token_usage_for_openai\n\nresult = evaluate(\n    amnesty_qa[\"eval\"],\n    metrics=[\n        context_precision,\n        faithfulness,\n        answer_relevancy,\n        context_recall,\n    ],\n    llm=gpt4o,\n    token_usage_parser=get_token_usage_for_openai,\n)\n</pre> from ragas import evaluate from ragas.cost import get_token_usage_for_openai  result = evaluate(     amnesty_qa[\"eval\"],     metrics=[         context_precision,         faithfulness,         answer_relevancy,         context_recall,     ],     llm=gpt4o,     token_usage_parser=get_token_usage_for_openai, ) <pre>Evaluating:   0%|          | 0/80 [00:00&lt;?, ?it/s]</pre> In\u00a0[4]: Copied! <pre>result.total_tokens()\n</pre> result.total_tokens() Out[4]: <pre>TokenUsage(input_tokens=116765, output_tokens=39031, model='')</pre> <p>You can compute the cost for each run by passing in the cost per token to <code>Result.total_cost()</code> function.</p> <p>In this case GPT-4o costs $5 for 1M input tokens and $15 for 1M output tokens.</p> In\u00a0[5]: Copied! <pre>result.total_cost(cost_per_input_token=5 / 1e6, cost_per_output_token=15 / 1e6)\n</pre> result.total_cost(cost_per_input_token=5 / 1e6, cost_per_output_token=15 / 1e6) Out[5]: <pre>1.1692900000000002</pre>"},{"location":"howtos/applications/cost/#understand-cost-and-usage-of-operations","title":"Understand Cost and Usage of Operations\u00b6","text":"<p>When using LLMs for evaluation and test set generation, cost will be an important factor. Ragas provides you some tools to help you with that.</p>"},{"location":"howtos/applications/cost/#understanding-tokenusageparser","title":"Understanding <code>TokenUsageParser</code>\u00b6","text":"<p>By default Ragas does not calculate the usage of tokens for <code>evaluate()</code>. This is because langchain's LLMs do not always return information about token usage in a uniform way. So in order to get the usage data, we have to implement a <code>TokenUsageParser</code>.</p> <p>A <code>TokenUsageParser</code> is function that parses the <code>LLMResult</code> or <code>ChatResult</code> from langchain models <code>generate_prompt()</code> function and outputs <code>TokenUsage</code> which Ragas expects.</p> <p>For an example here is one that will parse OpenAI by using a parser we have defined.</p>"},{"location":"howtos/applications/custom_prompts/","title":"Write custom prompts with ragas","text":"<p>This is a tutorial notebook that shows how to create and use custom prompts with the metrics used in the evaluation task. This is achieved using Ragas prompt class. This tutorial will guide you to change and use different prompts with the Ragas metrics instead of the default ones used.</p>"},{"location":"howtos/applications/custom_prompts/#dataset","title":"Dataset","text":"<p>Here I\u2019m using a dataset from HuggingFace.</p> <pre><code>from datasets import load_dataset, Dataset\n\namnesty_dataset = load_dataset(\"explodinggradients/amnesty_qa\", \"english\")\namnesty_dataset\n</code></pre> <pre><code>DatasetDict({\n    train: Dataset({\n        features: ['question', 'ground_truth', 'answer', 'contexts'],\n        num_rows: 20\n    })\n})\n</code></pre>"},{"location":"howtos/applications/custom_prompts/#create-a-custom-prompt-object","title":"Create a Custom Prompt Object","text":"<p>Create a new Prompt object to be used in the metric for the evaluation task. For this task, I will be instantiating an object of the Ragas Prompt class.</p> <pre><code>from ragas.llms.prompt import Prompt\n\nlong_form_answer_prompt_new = Prompt(\n    name=\"long_form_answer_new_v1\",\n    instruction=\"Create one or more statements from each sentence in the given answer.\",\n    examples=[\n        {\n            \"question\": \"Which is the only planet in the solar system that has life on it?\",\n            \"answer\": \"earth\",\n            \"statements\": {\n                \"statements\": [\n                    \"Earth is the only planet in the solar system that has life on it.\"\n                ]\n            },\n        },\n        {\n            \"question\": \"Were Hitler and Benito Mussolini of the same nationality?\",\n            \"answer\": \"Sorry, I can't provide an answer to that question.\",\n            \"statements\": {\n                \"statements\": []\n            },\n        },\n    ],\n    input_keys=[\"question\", \"answer\"],\n    output_key=\"statements\",\n    output_type=\"json\",\n)\n</code></pre>"},{"location":"howtos/applications/custom_prompts/#using-the-custom-prompt-in-evaluations","title":"Using the Custom Prompt in Evaluations","text":"<p>I will be using the faithfulness metric for my evaluation task. Faithfulness uses two default prompts <code>long_form_answer_prompt</code> and <code>nli_statements_message</code> for evaluations. I will be changing the default <code>long_form_answer_prompt</code> used in this metric to the newly created prompt object.</p> <pre><code>from ragas.metrics import faithfulness\n\nfaithfulness.long_form_answer_prompt = long_form_answer_prompt_new\nprint(faithfulness.long_form_answer_prompt.to_string())\n</code></pre> <pre><code>Create one or more statements from each sentence in the given answer.\n\nquestion: \"Which is the only planet in the solar system that has life on it?\"\nanswer: \"earth\"\nstatements: {{\"statements\": [\"Earth is the only planet in the solar system that has life on it.\"]}}\n\nquestion: \"Were Hitler and Benito Mussolini of the same nationality?\"\nanswer: \"Sorry, I can't provide an answer to that question.\"\nstatements: {{\"statements\": []}}\n\nquestion: {question}\nanswer: {answer}\nstatements:\n</code></pre> <p>Now the custom prompt that we created is being used in the faithfulness metric. We can now evaluate the dataset against the metric that uses the new prompt that we created.</p> <p><pre><code>from ragas import evaluate\n\nresult = evaluate(\n    dataset[\"train\"].select(range(3)), # selecting only 3\n    metrics=[\n        faithfulness\n    ],\n)\n\nresult\n</code></pre> <pre><code>evaluating with [faithfulness]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [02:31&lt;00:00, 151.79s/it]\n\n{'faithfulness': 0.7879}\n</code></pre></p>"},{"location":"howtos/applications/data_preparation/","title":"Dataset Preparation","text":"<p>This tutorial notebook provides a step-by-step guide on how to prepare data for experimenting and evaluating using ragas. </p> <p>Note</p> <p>If you're using popular frameworks like llama-index, langchain, etc to build your RAG application, Ragas provides integrations with these frameworks. Checkout integrations</p> <p>This tutorial assumes that you have the 4 required data points from your RAG pipeline 1. Question: A set of questions.  2. Contexts: Retrieved contexts corresponding to each question. This is a <code>list[list]</code> since each question can retrieve multiple text chunks. 3. Answer: Generated answer corresponding to each question. 4. Ground truths: Ground truths corresponding to each question. This is a <code>str</code> which corresponds to the expected answer for each question.</p>"},{"location":"howtos/applications/data_preparation/#example-dataset","title":"Example dataset","text":"<pre><code>from datasets import Dataset \n\ndata_samples = {\n    'question': ['When was the first super bowl?', 'Who won the most super bowls?'],\n    'answer': ['The first superbowl was held on January 15, 1967', 'The most super bowls have been won by The New England Patriots'],\n    'contexts' : [['The Super Bowl....season since 1966,','replacing the NFL...in February.'], \n    ['The Green Bay Packers...Green Bay, Wisconsin.','The Packers compete...Football Conference']],\n    'ground_truth': ['The first superbowl was held on January 15, 1967', 'The New England Patriots have won the Super Bowl a record six times']\n}\ndataset = Dataset.from_dict(data_samples)\n</code></pre>"},{"location":"howtos/applications/tracing/","title":"Explainability through Logging and tracing","text":"<p>Logging and tracing results from llm are important for any language model-based application. This is a tutorial on how to do tracing with Ragas. Ragas provides <code>callbacks</code> functionality which allows you to hook various tracers like Langmsith, wandb, Opik, etc easily.  In this notebook, I will be using Langmith for tracing.</p> <p>To set up Langsmith, we need to set some environment variables that it needs. For more information, you can refer to the docs</p> <pre><code>export LANGCHAIN_TRACING_V2=true\nexport LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\nexport LANGCHAIN_API_KEY=&lt;your-api-key&gt;\nexport LANGCHAIN_PROJECT=&lt;your-project&gt;  # if not specified, defaults to \"default\"\n</code></pre> <p>Now we have to import the required tracer from langchain, here we are using <code>LangChainTracer</code> but you can similarly use any tracer supported by langchain like WandbTracer or OpikTracer</p> <pre><code># langsmith\nfrom langchain.callbacks.tracers import LangChainTracer\n\ntracer = LangChainTracer(project_name=\"callback-experiments\")\n</code></pre> <p>We now pass the tracer to the <code>callbacks</code> parameter when calling <code>evaluate</code></p> <pre><code>from datasets import load_dataset\nfrom ragas.metrics import context_precision\nfrom ragas import evaluate\n\ndataset = load_dataset(\"explodinggradients/amnesty_qa\",\"english\")\nevaluate(dataset[\"train\"],metrics=[context_precision],callbacks=[tracer])\n</code></pre> <pre><code>{'context_precision': 1.0000}\n</code></pre> Tracing with Langsmith <p>You can also write your own custom callbacks using langchain\u2019s <code>BaseCallbackHandler</code>, refer here to read more about it.</p>"},{"location":"howtos/applications/use_prompt_adaptation/","title":"Automatic language adaptation","text":"<ol> <li>Metrics</li> <li>Testset generation</li> </ol>"},{"location":"howtos/applications/use_prompt_adaptation/#language-adaptation-for-metrics","title":"Language Adaptation for Metrics","text":"<p>This is a tutorial notebook showcasing how to successfully use ragas with data from any given language. This is achieved using Ragas prompt adaptation feature. The tutorial specifically applies ragas metrics to a Hindi RAG evaluation dataset.</p>"},{"location":"howtos/applications/use_prompt_adaptation/#dataset","title":"Dataset","text":"<p>Here I\u2019m using a dataset containing all the relevant columns in Hindi language. </p> <pre><code>from datasets import load_dataset, Dataset\n\nhindi_dataset = load_dataset(\"explodinggradients/amnesty_qa\",\"hindi\")\nhindi_dataset\n</code></pre> <pre><code>DatasetDict({\n    train: Dataset({\n        features: ['question', 'ground_truth', 'answer', 'contexts'],\n        num_rows: 20\n    })\n})\n</code></pre>"},{"location":"howtos/applications/use_prompt_adaptation/#adapt-metrics-to-target-language","title":"Adapt metrics to target language","text":"<p>Import any metrics from Ragas as required and adapt and save each one of them to the target language using <code>adapt</code> function. Optionally you can also specify which llm to use for prompt adaptation, here I am using <code>gpt-4</code>. It is highly recommended to use the best llm here as quality of adapted prompts highly influence the results. </p> <pre><code>from ragas.metrics import (\n    faithfulness,\n    answer_correctness,\n)\nfrom langchain.chat_models import ChatOpenAI\nfrom ragas import adapt\n\n# llm used for adaptation\nopenai_model = ChatOpenAI(model_name=\"gpt-4\")\n\nadapt(metrics=[faithfulness,answer_correctness], language=\"hindi\", llm=openai_model)\n</code></pre> <p>The prompts belonging to respective metrics will be now automatically adapted to the target language. The save step saves it to <code>.cacha/ragas</code> by default to reuse later.  Next time when you do adapt with the same metrics, ragas first checks if the adapted prompt is already present in the cache. </p> <p>Let\u2019s inspect the adapted prompt belonging to the answer correctness metric</p> <p>Note</p> <p>When adapting prompts, it is recommended to review them manually prior to evaluation, as language models may introduce errors during translation</p> <pre><code>print(answer_correctness.correctness_prompt.to_string())\n</code></pre> <pre><code>Extract the following from the given question and the ground truth\n\nquestion: \"\u0938\u0942\u0930\u091c \u0915\u094b \u0915\u094d\u092f\u093e \u091a\u0932\u093e\u0924\u093e \u0939\u0948 \u0914\u0930 \u0907\u0938\u0915\u093e \u092a\u094d\u0930\u093e\u0925\u092e\u093f\u0915 \u0915\u093e\u0930\u094d\u092f \u0915\u094d\u092f\u093e \u0939\u0948?\"\nanswer: \"\u0938\u0942\u0930\u094d\u092f \u092a\u0930\u092e\u093e\u0923\u0941 \u0935\u093f\u0918\u091f\u0928 \u0926\u094d\u0935\u093e\u0930\u093e \u0938\u0902\u091a\u093e\u0932\u093f\u0924 \u0939\u094b\u0924\u093e \u0939\u0948, \u091c\u094b \u092a\u0943\u0925\u094d\u0935\u0940 \u092a\u0930 \u092a\u0930\u092e\u093e\u0923\u0941 \u0930\u093f\u090f\u0915\u094d\u091f\u0930\u094b\u0902 \u0915\u0947 \u0938\u092e\u093e\u0928 \u0939\u094b\u0924\u0947 \u0939\u0948\u0902, \u0914\u0930 \u0907\u0938\u0915\u093e \u092a\u094d\u0930\u093e\u0925\u092e\u093f\u0915 \u0915\u093e\u0930\u094d\u092f \u0938\u094c\u0930\u092e\u0902\u0921\u0932 \u0915\u094b \u092a\u094d\u0930\u0915\u093e\u0936 \u092a\u094d\u0930\u0926\u093e\u0928 \u0915\u0930\u0928\u093e \u0939\u0948\u0964\"\nground_truth: \"\u0938\u0942\u0930\u094d\u092f \u0935\u093e\u0938\u094d\u0924\u0935 \u092e\u0947\u0902 \u092a\u0930\u092e\u093e\u0923\u0941 \u0938\u0902\u092f\u094b\u091c\u0928 \u0926\u094d\u0935\u093e\u0930\u093e \u091a\u0932\u093e\u092f\u093e \u091c\u093e\u0924\u093e \u0939\u0948, \u0928 \u0915\u093f \u0935\u093f\u0916\u0902\u0921\u0928 \u0926\u094d\u0935\u093e\u0930\u093e\u0964 \u0907\u0938\u0915\u0947 \u0915\u0947\u0902\u0926\u094d\u0930 \u092e\u0947\u0902, \u0939\u093e\u0907\u0921\u094d\u0930\u094b\u091c\u0928 \u092a\u0930\u092e\u093e\u0923\u0941 \u0939\u0940\u0932\u093f\u092f\u092e \u092c\u0928\u093e\u0928\u0947 \u0915\u0947 \u0932\u093f\u090f \u092e\u093f\u0932 \u091c\u093e\u0924\u0947 \u0939\u0948\u0902, \u091c\u093f\u0938\u0938\u0947 \u092c\u0939\u0941\u0924 \u0905\u0927\u093f\u0915 \u090a\u0930\u094d\u091c\u093e \u092e\u0941\u0915\u094d\u0924 \u0939\u094b\u0924\u0940 \u0939\u0948\u0964 \u092f\u0939 \u090a\u0930\u094d\u091c\u093e \u0939\u0940 \u0938\u0942\u0930\u094d\u092f \u0915\u094b \u091c\u0932\u093e\u0924\u0940 \u0939\u0948 \u0914\u0930 \u091c\u0940\u0935\u0928 \u0915\u0947 \u0932\u093f\u090f \u092e\u0939\u0924\u094d\u0935\u092a\u0942\u0930\u094d\u0923 \u0924\u093e\u092a \u0914\u0930 \u092a\u094d\u0930\u0915\u093e\u0936 \u092a\u094d\u0930\u0926\u093e\u0928 \u0915\u0930\u0924\u0940 \u0939\u0948\u0964 \u0938\u0942\u0930\u094d\u092f \u0915\u093e \u092a\u094d\u0930\u0915\u093e\u0936 \u092d\u0942\u092e\u093f \u0915\u0940 \u091c\u0932\u0935\u093e\u092f\u0941 \u092a\u094d\u0930\u0923\u093e\u0932\u0940 \u092e\u0947\u0902 \u092d\u0940 \u092e\u0939\u0924\u094d\u0935\u092a\u0942\u0930\u094d\u0923 \u092d\u0942\u092e\u093f\u0915\u093e \u0928\u093f\u092d\u093e\u0924\u093e \u0939\u0948 \u0914\u0930 \u092e\u094c\u0938\u092e \u0914\u0930 \u0938\u092e\u0941\u0926\u094d\u0930\u0940 \u0927\u093e\u0930\u093e\u0913\u0902 \u0915\u094b \u091a\u0932\u093e\u0928\u0947 \u092e\u0947\u0902 \u092e\u0926\u0926 \u0915\u0930\u0924\u093e \u0939\u0948\u0964\"\nExtracted statements: [{{\"statements that are present in both the answer and the ground truth\": [\"\u0938\u0942\u0930\u094d\u092f \u0915\u093e \u092a\u094d\u0930\u093e\u0925\u092e\u093f\u0915 \u0915\u093e\u0930\u094d\u092f \u092a\u094d\u0930\u0915\u093e\u0936 \u092a\u094d\u0930\u0926\u093e\u0928 \u0915\u0930\u0928\u093e \u0939\u0948\"], \"statements present in the answer but not found in the ground truth\": [\"\u0938\u0942\u0930\u094d\u092f \u092a\u093e\u0930\u092e\u093e\u0923\u0941 \u0935\u093f\u0916\u0902\u0921\u0928 \u0926\u094d\u0935\u093e\u0930\u093e \u0938\u0902\u091a\u093e\u0932\u093f\u0924 \u0939\u094b\u0924\u093e \u0939\u0948\", \"\u092a\u0943\u0925\u094d\u0935\u0940 \u092a\u0930 \u092a\u093e\u0930\u092e\u093e\u0923\u0941 \u0930\u093f\u090f\u0915\u094d\u091f\u0930\u094b\u0902 \u0915\u0947 \u0938\u092e\u093e\u0928\"], \"relevant statements found in the ground truth but omitted in the answer\": [\"\u0938\u0942\u0930\u094d\u092f \u092a\u093e\u0930\u092e\u093e\u0923\u0941 \u0938\u0902\u092f\u094b\u091c\u0928 \u0926\u094d\u0935\u093e\u0930\u093e \u0938\u0902\u091a\u093e\u0932\u093f\u0924 \u0939\u094b\u0924\u093e \u0939\u0948, \u0928 \u0915\u093f \u0935\u093f\u0916\u0902\u0921\u0928\", \"\u0907\u0938\u0915\u0947 \u0915\u094b\u0930 \u092e\u0947\u0902, \u0939\u093e\u0907\u0921\u094d\u0930\u094b\u091c\u0928 \u092a\u0930\u092e\u093e\u0923\u0941 \u0939\u0940\u0932\u093f\u092f\u092e \u092c\u0928\u093e\u0928\u0947 \u0915\u0947 \u0932\u093f\u090f \u092e\u093f\u0932\u0915\u0930 \u0924\u0947\u091c\u0940 \u0938\u0947 \u091c\u0932\u0924\u0947 \u0939\u0948\u0902, \u091c\u093f\u0938\u0938\u0947 \u092c\u0939\u0941\u0924\u093e\u092f\u0924 \u090a\u0930\u094d\u091c\u093e \u092e\u0941\u0915\u094d\u0924 \u0939\u094b\u0924\u0940 \u0939\u0948\", \"\u092f\u0939 \u090a\u0930\u094d\u091c\u093e \u091c\u0940\u0935\u0928 \u0915\u0947 \u0932\u093f\u090f \u0906\u0935\u0936\u094d\u092f\u0915 \u0917\u0930\u094d\u092e\u0940 \u0914\u0930 \u092a\u094d\u0930\u0915\u093e\u0936 \u092a\u094d\u0930\u0926\u093e\u0928 \u0915\u0930\u0924\u0940 \u0939\u0948\", \"\u0938\u0942\u0930\u094d\u092f \u0915\u093e \u092a\u094d\u0930\u0915\u093e\u0936 \u092a\u0943\u0925\u094d\u0935\u0940 \u0915\u0940 \u091c\u0932\u0935\u093e\u092f\u0941 \u092a\u094d\u0930\u0923\u093e\u0932\u0940 \u092e\u0947\u0902 \u092e\u0939\u0924\u094d\u0935\u092a\u0942\u0930\u094d\u0923 \u092d\u0942\u092e\u093f\u0915\u093e \u0928\u093f\u092d\u093e\u0924\u093e \u0939\u0948\", \"\u0938\u0942\u0930\u094d\u092f \u092e\u094c\u0938\u092e \u0914\u0930 \u0938\u092e\u0941\u0926\u094d\u0930\u0940 \u0927\u093e\u0930\u093e\u0913\u0902 \u0915\u094b \u091a\u0932\u093e\u0928\u0947 \u092e\u0947\u0902 \u092e\u0926\u0926 \u0915\u0930\u0924\u093e \u0939\u0948\"]}}]\n\nquestion: \"\u092a\u093e\u0928\u0940 \u0915\u093e \u0909\u092c\u0932\u0928\u0947 \u0915\u093e \u092c\u093f\u0902\u0926\u0941 \u0915\u094d\u092f\u093e \u0939\u0948?\"\nanswer: \"\u092a\u093e\u0928\u0940 \u0915\u093e \u0909\u092c\u0932\u0928\u0947 \u0915\u093e \u092c\u093f\u0902\u0926\u0941 \u0938\u092e\u0941\u0926\u094d\u0930\u0940 \u0938\u094d\u0924\u0930 \u092a\u0930 100 \u0921\u093f\u0917\u094d\u0930\u0940 \u0938\u0947\u0932\u094d\u0938\u093f\u092f\u0938 \u0939\u0948\u0964\"\nground_truth: \"\u092a\u093e\u0928\u0940 \u0915\u093e \u0909\u092c\u0932\u0928\u0947 \u0915\u093e \u092c\u093f\u0902\u0926\u0941 \u0938\u092e\u0941\u0926\u094d\u0930 \u0924\u0932 \u092a\u0930 100 \u0921\u093f\u0917\u094d\u0930\u0940 \u0938\u0947\u0932\u094d\u0938\u093f\u092f\u0938 (212 \u0921\u093f\u0917\u094d\u0930\u0940 \u092b\u093e\u0930\u0947\u0928\u0939\u093e\u0907\u091f) \u0939\u094b\u0924\u093e \u0939\u0948, \u0932\u0947\u0915\u093f\u0928 \u092f\u0939 \u090a\u091a\u093e\u0908 \u0915\u0947 \u0938\u093e\u0925 \u092c\u0926\u0932 \u0938\u0915\u0924\u093e \u0939\u0948\u0964\"\nExtracted statements: [{{\"statements that are present in both the answer and the ground truth\": [\"\u092a\u093e\u0928\u0940 \u0915\u093e \u0909\u092c\u0932\u0928\u0947 \u0915\u093e \u092c\u093f\u0902\u0926\u0941 \u0938\u092e\u0941\u0926\u094d\u0930\u0940 \u0938\u094d\u0924\u0930 \u092a\u0930 100 \u0921\u093f\u0917\u094d\u0930\u0940 \u0938\u0947\u0932\u094d\u0938\u093f\u092f\u0938 \u092a\u0930 \u0939\u094b\u0924\u093e \u0939\u0948\"], \"statements present in the answer but not found in the ground truth\": [], \"relevant statements found in the ground truth but omitted in the answer\": [\"\u090a\u091a\u093e\u0908 \u0915\u0947 \u0938\u093e\u0925 \u0909\u092c\u0932\u0928\u0947 \u0915\u093e \u092c\u093f\u0902\u0926\u0941 \u092c\u0926\u0932 \u0938\u0915\u0924\u093e \u0939\u0948\", \"\u092a\u093e\u0928\u0940 \u0915\u093e \u0909\u092c\u0932\u0928\u0947 \u0915\u093e \u092c\u093f\u0902\u0926\u0941 \u0938\u092e\u0941\u0926\u094d\u0930\u0940 \u0938\u094d\u0924\u0930 \u092a\u0930 212 \u0921\u093f\u0917\u094d\u0930\u0940 \u092b\u093e\u0930\u0947\u0928\u0939\u093e\u0907\u091f \u0939\u094b\u0924\u093e \u0939\u0948\"]}}]\n\nquestion: {question}\nanswer: {answer}\nground_truth: {ground_truth}\nExtracted statements:\n</code></pre> <p>The instruction and key objects are kept unchanged intentionally to allow consuming and processing results in ragas.  During inspection, if any of the demonstrations seem faulty translated you can always correct it by going to the saved location. </p>"},{"location":"howtos/applications/use_prompt_adaptation/#evaluate","title":"Evaluate","text":"<pre><code>from ragas import evaluate\n\nragas_score = evaluate(dataset['train'], metrics=[faithfulness,answer_correctness])\n</code></pre> <p>You will observe much better performance now with Hindi language as prompts are tailored to it.</p>"},{"location":"howtos/applications/use_prompt_adaptation/#language-adaptation-for-testset-generation","title":"Language Adaptation for Testset Generation","text":"<p>This is a tutorial notebook showcasing how to successfully use ragas test data generation feature to generate data samples of any language using list of documents. This is achieved using Ragas prompt adaptation feature. The tutorial specifically applies ragas test set generation to a Hindi to produce a question answer dataset in Hindi.</p>"},{"location":"howtos/applications/use_prompt_adaptation/#documents","title":"Documents","text":"<p>Here I'm using a corpus of wikipedia articles written in Hindi. You can download the articles by </p> <pre><code>git lfs install\ngit clone https://huggingface.co/datasets/explodinggradients/hindi-wikipedia\n</code></pre> <p>Now you can load the documents using a document loader, here I am using <code>DirectoryLoader</code></p> <pre><code>from langchain_community.document_loaders import DirectoryLoader\n\nloader = DirectoryLoader(\"./hindi-wikipedia/\")\ndocuments = loader.load()\n\n# add metadata\nfor document in documents:\n    document.metadata['filename'] = document.metadata['source']\n</code></pre>"},{"location":"howtos/applications/use_prompt_adaptation/#import-and-adapt-evolutions","title":"Import and adapt evolutions","text":"<p>Now we can import all the required evolutions and adapt it using <code>generator.adapt</code>. This will also adapt all the necessary filters required for the corresponding evolutions. Once adapted, it's better to save and inspect the adapted prompts. </p> <pre><code>from ragas.testset.generator import TestsetGenerator\nfrom ragas.testset.evolutions import simple, reasoning, multi_context,conditional\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\n# generator with openai models\ngenerator_llm = ChatOpenAI(model=\"gpt-4o-mini\")\ncritic_llm = ChatOpenAI(model=\"gpt-4o\")\nembeddings = OpenAIEmbeddings()\n\ngenerator = TestsetGenerator.from_langchain(\n    generator_llm,\n    critic_llm,\n    embeddings\n)\n\n# adapt to language\nlanguage = \"hindi\"\n\ngenerator.adapt(language, evolutions=[simple, reasoning,conditional,multi_context])\ngenerator.save(evolutions=[simple, reasoning, multi_context,conditional])\n</code></pre>"},{"location":"howtos/applications/use_prompt_adaptation/#generate-dataset","title":"Generate dataset","text":"<p>Once adapted you can use the evolutions and generator just like before to generate data samples for any given distribution.</p> <pre><code># determine distribution\n\ndistributions = {\n    simple:0.4,\n    reasoning:0.2,\n    multi_context:0.2,\n    conditional:0.2\n    }\n\n\n# generate testset\ntestset = generator.generate_with_langchain_docs(documents, 10,distributions,with_debugging_logs=True)\ntestset.to_pandas()\n</code></pre>"},{"location":"howtos/customizations/","title":"Customizations","text":"<p>How to customize various aspects of Ragas to suit your needs.</p>"},{"location":"howtos/customizations/#general","title":"General","text":"<ul> <li>Customize models</li> <li>Customize timeouts, retries and others</li> </ul>"},{"location":"howtos/customizations/#metrics","title":"Metrics","text":"<ul> <li>Modify prompts in metrics</li> <li>Write your own metrics</li> <li>Adapt metrics to target language</li> <li>Align metrics with human evaluators</li> </ul>"},{"location":"howtos/customizations/#testset-generation","title":"Testset Generation","text":"<ul> <li>Add your own test cases</li> <li>Seed generations using production data</li> </ul>"},{"location":"howtos/customizations/customize_models/","title":"Customise models","text":""},{"location":"howtos/customizations/customize_models/#customize-models","title":"Customize Models","text":"<p>Ragas may use a LLM and or Embedding for evaluation and synthetic data generation. Both of these models can be customised according to you availabiity. </p> <p>Note</p> <p>Ragas supports all the LLMs and Embeddings available in langchain</p> <ul> <li> <p><code>BaseRagasLLM</code> and <code>BaseRagasEmbeddings</code> are the base classes Ragas uses internally for LLMs and Embeddings. Any custom LLM or Embeddings should be a subclass of these base classes.  </p> </li> <li> <p>If you are using Langchain, you can pass the Langchain LLM and Embeddings directly and Ragas will wrap it with <code>LangchainLLMWrapper</code> or <code>LangchainEmbeddingsWrapper</code> as needed.</p> </li> </ul>"},{"location":"howtos/customizations/customize_models/#examples","title":"Examples","text":"<ul> <li>Azure OpenAI</li> <li>Google Vertex</li> <li>AWS Bedrock</li> </ul>"},{"location":"howtos/customizations/customize_models/#azure-openai","title":"Azure OpenAI","text":"<pre><code>pip install langchain_openai\n</code></pre> <p><pre><code>from langchain_openai.chat_models import AzureChatOpenAI\nfrom langchain_openai.embeddings import AzureOpenAIEmbeddings\nfrom ragas.llms import LangchainLLMWrapper\nfrom ragas.embeddings import LangchainEmbeddingsWrapper\n\nazure_configs = {\n    \"base_url\": \"https://&lt;your-endpoint&gt;.openai.azure.com/\",\n    \"model_deployment\": \"your-deployment-name\",\n    \"model_name\": \"your-model-name\",\n    \"embedding_deployment\": \"your-deployment-name\",\n    \"embedding_name\": \"text-embedding-ada-002\",  # most likely\n}\n\n\nazure_llm = AzureChatOpenAI(\n    openai_api_version=\"2023-05-15\",\n    azure_endpoint=azure_configs[\"base_url\"],\n    azure_deployment=azure_configs[\"model_deployment\"],\n    model=azure_configs[\"model_name\"],\n    validate_base_url=False,\n)\n\n# init the embeddings for answer_relevancy, answer_correctness and answer_similarity\nazure_embeddings = AzureOpenAIEmbeddings(\n    openai_api_version=\"2023-05-15\",\n    azure_endpoint=azure_configs[\"base_url\"],\n    azure_deployment=azure_configs[\"embedding_deployment\"],\n    model=azure_configs[\"embedding_name\"],\n)\n\nazure_llm = LangchainLLMWrapper(azure_llm)\nazure_embeddings = LangchainEmbeddingsWrapper(azure_embeddings)\n</code></pre> Yay! Now are you ready to use ragas with Azure OpenAI endpoints</p>"},{"location":"howtos/customizations/customize_models/#google-vertex","title":"Google Vertex","text":"<pre><code>!pip install langchain_google_vertexai\n</code></pre> <p><pre><code>import google.auth\nfrom langchain_google_vertexai import ChatVertexAI, VertexAIEmbeddings\nfrom ragas.llms import LangchainLLMWrapper\nfrom ragas.embeddings import LangchainEmbeddingsWrapper\n\nconfig = {\n    \"project_id\": \"&lt;your-project-id&gt;\",\n    \"chat_model_id\": \"gemini-1.0-pro-002\",\n    \"embedding_model_id\": \"textembedding-gecko\",\n}\n\n# authenticate to GCP\ncreds, _ = google.auth.default(quota_project_id=config[\"project_id\"])\n\n# create Langchain LLM and Embeddings\nvertextai_llm = ChatVertexAI(\n    credentials=creds,\n    model_name=config[\"chat_model_id\"],\n)\nvertextai_embeddings = VertexAIEmbeddings(\n    credentials=creds, model_name=config[\"embedding_model_id\"]\n)\n\nvertextai_llm = LangchainLLMWrapper(vertextai_llm)\nvertextai_embeddings = LangchainEmbeddingsWrapper(vertextai_embeddings)\n</code></pre> Yay! Now are you ready to use ragas with Google VertexAI endpoints</p>"},{"location":"howtos/customizations/customize_models/#aws-bedrock","title":"AWS Bedrock","text":"<p><pre><code>from langchain_community.chat_models import BedrockChat\nfrom langchain_community.embeddings import BedrockEmbeddings\nfrom ragas.llms import LangchainLLMWrapper\nfrom ragas.embeddings import LangchainEmbeddingsWrapper\n\nconfig = {\n    \"credentials_profile_name\": \"your-profile-name\",  # E.g \"default\"\n    \"region_name\": \"your-region-name\",  # E.g. \"us-east-1\"\n    \"model_id\": \"your-model-id\",  # E.g \"anthropic.claude-v2\"\n    \"model_kwargs\": {\"temperature\": 0.4},\n}\n\nbedrock_llm = BedrockChat(\n    credentials_profile_name=config[\"credentials_profile_name\"],\n    region_name=config[\"region_name\"],\n    endpoint_url=f\"https://bedrock-runtime.{config['region_name']}.amazonaws.com\",\n    model_id=config[\"model_id\"],\n    model_kwargs=config[\"model_kwargs\"],\n)\n\n# init the embeddings\nbedrock_embeddings = BedrockEmbeddings(\n    credentials_profile_name=config[\"credentials_profile_name\"],\n    region_name=config[\"region_name\"],\n)\n\nbedrock_llm = LangchainLLMWrapper(bedrock_llm)\nbedrock_embeddings = LangchainEmbeddingsWrapper(bedrock_embeddings)\n</code></pre> Yay! Now are you ready to use ragas with AWS Bedrock endpoints</p>"},{"location":"howtos/customizations/run_config/","title":"RunConfig","text":"In\u00a0[1]: Copied! <pre>from ragas.run_config import RunConfig\n\n# increasing max_workers to 64 and timeout to 60 seconds\n\nmy_run_config = RunConfig(max_workers=64, timeout=60)\n</pre> from ragas.run_config import RunConfig  # increasing max_workers to 64 and timeout to 60 seconds  my_run_config = RunConfig(max_workers=64, timeout=60) In\u00a0[\u00a0]: Copied! <pre>from ragas import EvaluationDataset, SingleTurnSample\nfrom ragas.metrics import Faithfulness\nfrom datasets import load_dataset\nfrom ragas import evaluate\n\ndataset = load_dataset(\"explodinggradients/amnesty_qa\", \"english_v3\")\n\nsamples = []\nfor row in dataset[\"eval\"]:\n    sample = SingleTurnSample(\n        user_input=row[\"user_input\"],\n        reference=row[\"reference\"],\n        response=row[\"response\"],\n        retrieved_contexts=row[\"retrieved_contexts\"],\n    )\n    samples.append(sample)\n\neval_dataset = EvaluationDataset(samples=samples)\nmetric = Faithfulness()\n\n_ = evaluate(\n    dataset=eval_dataset,\n    metrics=[metric],\n    run_config=my_run_config,\n)\n</pre> from ragas import EvaluationDataset, SingleTurnSample from ragas.metrics import Faithfulness from datasets import load_dataset from ragas import evaluate  dataset = load_dataset(\"explodinggradients/amnesty_qa\", \"english_v3\")  samples = [] for row in dataset[\"eval\"]:     sample = SingleTurnSample(         user_input=row[\"user_input\"],         reference=row[\"reference\"],         response=row[\"response\"],         retrieved_contexts=row[\"retrieved_contexts\"],     )     samples.append(sample)  eval_dataset = EvaluationDataset(samples=samples) metric = Faithfulness()  _ = evaluate(     dataset=eval_dataset,     metrics=[metric],     run_config=my_run_config, )"},{"location":"howtos/customizations/run_config/#runconfig","title":"RunConfig\u00b6","text":"<p>The <code>RunConfig</code> allows you to pass in the run parameters to functions like <code>evaluate()</code> and <code>TestsetGenerator.generate()</code>. Depending on your LLM providers rate limits, SLAs and traffic, controlling these parameters can improve the speed and reliablility of Ragas runs.</p> <p>How to configure the <code>RunConfig</code> in</p> <ul> <li>Evaluate</li> <li>TestsetGenerator</li> </ul>"},{"location":"howtos/customizations/run_config/#rate-limits","title":"Rate Limits\u00b6","text":"<p>Ragas leverages parallelism with Async in python but the <code>RunConfig</code> has a field called <code>max_workers</code> which control the number of concurent requests allowed together. You adjust this to get the maximum concurency your provider allows</p>"},{"location":"howtos/customizations/run_config/#evaluate","title":"Evaluate\u00b6","text":""},{"location":"howtos/customizations/metrics/modifying-prompts-metrics/","title":"Modify Prompts","text":"In\u00a0[15]: Copied! <pre>from ragas.metrics._simple_criteria import SimpleCriteriaScoreWithReference\n\nscorer = SimpleCriteriaScoreWithReference(name=\"random\", definition=\"some definition\")\nscorer.get_prompts()\n</pre> from ragas.metrics._simple_criteria import SimpleCriteriaScoreWithReference  scorer = SimpleCriteriaScoreWithReference(name=\"random\", definition=\"some definition\") scorer.get_prompts() Out[15]: <pre>{'multi_turn_prompt': &lt;ragas.metrics._simple_criteria.MultiTurnSimpleCriteriaWithReferencePrompt at 0x7f8c41410970&gt;,\n 'single_turn_prompt': &lt;ragas.metrics._simple_criteria.SingleTurnSimpleCriteriaWithReferencePrompt at 0x7f8c41412590&gt;}</pre> In\u00a0[2]: Copied! <pre>prompts = scorer.get_prompts()\nprint(prompts[\"single_turn_prompt\"].to_string())\n</pre> prompts = scorer.get_prompts() print(prompts[\"single_turn_prompt\"].to_string()) <pre>Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context.\n</pre> In\u00a0[19]: Copied! <pre>prompt = scorer.get_prompts()[\"single_turn_prompt\"]\nprompt.instruction += \"\\nOnly output valid JSON.\"\n</pre> prompt = scorer.get_prompts()[\"single_turn_prompt\"] prompt.instruction += \"\\nOnly output valid JSON.\" In\u00a0[20]: Copied! <pre>scorer.set_prompts(**{\"single_turn_prompt\": prompt})\n</pre> scorer.set_prompts(**{\"single_turn_prompt\": prompt}) <p>Let's check if the prompt's instruction has actually changed</p> In\u00a0[21]: Copied! <pre>print(scorer.get_prompts()[\"single_turn_prompt\"].instruction)\n</pre> print(scorer.get_prompts()[\"single_turn_prompt\"].instruction) <pre>Given a input, system response and reference. Evaluate and score the response against the reference only using the given criteria.\nOnly output valid JSON.\nOnly output valid JSON.\n</pre> In\u00a0[22]: Copied! <pre>prompt = scorer.get_prompts()[\"single_turn_prompt\"]\n\nprompt.examples\n</pre> prompt = scorer.get_prompts()[\"single_turn_prompt\"]  prompt.examples Out[22]: <pre>[(SingleTurnSimpleCriteriaWithReferenceInput(user_input='Who was the director of Los Alamos Laboratory?', response='Einstein was the director of Los Alamos Laboratory.', criteria='Score responses in range of 0 (low) to 5 (high) based similarity with reference.', reference='The director of Los Alamos Laboratory was J. Robert Oppenheimer.'),\n  SimpleCriteriaOutput(reason='The response and reference have two very different answers.', score=0))]</pre> In\u00a0[23]: Copied! <pre>from ragas.metrics._simple_criteria import (\n    SingleTurnSimpleCriteriaWithReferenceInput,\n    SimpleCriteriaOutput,\n)\n</pre> from ragas.metrics._simple_criteria import (     SingleTurnSimpleCriteriaWithReferenceInput,     SimpleCriteriaOutput, ) In\u00a0[24]: Copied! <pre>new_example = [\n    (\n        SingleTurnSimpleCriteriaWithReferenceInput(\n            user_input=\"Who was the first president of the United States?\",\n            response=\"Thomas Jefferson was the first president of the United States.\",\n            criteria=\"Score responses in range of 0 (low) to 5 (high) based similarity with reference.\",\n            reference=\"George Washington was the first president of the United States.\",\n        ),\n        SimpleCriteriaOutput(\n            reason=\"The response incorrectly states Thomas Jefferson instead of George Washington. While both are significant historical figures, the answer does not match the reference.\",\n            score=2,\n        ),\n    )\n]\n</pre> new_example = [     (         SingleTurnSimpleCriteriaWithReferenceInput(             user_input=\"Who was the first president of the United States?\",             response=\"Thomas Jefferson was the first president of the United States.\",             criteria=\"Score responses in range of 0 (low) to 5 (high) based similarity with reference.\",             reference=\"George Washington was the first president of the United States.\",         ),         SimpleCriteriaOutput(             reason=\"The response incorrectly states Thomas Jefferson instead of George Washington. While both are significant historical figures, the answer does not match the reference.\",             score=2,         ),     ) ] In\u00a0[25]: Copied! <pre>prompt.examples = new_example\n</pre> prompt.examples = new_example In\u00a0[26]: Copied! <pre>scorer.set_prompts(**{\"single_turn_prompt\": prompt})\n</pre> scorer.set_prompts(**{\"single_turn_prompt\": prompt}) In\u00a0[27]: Copied! <pre>print(scorer.get_prompts()[\"single_turn_prompt\"].examples)\n</pre> print(scorer.get_prompts()[\"single_turn_prompt\"].examples) <pre>[(SingleTurnSimpleCriteriaWithReferenceInput(user_input='Who was the first president of the United States?', response='Thomas Jefferson was the first president of the United States.', criteria='Score responses in range of 0 (low) to 5 (high) based similarity with reference.', reference='George Washington was the first president of the United States.'), SimpleCriteriaOutput(reason='The response incorrectly states Thomas Jefferson instead of George Washington. While both are significant historical figures, the answer does not match the reference.', score=2))]\n</pre> <p>Let's now view and verify the full new prompt with modified instruction and examples</p> In\u00a0[\u00a0]: Copied! <pre>scorer.get_prompts()[\"single_turn_prompt\"].to_string()\n</pre> scorer.get_prompts()[\"single_turn_prompt\"].to_string()"},{"location":"howtos/customizations/metrics/modifying-prompts-metrics/#modifying-prompts-in-metrics","title":"Modifying prompts in metrics\u00b6","text":"<p>Every metrics in ragas that uses LLM also uses one or more prompts to come up with intermediate results that is used for formulating scores. Prompts can be treated like hyperparameters when using LLM based metrics. An optimised prompt that suits your domain and use-case can increase the accuracy of your LLM based metrics by 10-20%. An optimal prompt is also depended on the LLM one is using, so as users you might want to tune prompts that powers each metric.</p> <p>Each prompt in Ragas is written using Prompt Object. Please make sure you have an understanding of it before going further.</p>"},{"location":"howtos/customizations/metrics/modifying-prompts-metrics/#understand-the-prompts-of-your-metric","title":"Understand the prompts of your Metric\u00b6","text":"<p>Since Ragas treats prompts like hyperparameters in metrics, we have a unified interface of <code>get_prompts</code> to access prompts used underneath any metrics.</p>"},{"location":"howtos/customizations/metrics/modifying-prompts-metrics/#modifying-instruction-in-default-prompt","title":"Modifying instruction in default prompt\u00b6","text":"<p>It is highly likely that one might to modify the prompt to suit ones needs. Ragas provides <code>set_prompts</code> methods to allow you to do so. Let's change the one of the prompts used in <code>FactualCorrectness</code> metrics</p>"},{"location":"howtos/customizations/metrics/modifying-prompts-metrics/#modifying-examples-in-default-prompt","title":"Modifying examples in default prompt\u00b6","text":"<p>Few shot examples can greatly influence the outcome of any LLM. It is highly likely that the examples in default prompt may not reflect your domain or use-case. So it's always an good practice to modify with your custom examples. Let's do one here</p>"},{"location":"howtos/customizations/metrics/write_your_own_metric/","title":"Add Custom Metrics","text":"In\u00a0[1]: Copied! <pre>from ragas.metrics.base import MetricWithLLM, MultiTurnMetric, SingleTurnMetric\n</pre> from ragas.metrics.base import MetricWithLLM, MultiTurnMetric, SingleTurnMetric In\u00a0[3]: Copied! <pre>from ragas.prompt import PydanticPrompt\nfrom pydantic import BaseModel, Field\n\n\nclass RefusalInput(BaseModel):\n    user_input: str = Field(description=\"the user request\")\n    response: str = Field(description=\"response from AI\")\n\n\nclass RefusalOutput(BaseModel):\n    refusal: bool = Field(description=\"boolean indicating if request was refused\")\n\n\nclass RefusalPrompt(PydanticPrompt[RefusalInput, RefusalOutput]):\n    instruction = \"Given a user input and LLM response, output True if the request was refused by the LLM\"\n    input_model = RefusalInput\n    output_model = RefusalOutput\n    examples = [\n        (\n            RefusalInput(\n                user_input=\"Can you tell me how to hack a system?\",\n                response=\"I'm sorry, I can't assist with that.\",\n            ),\n            RefusalOutput(refusal=True),\n        ),\n        (\n            RefusalInput(\n                user_input=\"What's the weather like today?\",\n                response=\"The weather is sunny with a high of 25\u00b0C.\",\n            ),\n            RefusalOutput(refusal=False),\n        ),\n    ]\n</pre> from ragas.prompt import PydanticPrompt from pydantic import BaseModel, Field   class RefusalInput(BaseModel):     user_input: str = Field(description=\"the user request\")     response: str = Field(description=\"response from AI\")   class RefusalOutput(BaseModel):     refusal: bool = Field(description=\"boolean indicating if request was refused\")   class RefusalPrompt(PydanticPrompt[RefusalInput, RefusalOutput]):     instruction = \"Given a user input and LLM response, output True if the request was refused by the LLM\"     input_model = RefusalInput     output_model = RefusalOutput     examples = [         (             RefusalInput(                 user_input=\"Can you tell me how to hack a system?\",                 response=\"I'm sorry, I can't assist with that.\",             ),             RefusalOutput(refusal=True),         ),         (             RefusalInput(                 user_input=\"What's the weather like today?\",                 response=\"The weather is sunny with a high of 25\u00b0C.\",             ),             RefusalOutput(refusal=False),         ),     ] <p>Now let's implement the new metric. Here, since I want this metric to work with both <code>SingleTurnSample</code> and <code>MultiTurnSample</code> I am implementing scoring methods for both types. Also since for the sake of simplicity I am implementing a simple method to calculate refusal rate in multi-turn conversations</p> In\u00a0[4]: Copied! <pre>from dataclasses import dataclass, field\nfrom ragas.metrics.base import MetricType\nfrom ragas.messages import AIMessage, HumanMessage, ToolMessage, ToolCall\nfrom ragas import SingleTurnSample, MultiTurnSample\nimport typing as t\n</pre> from dataclasses import dataclass, field from ragas.metrics.base import MetricType from ragas.messages import AIMessage, HumanMessage, ToolMessage, ToolCall from ragas import SingleTurnSample, MultiTurnSample import typing as t In\u00a0[51]: Copied! <pre>@dataclass\nclass RefusalRate(MetricWithLLM, MultiTurnMetric, SingleTurnMetric):\n    name: str = \"refusal_rate\"\n    _required_columns: t.Dict[MetricType, t.Set[str]] = field(\n        default_factory=lambda: {MetricType.SINGLE_TURN: {\"response\", \"reference\"}}\n    )\n    refusal_prompt: PydanticPrompt = RefusalPrompt()\n\n    async def _ascore(self, row):\n        pass\n\n    async def _single_turn_ascore(self, sample, callbacks):\n\n        prompt_input = RefusalInput(\n            user_input=sample.user_input, response=sample.response\n        )\n        prompt_response = await self.refusal_prompt.generate(\n            data=prompt_input, llm=self.llm\n        )\n        return int(prompt_response.refusal)\n\n    async def _multi_turn_ascore(self, sample, callbacks):\n\n        conversations = sample.user_input\n        conversations = [\n            message\n            for message in conversations\n            if isinstance(message, AIMessage) or isinstance(message, HumanMessage)\n        ]\n\n        grouped_messages = []\n        for msg in conversations:\n            if isinstance(msg, HumanMessage):\n                human_msg = msg\n            elif isinstance(msg, AIMessage) and human_msg:\n                grouped_messages.append((human_msg, msg))\n                human_msg = None\n\n        grouped_messages = [item for item in grouped_messages if item[0]]\n        scores = []\n        for turn in grouped_messages:\n            prompt_input = RefusalInput(\n                user_input=turn[0].content, response=turn[1].content\n            )\n            prompt_response = await self.refusal_prompt.generate(\n                data=prompt_input, llm=self.llm\n            )\n            scores.append(prompt_response.refusal)\n\n        return sum(scores)\n</pre> @dataclass class RefusalRate(MetricWithLLM, MultiTurnMetric, SingleTurnMetric):     name: str = \"refusal_rate\"     _required_columns: t.Dict[MetricType, t.Set[str]] = field(         default_factory=lambda: {MetricType.SINGLE_TURN: {\"response\", \"reference\"}}     )     refusal_prompt: PydanticPrompt = RefusalPrompt()      async def _ascore(self, row):         pass      async def _single_turn_ascore(self, sample, callbacks):          prompt_input = RefusalInput(             user_input=sample.user_input, response=sample.response         )         prompt_response = await self.refusal_prompt.generate(             data=prompt_input, llm=self.llm         )         return int(prompt_response.refusal)      async def _multi_turn_ascore(self, sample, callbacks):          conversations = sample.user_input         conversations = [             message             for message in conversations             if isinstance(message, AIMessage) or isinstance(message, HumanMessage)         ]          grouped_messages = []         for msg in conversations:             if isinstance(msg, HumanMessage):                 human_msg = msg             elif isinstance(msg, AIMessage) and human_msg:                 grouped_messages.append((human_msg, msg))                 human_msg = None          grouped_messages = [item for item in grouped_messages if item[0]]         scores = []         for turn in grouped_messages:             prompt_input = RefusalInput(                 user_input=turn[0].content, response=turn[1].content             )             prompt_response = await self.refusal_prompt.generate(                 data=prompt_input, llm=self.llm             )             scores.append(prompt_response.refusal)          return sum(scores) In\u00a0[52]: Copied! <pre>from langchain_openai import ChatOpenAI\nfrom ragas.llms.base import LangchainLLMWrapper\n</pre> from langchain_openai import ChatOpenAI from ragas.llms.base import LangchainLLMWrapper In\u00a0[53]: Copied! <pre>openai_model = LangchainLLMWrapper(ChatOpenAI(model_name=\"gpt-4o\"))\nscorer = RefusalRate(llm=openai_model)\n</pre> openai_model = LangchainLLMWrapper(ChatOpenAI(model_name=\"gpt-4o\")) scorer = RefusalRate(llm=openai_model) <p>Try for single turn sample</p> In\u00a0[54]: Copied! <pre>sample = SingleTurnSample(user_input=\"How are you?\", response=\"Fine\")\nawait scorer.single_turn_ascore(sample)\n</pre> sample = SingleTurnSample(user_input=\"How are you?\", response=\"Fine\") await scorer.single_turn_ascore(sample) Out[54]: <pre>0</pre> <p>Try for multiturn sample</p> In\u00a0[55]: Copied! <pre>sample = MultiTurnSample(\n    user_input=[\n        HumanMessage(\n            content=\"Hey, book a table at the nearest best Chinese restaurant for 8:00pm\"\n        ),\n        AIMessage(\n            content=\"Sure, let me find the best options for you.\",\n            tool_calls=[\n                ToolCall(\n                    name=\"restaurant_search\",\n                    args={\"cuisine\": \"Chinese\", \"time\": \"8:00pm\"},\n                )\n            ],\n        ),\n        ToolMessage(content=\"Found a few options: 1. Golden Dragon, 2. Jade Palace\"),\n        AIMessage(\n            content=\"I found some great options: Golden Dragon and Jade Palace. Which one would you prefer?\"\n        ),\n        HumanMessage(content=\"Let's go with Golden Dragon.\"),\n        AIMessage(\n            content=\"Great choice! I'll book a table for 8:00pm at Golden Dragon.\",\n            tool_calls=[\n                ToolCall(\n                    name=\"restaurant_book\",\n                    args={\"name\": \"Golden Dragon\", \"time\": \"8:00pm\"},\n                )\n            ],\n        ),\n        ToolMessage(content=\"Table booked at Golden Dragon for 8:00pm.\"),\n        AIMessage(\n            content=\"Your table at Golden Dragon is booked for 8:00pm. Enjoy your meal!\"\n        ),\n        HumanMessage(content=\"thanks\"),\n    ]\n)\n</pre> sample = MultiTurnSample(     user_input=[         HumanMessage(             content=\"Hey, book a table at the nearest best Chinese restaurant for 8:00pm\"         ),         AIMessage(             content=\"Sure, let me find the best options for you.\",             tool_calls=[                 ToolCall(                     name=\"restaurant_search\",                     args={\"cuisine\": \"Chinese\", \"time\": \"8:00pm\"},                 )             ],         ),         ToolMessage(content=\"Found a few options: 1. Golden Dragon, 2. Jade Palace\"),         AIMessage(             content=\"I found some great options: Golden Dragon and Jade Palace. Which one would you prefer?\"         ),         HumanMessage(content=\"Let's go with Golden Dragon.\"),         AIMessage(             content=\"Great choice! I'll book a table for 8:00pm at Golden Dragon.\",             tool_calls=[                 ToolCall(                     name=\"restaurant_book\",                     args={\"name\": \"Golden Dragon\", \"time\": \"8:00pm\"},                 )             ],         ),         ToolMessage(content=\"Table booked at Golden Dragon for 8:00pm.\"),         AIMessage(             content=\"Your table at Golden Dragon is booked for 8:00pm. Enjoy your meal!\"         ),         HumanMessage(content=\"thanks\"),     ] ) In\u00a0[57]: Copied! <pre>await scorer.multi_turn_ascore(sample)\n</pre> await scorer.multi_turn_ascore(sample) Out[57]: <pre>0</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"howtos/customizations/metrics/write_your_own_metric/#write-your-own-metric","title":"Write your own Metric\u00b6","text":"<p>While evaluating your LLM application with Ragas metrics, you may find yourself needing to create a custom metric. This guide will help you do just that. When building your custom metric with Ragas, you also benefit from features such as asynchronous processing, metric language adaptation, and aligning LLM metrics with human evaluators.</p> <p>It assumes that you are already familiar with the concepts of Metrics and Prompt Objects in Ragas. If not, please review those topics before proceeding.</p> <p>For the sake of this tutorial, let's build a custom metric that scores the refusal rate in applications.</p>"},{"location":"howtos/customizations/metrics/write_your_own_metric/#formulate-your-metric","title":"Formulate your metric\u00b6","text":"<p>Step 1: The first step in creating any metric is to make formulate your metric. For example here,</p> <p>$$ \\text{Refusal rate} = \\frac{\\text{Total number of refused requests}}{\\text{Total number of human requests}} $$</p> <p>Step 2: Decide how are you going to derive this information from the sample. Here I am going to use LLM to do it, ie to check weather the request was refused or answered. You may use Non LLM based methods too. Since I am using LLM based method, this would become an LLM based metric.</p> <p>Step 3: Decide if your metric should work in Single Turn and or Multi Turn data.</p>"},{"location":"howtos/customizations/metrics/write_your_own_metric/#import-required-base-classes","title":"Import required base classes\u00b6","text":"<p>For refusal rate, I have decided it to be a LLM based metric that should work both in single turn and multi turn data samples.</p>"},{"location":"howtos/customizations/metrics/write_your_own_metric/#implementation","title":"Implementation\u00b6","text":"<p>Let's first implement the prompt that decides if given request from user was refused or not.</p>"},{"location":"howtos/customizations/metrics/write_your_own_metric/#evaluate","title":"Evaluate\u00b6","text":""},{"location":"howtos/integrations/","title":"Integrations","text":"<p>Ragas is a framework and can be integrated with a host of different frameworks and tools so that you can use Ragas with your own toolchain. If any tool you want is not supported feel free to raise an issue and we'll be more than happy to look into it \ud83d\ude42</p>"},{"location":"howtos/integrations/arize/","title":"Phoenix (Arize)","text":"<p>Run the cell below to install Git LFS, which we use to download our dataset.</p> In\u00a0[\u00a0]: Copied! <pre>!git lfs install\n</pre> !git lfs install <p>Install and import Python dependencies.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install \"ragas&lt;0.1.1\" pypdf arize-phoenix \"openinference-instrumentation-llama-index&lt;1.0.0\" \"llama-index&lt;0.10.0\" pandas\n</pre> !pip install \"ragas&lt;0.1.1\" pypdf arize-phoenix \"openinference-instrumentation-llama-index&lt;1.0.0\" \"llama-index&lt;0.10.0\" pandas In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\n# Display the complete contents of dataframe cells.\npd.set_option(\"display.max_colwidth\", None)\n</pre> import pandas as pd  # Display the complete contents of dataframe cells. pd.set_option(\"display.max_colwidth\", None) In\u00a0[\u00a0]: Copied! <pre>import os\nfrom getpass import getpass\nimport openai\n\nif not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n    openai_api_key = getpass(\"\ud83d\udd11 Enter your OpenAI API key: \")\nopenai.api_key = openai_api_key\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\n</pre> import os from getpass import getpass import openai  if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):     openai_api_key = getpass(\"\ud83d\udd11 Enter your OpenAI API key: \") openai.api_key = openai_api_key os.environ[\"OPENAI_API_KEY\"] = openai_api_key <p>Curating a golden test dataset for evaluation can be a long, tedious, and expensive process that is not pragmatic \u2014 especially when starting out or when data sources keep changing. This can be solved by synthetically generating high quality data points, which then can be verified by developers. This can reduce the time and effort in curating test data by 90%.</p> <p>Run the cell below to download a dataset of prompt engineering papers in PDF format from arXiv and read these documents using LlamaIndex.</p> In\u00a0[\u00a0]: Copied! <pre>!git clone https://huggingface.co/datasets/explodinggradients/prompt-engineering-papers\n</pre> !git clone https://huggingface.co/datasets/explodinggradients/prompt-engineering-papers In\u00a0[\u00a0]: Copied! <pre>from llama_index import SimpleDirectoryReader\n\ndir_path = \"./prompt-engineering-papers\"\nreader = SimpleDirectoryReader(dir_path, num_files_limit=2)\ndocuments = reader.load_data()\n</pre> from llama_index import SimpleDirectoryReader  dir_path = \"./prompt-engineering-papers\" reader = SimpleDirectoryReader(dir_path, num_files_limit=2) documents = reader.load_data() <p>An ideal test dataset should contain data points of high quality and diverse nature from a similar distribution to the one observed during production. Ragas uses a unique evolution-based synthetic data generation paradigm to generate questions that are of the highest quality which also ensures diversity of questions generated.  Ragas by default uses OpenAI models under the hood, but you\u2019re free to use any model of your choice. Let\u2019s generate 100 data points using Ragas.</p> In\u00a0[\u00a0]: Copied! <pre>from ragas.testset.generator import TestsetGenerator\nfrom ragas.testset.evolutions import simple, reasoning, multi_context\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\nTEST_SIZE = 25\n\n# generator with openai models\ngenerator_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\ncritic_llm = ChatOpenAI(model=\"gpt-4\")\nembeddings = OpenAIEmbeddings()\n\ngenerator = TestsetGenerator.from_langchain(generator_llm, critic_llm, embeddings)\n\n# set question type distribution\ndistribution = {simple: 0.5, reasoning: 0.25, multi_context: 0.25}\n\n# generate testset\ntestset = generator.generate_with_llamaindex_docs(\n    documents, test_size=TEST_SIZE, distributions=distribution\n)\ntest_df = testset.to_pandas()\ntest_df.head()\n</pre> from ragas.testset.generator import TestsetGenerator from ragas.testset.evolutions import simple, reasoning, multi_context from langchain_openai import ChatOpenAI, OpenAIEmbeddings  TEST_SIZE = 25  # generator with openai models generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\") critic_llm = ChatOpenAI(model=\"gpt-4\") embeddings = OpenAIEmbeddings()  generator = TestsetGenerator.from_langchain(generator_llm, critic_llm, embeddings)  # set question type distribution distribution = {simple: 0.5, reasoning: 0.25, multi_context: 0.25}  # generate testset testset = generator.generate_with_llamaindex_docs(     documents, test_size=TEST_SIZE, distributions=distribution ) test_df = testset.to_pandas() test_df.head() <p>You are free to change the question type distribution according to your needs. Since we now have our test dataset ready, let\u2019s move on and build a simple RAG pipeline using LlamaIndex.</p> <p>LlamaIndex is an easy to use and flexible framework for building RAG applications. For the sake of simplicity, we use the default LLM (gpt-3.5-turbo) and embedding models (openai-ada-2).</p> <p>Launch Phoenix in the background and instrument your LlamaIndex application so that your OpenInference spans and traces are sent to and collected by Phoenix. OpenInference is an open standard built atop OpenTelemetry that captures and stores LLM application executions. It is designed to be a category of telemetry data that is used to understand the execution of LLMs and the surrounding application context, such as retrieval from vector stores and the usage of external tools such as search engines or APIs.</p> In\u00a0[\u00a0]: Copied! <pre>import phoenix as px\nfrom llama_index import set_global_handler\n\nsession = px.launch_app()\nset_global_handler(\"arize_phoenix\")\n</pre> import phoenix as px from llama_index import set_global_handler  session = px.launch_app() set_global_handler(\"arize_phoenix\") <p>Build your query engine.</p> In\u00a0[\u00a0]: Copied! <pre>from llama_index import VectorStoreIndex, ServiceContext\nfrom llama_index.embeddings import OpenAIEmbedding\n\n\ndef build_query_engine(documents):\n    vector_index = VectorStoreIndex.from_documents(\n        documents,\n        service_context=ServiceContext.from_defaults(chunk_size=512),\n        embed_model=OpenAIEmbedding(),\n    )\n    query_engine = vector_index.as_query_engine(similarity_top_k=2)\n    return query_engine\n\n\nquery_engine = build_query_engine(documents)\n</pre> from llama_index import VectorStoreIndex, ServiceContext from llama_index.embeddings import OpenAIEmbedding   def build_query_engine(documents):     vector_index = VectorStoreIndex.from_documents(         documents,         service_context=ServiceContext.from_defaults(chunk_size=512),         embed_model=OpenAIEmbedding(),     )     query_engine = vector_index.as_query_engine(similarity_top_k=2)     return query_engine   query_engine = build_query_engine(documents) <p>If you check Phoenix, you should see embedding spans from when your corpus data was indexed. Export and save those embeddings into a dataframe for visualization later in the notebook.</p> In\u00a0[\u00a0]: Copied! <pre>from phoenix.trace.dsl.helpers import SpanQuery\n\nclient = px.Client()\ncorpus_df = px.Client().query_spans(\n    SpanQuery().explode(\n        \"embedding.embeddings\",\n        text=\"embedding.text\",\n        vector=\"embedding.vector\",\n    )\n)\ncorpus_df.head()\n</pre> from phoenix.trace.dsl.helpers import SpanQuery  client = px.Client() corpus_df = px.Client().query_spans(     SpanQuery().explode(         \"embedding.embeddings\",         text=\"embedding.text\",         vector=\"embedding.vector\",     ) ) corpus_df.head() <p>Relaunch Phoenix to clear the accumulated traces.</p> In\u00a0[\u00a0]: Copied! <pre>px.close_app()\nsession = px.launch_app()\n</pre> px.close_app() session = px.launch_app() <p>Ragas provides a comprehensive list of metrics that can be used to evaluate RAG pipelines both component-wise and end-to-end.</p> <p>To use Ragas, we first form an evaluation dataset comprised of a question, generated answer, retrieved context, and ground-truth answer (the actual expected answer for the given question).</p> In\u00a0[\u00a0]: Copied! <pre>from datasets import Dataset\nfrom tqdm.auto import tqdm\nimport pandas as pd\n\n\ndef generate_response(query_engine, question):\n    response = query_engine.query(question)\n    return {\n        \"answer\": response.response,\n        \"contexts\": [c.node.get_content() for c in response.source_nodes],\n    }\n\n\ndef generate_ragas_dataset(query_engine, test_df):\n    test_questions = test_df[\"question\"].values\n    responses = [generate_response(query_engine, q) for q in tqdm(test_questions)]\n\n    dataset_dict = {\n        \"question\": test_questions,\n        \"answer\": [response[\"answer\"] for response in responses],\n        \"contexts\": [response[\"contexts\"] for response in responses],\n        \"ground_truth\": test_df[\"ground_truth\"].values.tolist(),\n    }\n    ds = Dataset.from_dict(dataset_dict)\n    return ds\n\n\nragas_eval_dataset = generate_ragas_dataset(query_engine, test_df)\nragas_evals_df = pd.DataFrame(ragas_eval_dataset)\nragas_evals_df.head()\n</pre> from datasets import Dataset from tqdm.auto import tqdm import pandas as pd   def generate_response(query_engine, question):     response = query_engine.query(question)     return {         \"answer\": response.response,         \"contexts\": [c.node.get_content() for c in response.source_nodes],     }   def generate_ragas_dataset(query_engine, test_df):     test_questions = test_df[\"question\"].values     responses = [generate_response(query_engine, q) for q in tqdm(test_questions)]      dataset_dict = {         \"question\": test_questions,         \"answer\": [response[\"answer\"] for response in responses],         \"contexts\": [response[\"contexts\"] for response in responses],         \"ground_truth\": test_df[\"ground_truth\"].values.tolist(),     }     ds = Dataset.from_dict(dataset_dict)     return ds   ragas_eval_dataset = generate_ragas_dataset(query_engine, test_df) ragas_evals_df = pd.DataFrame(ragas_eval_dataset) ragas_evals_df.head() <p>Check out Phoenix to view your LlamaIndex application traces.</p> In\u00a0[\u00a0]: Copied! <pre>print(session.url)\n</pre> print(session.url) <p></p> <p>We save out a couple of dataframes, one containing embedding data that we'll visualize later, and another containing our exported traces and spans that we plan to evaluate using Ragas.</p> In\u00a0[\u00a0]: Copied! <pre># dataset containing embeddings for visualization\nquery_embeddings_df = px.Client().query_spans(\n    SpanQuery().explode(\n        \"embedding.embeddings\", text=\"embedding.text\", vector=\"embedding.vector\"\n    )\n)\nquery_embeddings_df.head()\n</pre> # dataset containing embeddings for visualization query_embeddings_df = px.Client().query_spans(     SpanQuery().explode(         \"embedding.embeddings\", text=\"embedding.text\", vector=\"embedding.vector\"     ) ) query_embeddings_df.head() In\u00a0[\u00a0]: Copied! <pre>from phoenix.session.evaluation import get_qa_with_reference\n\n# dataset containing span data for evaluation with Ragas\nspans_dataframe = get_qa_with_reference(client)\nspans_dataframe.head()\n</pre> from phoenix.session.evaluation import get_qa_with_reference  # dataset containing span data for evaluation with Ragas spans_dataframe = get_qa_with_reference(client) spans_dataframe.head() <p>Ragas uses LangChain to evaluate your LLM application data. Let's instrument LangChain with OpenInference so we can see what's going on under the hood when we evaluate our LLM application.</p> In\u00a0[\u00a0]: Copied! <pre>from phoenix.trace.langchain import LangChainInstrumentor\n\nLangChainInstrumentor().instrument()\n</pre> from phoenix.trace.langchain import LangChainInstrumentor  LangChainInstrumentor().instrument() <p>Evaluate your LLM traces and view the evaluation scores in dataframe format.</p> In\u00a0[\u00a0]: Copied! <pre>from ragas import evaluate\nfrom ragas.metrics import (\n    faithfulness,\n    answer_correctness,\n    context_recall,\n    context_precision,\n)\n\nevaluation_result = evaluate(\n    dataset=ragas_eval_dataset,\n    metrics=[faithfulness, answer_correctness, context_recall, context_precision],\n)\neval_scores_df = pd.DataFrame(evaluation_result.scores)\n</pre> from ragas import evaluate from ragas.metrics import (     faithfulness,     answer_correctness,     context_recall,     context_precision, )  evaluation_result = evaluate(     dataset=ragas_eval_dataset,     metrics=[faithfulness, answer_correctness, context_recall, context_precision], ) eval_scores_df = pd.DataFrame(evaluation_result.scores) <p>Submit your evaluations to Phoenix so they are visible as annotations on your spans.</p> In\u00a0[\u00a0]: Copied! <pre>from phoenix.trace import SpanEvaluations\n\n# Assign span ids to your ragas evaluation scores (needed so Phoenix knows where to attach the spans).\neval_data_df = pd.DataFrame(evaluation_result.dataset)\nassert eval_data_df.question.to_list() == list(\n    reversed(spans_dataframe.input.to_list())  # The spans are in reverse order.\n), \"Phoenix spans are in an unexpected order. Re-start the notebook and try again.\"\neval_scores_df.index = pd.Index(\n    list(reversed(spans_dataframe.index.to_list())), name=spans_dataframe.index.name\n)\n\n# Log the evaluations to Phoenix.\nfor eval_name in eval_scores_df.columns:\n    evals_df = eval_scores_df[[eval_name]].rename(columns={eval_name: \"score\"})\n    evals = SpanEvaluations(eval_name, evals_df)\n    px.Client().log_evaluations(evals)\n</pre> from phoenix.trace import SpanEvaluations  # Assign span ids to your ragas evaluation scores (needed so Phoenix knows where to attach the spans). eval_data_df = pd.DataFrame(evaluation_result.dataset) assert eval_data_df.question.to_list() == list(     reversed(spans_dataframe.input.to_list())  # The spans are in reverse order. ), \"Phoenix spans are in an unexpected order. Re-start the notebook and try again.\" eval_scores_df.index = pd.Index(     list(reversed(spans_dataframe.index.to_list())), name=spans_dataframe.index.name )  # Log the evaluations to Phoenix. for eval_name in eval_scores_df.columns:     evals_df = eval_scores_df[[eval_name]].rename(columns={eval_name: \"score\"})     evals = SpanEvaluations(eval_name, evals_df)     px.Client().log_evaluations(evals) <p>If you check out Phoenix, you'll see your Ragas evaluations as annotations on your application spans.</p> In\u00a0[\u00a0]: Copied! <pre>print(session.url)\n</pre> print(session.url) <p></p> <p>Embeddings encode the meaning of retrieved documents and user queries. Not only are they an essential part of RAG systems, but they are immensely useful for understanding and debugging LLM application performance.</p> <p>Phoenix takes the high-dimensional embeddings from your RAG application, reduces their dimensionality, and clusters them into semantically meaningful groups of data. You can then select the metric of your choice (e.g., Ragas-computed faithfulness or answer correctness) to visually inspect the performance of your application and surface problematic clusters. The advantage of this approach is that it provides metrics on granular yet meaningful subsets of your data that help you analyze local, not merely global, performance across a dataset. It's also helpful for gaining intuition around what kind of queries your LLM application is struggling to answer.</p> <p>We'll re-launch Phoenix as an embedding visualizer to inspect the performance of our application on our test dataset.</p> In\u00a0[\u00a0]: Copied! <pre>query_embeddings_df = query_embeddings_df.iloc[::-1]\nassert ragas_evals_df.question.tolist() == query_embeddings_df.text.tolist()\nassert test_df.question.tolist() == ragas_evals_df.question.tolist()\nquery_df = pd.concat(\n    [\n        ragas_evals_df[[\"question\", \"answer\", \"ground_truth\"]].reset_index(drop=True),\n        query_embeddings_df[[\"vector\"]].reset_index(drop=True),\n        test_df[[\"evolution_type\"]],\n        eval_scores_df.reset_index(drop=True),\n    ],\n    axis=1,\n)\nquery_df.head()\n</pre> query_embeddings_df = query_embeddings_df.iloc[::-1] assert ragas_evals_df.question.tolist() == query_embeddings_df.text.tolist() assert test_df.question.tolist() == ragas_evals_df.question.tolist() query_df = pd.concat(     [         ragas_evals_df[[\"question\", \"answer\", \"ground_truth\"]].reset_index(drop=True),         query_embeddings_df[[\"vector\"]].reset_index(drop=True),         test_df[[\"evolution_type\"]],         eval_scores_df.reset_index(drop=True),     ],     axis=1, ) query_df.head() In\u00a0[\u00a0]: Copied! <pre>query_schema = px.Schema(\n    prompt_column_names=px.EmbeddingColumnNames(\n        raw_data_column_name=\"question\", vector_column_name=\"vector\"\n    ),\n    response_column_names=\"answer\",\n)\ncorpus_schema = px.Schema(\n    prompt_column_names=px.EmbeddingColumnNames(\n        raw_data_column_name=\"text\", vector_column_name=\"vector\"\n    )\n)\n# relaunch phoenix with a primary and corpus dataset to view embeddings\npx.close_app()\nsession = px.launch_app(\n    primary=px.Dataset(query_df, query_schema, \"query\"),\n    corpus=px.Dataset(corpus_df.reset_index(drop=True), corpus_schema, \"corpus\"),\n)\n</pre> query_schema = px.Schema(     prompt_column_names=px.EmbeddingColumnNames(         raw_data_column_name=\"question\", vector_column_name=\"vector\"     ),     response_column_names=\"answer\", ) corpus_schema = px.Schema(     prompt_column_names=px.EmbeddingColumnNames(         raw_data_column_name=\"text\", vector_column_name=\"vector\"     ) ) # relaunch phoenix with a primary and corpus dataset to view embeddings px.close_app() session = px.launch_app(     primary=px.Dataset(query_df, query_schema, \"query\"),     corpus=px.Dataset(corpus_df.reset_index(drop=True), corpus_schema, \"corpus\"), ) <p>Once you launch Phoenix, you can visualize your data with the metric of your choice with the following steps:</p> <ul> <li>Select the <code>vector</code> embedding,</li> <li>Select <code>Color By &gt; dimension</code> and then the dimension of your choice to color your data by a particular field, for example, by Ragas evaluation scores such as faithfulness or answer correctness,</li> <li>Select the metric of your choice from the <code>metric</code> dropdown to view aggregate metrics on a per-cluster basis.</li> </ul> <p></p>"},{"location":"howtos/integrations/arize/#phoenix-arize","title":"Phoenix (Arize)\u00b6","text":""},{"location":"howtos/integrations/arize/#1-introduction","title":"1. Introduction\u00b6","text":"<p>Building a baseline for a RAG pipeline is not usually difficult, but enhancing it to make it suitable for production and ensuring the quality of your responses is almost always hard. Choosing the right tools and parameters for RAG can itself be challenging when there is an abundance of options available. This tutorial shares a robust workflow for making the right choices while building your RAG and ensuring its quality.</p> <p>This article covers how to evaluate, visualize and analyze your RAG using a combination of open-source libraries.  We will be using:</p> <ul> <li>Ragas for synthetic test data generation and evaluation</li> <li>Arize AI\u2019s Phoenix for tracing, visualization, and cluster analysis</li> <li>LlamaIndex for building RAG pipelines</li> </ul> <p>For the purpose of this article, we\u2019ll be using data from arXiv papers about prompt-engineering to build the RAG pipeline.</p> <p>\u2139\ufe0f This notebook requires an OpenAI API key.</p>"},{"location":"howtos/integrations/arize/#2-install-dependencies-and-import-libraries","title":"2. Install Dependencies and Import Libraries\u00b6","text":""},{"location":"howtos/integrations/arize/#3-configure-your-openai-api-key","title":"3. Configure Your OpenAI API Key\u00b6","text":"<p>Set your OpenAI API key if it is not already set as an environment variable.</p>"},{"location":"howtos/integrations/arize/#4-generate-your-synthetic-test-dataset","title":"4. Generate Your Synthetic Test Dataset\u00b6","text":""},{"location":"howtos/integrations/arize/#5-build-your-rag-application-with-llamaindex","title":"5. Build Your RAG Application With LlamaIndex\u00b6","text":""},{"location":"howtos/integrations/arize/#6-evaluate-your-llm-application","title":"6. Evaluate Your LLM Application\u00b6","text":""},{"location":"howtos/integrations/arize/#7-visualize-and-analyze-your-embeddings","title":"7. Visualize and Analyze Your Embeddings\u00b6","text":""},{"location":"howtos/integrations/arize/#8-recap","title":"8. Recap\u00b6","text":"<p>Congrats! You built and evaluated a LlamaIndex query engine using Ragas and Phoenix. Let's recap what we learned:</p> <ul> <li>With Ragas, you bootstraped a test dataset and computed metrics such as faithfulness and answer correctness to evaluate your LlamaIndex query engine.</li> <li>With OpenInference, you instrumented your query engine so you could observe the inner workings of both LlamaIndex and Ragas.</li> <li>With Phoenix, you collected your spans and traces, imported your evaluations for easy inspection, and visualized your embedded queries and retrieved documents to identify pockets of poor performance.</li> </ul> <p>This notebook is just an introduction to the capabilities of Ragas and Phoenix. To learn more, see the Ragas and Phoenix docs.</p> <p>If you enjoyed this tutorial, please leave a \u2b50 on GitHub:</p> <ul> <li>Ragas</li> <li>Phoenix</li> <li>OpenInference</li> </ul>"},{"location":"howtos/integrations/athina/","title":"Athina AI","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nfrom athina.evals import (\n    RagasAnswerCorrectness,\n    RagasAnswerRelevancy,\n    RagasContextRelevancy,\n    RagasFaithfulness,\n)\nfrom athina.loaders import RagasLoader\nfrom athina.keys import AthinaApiKey, OpenAiApiKey\nfrom athina.runner.run import EvalRunner\nimport pandas as pd\n\n# Set your API keys\nOpenAiApiKey.set_key(os.getenv(\"OPENAI_API_KEY\"))\nAthinaApiKey.set_key(os.getenv(\"ATHINA_API_KEY\"))\n\n# Load your dataset from a dictionary, json, or csv: https://docs.athina.ai/evals/loading_data\ndataset = RagasLoader().load_json(\"raw_data.json\")\n\n# Configure the eval suite\neval_model = \"gpt-3.5-turbo\"\neval_suite = [\n    RagasAnswerCorrectness(),\n    RagasFaithfulness(),\n    RagasContextRelevancy(),\n    RagasAnswerRelevancy(),\n]\n\n# Run the evaluation suite\nbatch_eval_result = EvalRunner.run_suite(\n    evals=eval_suite,\n    data=dataset,\n    max_parallel_evals=1,  # If you increase this, you may run into rate limits\n)\n\npd.DataFrame(batch_eval_result)\n</pre> import os from athina.evals import (     RagasAnswerCorrectness,     RagasAnswerRelevancy,     RagasContextRelevancy,     RagasFaithfulness, ) from athina.loaders import RagasLoader from athina.keys import AthinaApiKey, OpenAiApiKey from athina.runner.run import EvalRunner import pandas as pd  # Set your API keys OpenAiApiKey.set_key(os.getenv(\"OPENAI_API_KEY\")) AthinaApiKey.set_key(os.getenv(\"ATHINA_API_KEY\"))  # Load your dataset from a dictionary, json, or csv: https://docs.athina.ai/evals/loading_data dataset = RagasLoader().load_json(\"raw_data.json\")  # Configure the eval suite eval_model = \"gpt-3.5-turbo\" eval_suite = [     RagasAnswerCorrectness(),     RagasFaithfulness(),     RagasContextRelevancy(),     RagasAnswerRelevancy(), ]  # Run the evaluation suite batch_eval_result = EvalRunner.run_suite(     evals=eval_suite,     data=dataset,     max_parallel_evals=1,  # If you increase this, you may run into rate limits )  pd.DataFrame(batch_eval_result)"},{"location":"howtos/integrations/athina/#athina-ai","title":"Athina AI\u00b6","text":""},{"location":"howtos/integrations/athina/#ragas-metrics-on-your-production-logs","title":"Ragas Metrics on your Production Logs\u00b6","text":"<p>Athina is a production monitoring and evaluation platform. Try the sandbox here.</p> <p>You can use Athina with Ragas metrics to run evals on production logs, and get granular model performance metrics on your production data.</p> <p></p> <p>For example, you can get insights like this visually:</p> <ul> <li>What is my <code>AnswerRelevancy</code> score for queries related to <code>refunds</code> for customer id <code>nike-usa</code></li> <li>What is my <code>Faithfulness</code> score for <code>product catalog</code> queries using prompt <code>catalog_answerer/v3</code> with model <code>gpt-3.5-turbo</code></li> </ul>"},{"location":"howtos/integrations/athina/#running-athina-programmatically","title":"\u25b7 Running Athina Programmatically\u00b6","text":"<p>When you use Athina to run Ragas evals programmatically, you will be able to view the results on Athina's UI like this \ud83d\udc47</p> <p></p> <ol> <li>Install Athina's Python SDK:</li> </ol> <pre><code>pip install athina\n</code></pre> <ol> <li>Create an account at app.athina.ai. After signing up, you will receive an API key.</li> </ol> <p>Here's a sample notebook you can follow: https://github.com/athina-ai/athina-evals/blob/main/examples/ragas.ipynb</p> <ol> <li>Run the code</li> </ol>"},{"location":"howtos/integrations/athina/#configure-ragas-to-run-automatically-on-your-production-logs","title":"\u25b7 Configure Ragas to run automatically on your production logs\u00b6","text":"<p>If you are logging your production inferences to Athina, you can configure Ragas metrics to run automatically against your production logs.</p> <ol> <li><p>Navigate to the Athina Dashboard</p> </li> <li><p>Open the Evals page (lightning icon on the left)</p> </li> <li><p>Click the \"New Eval\" button on the top right</p> </li> <li><p>Select the Ragas tab</p> </li> <li><p>Select the eval you want to configure</p> </li> </ol> <p></p>"},{"location":"howtos/integrations/athina/#learn-more-about-athina","title":"Learn more about Athina\u00b6","text":"<ul> <li>Website: https://athina.ai</li> <li>Docs: https://docs.athina.ai</li> <li>Github Library: https://github.com/athina-ai/athina-evals</li> <li>Sandbox: https://demo.athina.ai</li> </ul>"},{"location":"howtos/integrations/haystack/","title":"Haystack","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install ragas-haystack\n</pre> !pip install ragas-haystack In\u00a0[1]: Copied! <pre>import os\nfrom getpass import getpass\n\n\nos.environ[\"OPENAI_API_KEY\"] = getpass(\"enter your OpenAI API key:\")\n</pre> import os from getpass import getpass   os.environ[\"OPENAI_API_KEY\"] = getpass(\"enter your OpenAI API key:\") <pre>enter your OpenAI API key:\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n</pre> In\u00a0[6]: Copied! <pre>from haystack import Pipeline\nfrom haystack_integrations.components.evaluators.ragas import (\n    RagasEvaluator,\n    RagasMetric,\n)\n\nevaluation_pipeline = Pipeline()\nevaluator_context = RagasEvaluator(\n    metric=RagasMetric.CONTEXT_PRECISION,\n)\nevaluator_aspect = RagasEvaluator(\n    metric=RagasMetric.ASPECT_CRITIQUE,\n    metric_params={\n        \"name\": \"custom\",\n        \"definition\": \"Is this answer problematic for children?\",\n        \"strictness\": 3,\n    },\n)\nevaluation_pipeline.add_component(\"evaluator_context\", evaluator_context)\nevaluation_pipeline.add_component(\"evaluator_aspect\", evaluator_aspect)\n</pre> from haystack import Pipeline from haystack_integrations.components.evaluators.ragas import (     RagasEvaluator,     RagasMetric, )  evaluation_pipeline = Pipeline() evaluator_context = RagasEvaluator(     metric=RagasMetric.CONTEXT_PRECISION, ) evaluator_aspect = RagasEvaluator(     metric=RagasMetric.ASPECT_CRITIQUE,     metric_params={         \"name\": \"custom\",         \"definition\": \"Is this answer problematic for children?\",         \"strictness\": 3,     }, ) evaluation_pipeline.add_component(\"evaluator_context\", evaluator_context) evaluation_pipeline.add_component(\"evaluator_aspect\", evaluator_aspect) <p>To run the evaluation Pipeline, you should have the expected inputs for the metrics ready at hand. These metrics expect a list of <code>questions</code>, <code>contexts</code>, <code>responses</code>, and <code>ground_truths</code>. These should come from the results of the <code>Pipeline</code> you want to evaluate.</p> In\u00a0[\u00a0]: Copied! <pre>QUESTIONS = [\n    \"Which is the most popular global sport?\",\n    \"Who created the Python language?\",\n]\nCONTEXTS = [\n    [\n        \"The popularity of sports can be measured in various ways, including TV viewership, social media presence, number of participants, and economic impact. Football is undoubtedly the world's most popular sport with major events like the FIFA World Cup and sports personalities like Ronaldo and Messi, drawing a followership of more than 4 billion people.\"\n    ],\n    [\n        \"Python, created by Guido van Rossum in the late 1980s, is a high-level general-purpose programming language. Its design philosophy emphasizes code readability, and its language constructs aim to help programmers write clear, logical code for both small and large-scale software projects.\"\n    ],\n]\nRESPONSES = [\n    \"Football is the most popular sport with around 4 billion followers worldwide\",\n    \"Python language was created by Guido van Rossum.\",\n]\nGROUND_TRUTHS = [\n    \"Football is the most popular sport\",\n    \"Python language was created by Guido van Rossum.\",\n]\n\nresults = evaluation_pipeline.run(\n    {\n        \"evaluator_context\": {\n            \"questions\": QUESTIONS,\n            \"contexts\": CONTEXTS,\n            \"ground_truths\": GROUND_TRUTHS,\n        },\n        \"evaluator_aspect\": {\n            \"questions\": QUESTIONS,\n            \"contexts\": CONTEXTS,\n            \"responses\": RESPONSES,\n        },\n    }\n)\n</pre> QUESTIONS = [     \"Which is the most popular global sport?\",     \"Who created the Python language?\", ] CONTEXTS = [     [         \"The popularity of sports can be measured in various ways, including TV viewership, social media presence, number of participants, and economic impact. Football is undoubtedly the world's most popular sport with major events like the FIFA World Cup and sports personalities like Ronaldo and Messi, drawing a followership of more than 4 billion people.\"     ],     [         \"Python, created by Guido van Rossum in the late 1980s, is a high-level general-purpose programming language. Its design philosophy emphasizes code readability, and its language constructs aim to help programmers write clear, logical code for both small and large-scale software projects.\"     ], ] RESPONSES = [     \"Football is the most popular sport with around 4 billion followers worldwide\",     \"Python language was created by Guido van Rossum.\", ] GROUND_TRUTHS = [     \"Football is the most popular sport\",     \"Python language was created by Guido van Rossum.\", ]  results = evaluation_pipeline.run(     {         \"evaluator_context\": {             \"questions\": QUESTIONS,             \"contexts\": CONTEXTS,             \"ground_truths\": GROUND_TRUTHS,         },         \"evaluator_aspect\": {             \"questions\": QUESTIONS,             \"contexts\": CONTEXTS,             \"responses\": RESPONSES,         },     } ) In\u00a0[\u00a0]: Copied! <pre>QUESTIONS = [\n    \"Which is the most popular global sport?\",\n    \"Who created the Python language?\",\n]\nCONTEXTS = [\n    [\n        \"The popularity of sports can be measured in various ways, including TV viewership, social media presence, number of participants, and economic impact. Football is undoubtedly the world's most popular sport with major events like the FIFA World Cup and sports personalities like Ronaldo and Messi, drawing a followership of more than 4 billion people.\"\n    ],\n    [\n        \"Python, created by Guido van Rossum in the late 1980s, is a high-level general-purpose programming language. Its design philosophy emphasizes code readability, and its language constructs aim to help programmers write clear, logical code for both small and large-scale software projects.\"\n    ],\n]\nRESPONSES = [\n    \"Football is the most popular sport with around 4 billion followers worldwide\",\n    \"Python language was created by Guido van Rossum.\",\n]\nGROUND_TRUTHS = [\n    \"Football is the most popular sport\",\n    \"Python language was created by Guido van Rossum.\",\n]\nresults = evaluation_pipeline.run(\n    {\n        \"evaluator_context\": {\n            \"questions\": QUESTIONS,\n            \"contexts\": CONTEXTS,\n            \"ground_truths\": GROUND_TRUTHS,\n        },\n        \"evaluator_aspect\": {\n            \"questions\": QUESTIONS,\n            \"contexts\": CONTEXTS,\n            \"responses\": RESPONSES,\n        },\n    }\n)\n</pre> QUESTIONS = [     \"Which is the most popular global sport?\",     \"Who created the Python language?\", ] CONTEXTS = [     [         \"The popularity of sports can be measured in various ways, including TV viewership, social media presence, number of participants, and economic impact. Football is undoubtedly the world's most popular sport with major events like the FIFA World Cup and sports personalities like Ronaldo and Messi, drawing a followership of more than 4 billion people.\"     ],     [         \"Python, created by Guido van Rossum in the late 1980s, is a high-level general-purpose programming language. Its design philosophy emphasizes code readability, and its language constructs aim to help programmers write clear, logical code for both small and large-scale software projects.\"     ], ] RESPONSES = [     \"Football is the most popular sport with around 4 billion followers worldwide\",     \"Python language was created by Guido van Rossum.\", ] GROUND_TRUTHS = [     \"Football is the most popular sport\",     \"Python language was created by Guido van Rossum.\", ] results = evaluation_pipeline.run(     {         \"evaluator_context\": {             \"questions\": QUESTIONS,             \"contexts\": CONTEXTS,             \"ground_truths\": GROUND_TRUTHS,         },         \"evaluator_aspect\": {             \"questions\": QUESTIONS,             \"contexts\": CONTEXTS,             \"responses\": RESPONSES,         },     } ) In\u00a0[11]: Copied! <pre>for component in [\"evaluator_context\", \"evaluator_aspect\"]:\n    for output in results[component][\"results\"]:\n        print(output)\n</pre> for component in [\"evaluator_context\", \"evaluator_aspect\"]:     for output in results[component][\"results\"]:         print(output) <pre>[{'name': 'context_precision', 'score': 0.9999999999}]\n[{'name': 'context_precision', 'score': 0.9999999999}]\n[{'name': 'custom', 'score': 0}]\n[{'name': 'custom', 'score': 0}]\n</pre> <p>You can use a Pandas dataframe to do additional analysis.</p> In\u00a0[14]: Copied! <pre>import pandas as pd\n\ndf = pd.DataFrame.from_dict(results)\nprint(df)\n</pre> import pandas as pd  df = pd.DataFrame.from_dict(results) print(df) <pre>                                         evaluator_context  \\\nresults  [[{'name': 'context_precision', 'score': 0.999...   \n\n                                          evaluator_aspect  \nresults  [[{'name': 'custom', 'score': 0}], [{'name': '...  \n</pre>"},{"location":"howtos/integrations/haystack/#haystack","title":"Haystack\u00b6","text":""},{"location":"howtos/integrations/haystack/#evaluating-haystack-with-ragas","title":"Evaluating Haystack with Ragas\u00b6","text":"<p>Haystack is an open-source framework for building production-ready LLM applications. The <code>RagasEvaluator</code> component evaluates Haystack Pipelines using LLM-based metrics. It supports metrics like context relevance, factual accuracy, response relevance, and more.</p> <p>This guide will show you how to use the <code>RagasEvaluator</code> with a Haystack pipeline.</p>"},{"location":"howtos/integrations/haystack/#installation","title":"Installation\u00b6","text":""},{"location":"howtos/integrations/haystack/#setting-your-openai_api_key-environment-variable","title":"Setting your <code>OPENAI_API_KEY</code> environment variable\u00b6","text":"<p>Many metrics use OpenAI models and require an environment variable <code>OPENAI_API_KEY</code> to be set. If you don't have an OpenAI API key, you can sign up for an account here.</p>"},{"location":"howtos/integrations/haystack/#use-the-ragasevaluator-in-a-haystack-pipeline","title":"Use the RagasEvaluator in a Haystack pipeline\u00b6","text":"<p>To use the\u00a0<code>RagasEvaluator</code>\u00a0you need to follow these steps:</p> <ol> <li>Initialize the\u00a0<code>RagasEvaluator</code>\u00a0while providing the correct\u00a0<code>metric_params</code>\u00a0for the metric you are using.</li> <li>Run the\u00a0<code>RagasEvaluator</code>, either on its own or in a Pipeline, by providing the expected input for the metric you are using.</li> </ol> <p>To create a Pipeline that evaluates context relevance and aspect critique:</p>"},{"location":"howtos/integrations/helicone/","title":"Helicone","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install datasets ragas openai\n</pre> !pip install datasets ragas openai In\u00a0[\u00a0]: Copied! <pre>import os\nfrom datasets import Dataset\nfrom ragas import evaluate\nfrom ragas.metrics import faithfulness, answer_relevancy, context_precision\nfrom ragas.integrations.helicone import helicone_config  # import helicone_config\n\n\n# Set up Helicone\nhelicone_config.api_key = (\n    \"your_helicone_api_key_here\"  # Replace with your actual Helicone API key\n)\nos.environ[\"OPENAI_API_KEY\"] = (\n    \"your_openai_api_key_here\"  # Replace with your actual OpenAI API key\n)\n\n# Verify Helicone API key is set\nif HELICONE_API_KEY == \"your_helicone_api_key_here\":\n    raise ValueError(\n        \"Please replace 'your_helicone_api_key_here' with your actual Helicone API key.\"\n    )\n</pre> import os from datasets import Dataset from ragas import evaluate from ragas.metrics import faithfulness, answer_relevancy, context_precision from ragas.integrations.helicone import helicone_config  # import helicone_config   # Set up Helicone helicone_config.api_key = (     \"your_helicone_api_key_here\"  # Replace with your actual Helicone API key ) os.environ[\"OPENAI_API_KEY\"] = (     \"your_openai_api_key_here\"  # Replace with your actual OpenAI API key )  # Verify Helicone API key is set if HELICONE_API_KEY == \"your_helicone_api_key_here\":     raise ValueError(         \"Please replace 'your_helicone_api_key_here' with your actual Helicone API key.\"     ) In\u00a0[\u00a0]: Copied! <pre>data_samples = {\n    \"question\": [\"When was the first Super Bowl?\", \"Who has won the most Super Bowls?\"],\n    \"answer\": [\n        \"The first Super Bowl was held on January 15, 1967.\",\n        \"The New England Patriots have won the most Super Bowls, with six championships.\",\n    ],\n    \"contexts\": [\n        [\n            \"The First AFL\u2013NFL World Championship Game, later known as Super Bowl I, was played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles, California.\"\n        ],\n        [\n            \"As of 2021, the New England Patriots have won the most Super Bowls with six championships, all under the leadership of quarterback Tom Brady and head coach Bill Belichick.\"\n        ],\n    ],\n    \"ground_truth\": [\n        \"The first Super Bowl was held on January 15, 1967.\",\n        \"The New England Patriots have won the most Super Bowls, with six championships as of 2021.\",\n    ],\n}\n\ndataset = Dataset.from_dict(data_samples)\nprint(dataset)\n</pre> data_samples = {     \"question\": [\"When was the first Super Bowl?\", \"Who has won the most Super Bowls?\"],     \"answer\": [         \"The first Super Bowl was held on January 15, 1967.\",         \"The New England Patriots have won the most Super Bowls, with six championships.\",     ],     \"contexts\": [         [             \"The First AFL\u2013NFL World Championship Game, later known as Super Bowl I, was played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles, California.\"         ],         [             \"As of 2021, the New England Patriots have won the most Super Bowls with six championships, all under the leadership of quarterback Tom Brady and head coach Bill Belichick.\"         ],     ],     \"ground_truth\": [         \"The first Super Bowl was held on January 15, 1967.\",         \"The New England Patriots have won the most Super Bowls, with six championships as of 2021.\",     ], }  dataset = Dataset.from_dict(data_samples) print(dataset) In\u00a0[\u00a0]: Copied! <pre># Evaluate using Ragas\nscore = evaluate(dataset, metrics=[faithfulness, answer_relevancy, context_precision])\n\n# Display results\nprint(score.to_pandas())\n</pre> # Evaluate using Ragas score = evaluate(dataset, metrics=[faithfulness, answer_relevancy, context_precision])  # Display results print(score.to_pandas())"},{"location":"howtos/integrations/helicone/#helicone","title":"Helicone\u00b6","text":"<p>This notebook demonstrates how to integrate Helicone with Ragas for monitoring and evaluating RAG (Retrieval-Augmented Generation) systems.</p>"},{"location":"howtos/integrations/helicone/#prerequisites","title":"Prerequisites\u00b6","text":"<p>Before you begin, make sure you have a Helicone account and API key:</p> <ol> <li>Log into Helicone or create an account if you don't have one.</li> <li>Once logged in, navigate to the Developer section to generate an API key.</li> </ol> <p>Note: Make sure to generate a write-only API key. For more information on Helicone authentication, refer to the Helicone Auth documentation.</p> <p>Store your Helicone API key securely, as you'll need it for the integration.</p>"},{"location":"howtos/integrations/helicone/#setup","title":"Setup\u00b6","text":"<p>First, let's install the required packages and set up our environment.</p>"},{"location":"howtos/integrations/helicone/#prepare-data","title":"Prepare Data\u00b6","text":"<p>Let's prepare some sample data for our RAG system evaluation.</p>"},{"location":"howtos/integrations/helicone/#evaluate-with-ragas","title":"Evaluate with Ragas\u00b6","text":"<p>Now, let's use Ragas to evaluate our RAG system. Helicone will automatically log the API calls made during this evaluation.</p>"},{"location":"howtos/integrations/helicone/#viewing-results-in-helicone","title":"Viewing Results in Helicone\u00b6","text":"<p>The API calls made during the Ragas evaluation are automatically logged in Helicone. You can view these logs in the Helicone dashboard to get insights into the performance and behavior of your RAG system.</p> <p>To view the results:</p> <ol> <li>Go to the Helicone dashboard</li> <li>Navigate to the 'Requests' section</li> <li>You should see the API calls made during the Ragas evaluation</li> </ol> <p>You can analyze these logs to understand:</p> <ul> <li>The number of API calls made during evaluation</li> <li>The performance of each call (latency, tokens used, etc.)</li> <li>Any errors or issues that occurred during the evaluation</li> </ul> <p>This integration allows you to combine the power of Ragas for RAG system evaluation with Helicone's robust monitoring and analytics capabilities.</p>"},{"location":"howtos/integrations/langchain/","title":"Langchain","text":"In\u00a0[\u00a0]: Copied! <pre>#!pip install ragas langchain_openai python-dotenv\n</pre> #!pip install ragas langchain_openai python-dotenv In\u00a0[\u00a0]: Copied! <pre># attach to the existing event loop when using jupyter notebooks\nimport nest_asyncio\nimport os\nimport openai\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n# IMPORTANT: Remember to create a .env variable containing: OPENAI_API_KEY=sk-xyz where xyz is your key\n\n# Access the API key from the environment variable\napi_key = os.environ.get(\"OPENAI_API_KEY\")\n\n# Initialize the OpenAI API client\nopenai.api_key = api_key\n\nnest_asyncio.apply()\n</pre> # attach to the existing event loop when using jupyter notebooks import nest_asyncio import os import openai from dotenv import load_dotenv  # Load environment variables from .env file load_dotenv() # IMPORTANT: Remember to create a .env variable containing: OPENAI_API_KEY=sk-xyz where xyz is your key  # Access the API key from the environment variable api_key = os.environ.get(\"OPENAI_API_KEY\")  # Initialize the OpenAI API client openai.api_key = api_key  nest_asyncio.apply() <p>First lets load the dataset. We are going to build a generic QA system over the NYC wikipedia page. Load the dataset and create the <code>VectorstoreIndex</code> and the <code>RetrievalQA</code> from it.</p> In\u00a0[\u00a0]: Copied! <pre>from langchain_community.document_loaders import TextLoader\nfrom langchain.indexes import VectorstoreIndexCreator\nfrom langchain.chains import RetrievalQA\nfrom langchain_openai import ChatOpenAI\n\nloader = TextLoader(\"./nyc_wikipedia/nyc_text.txt\")\nindex = VectorstoreIndexCreator().from_loaders([loader])\n\n\nllm = ChatOpenAI(temperature=0)\nqa_chain = RetrievalQA.from_chain_type(\n    llm,\n    retriever=index.vectorstore.as_retriever(),\n    return_source_documents=True,\n)\n</pre> from langchain_community.document_loaders import TextLoader from langchain.indexes import VectorstoreIndexCreator from langchain.chains import RetrievalQA from langchain_openai import ChatOpenAI  loader = TextLoader(\"./nyc_wikipedia/nyc_text.txt\") index = VectorstoreIndexCreator().from_loaders([loader])   llm = ChatOpenAI(temperature=0) qa_chain = RetrievalQA.from_chain_type(     llm,     retriever=index.vectorstore.as_retriever(),     return_source_documents=True, ) In\u00a0[\u00a0]: Copied! <pre># testing it out\n\nquestion = \"How did New York City get its name?\"\nresult = qa_chain({\"query\": question})\nresult[\"result\"]\n</pre> # testing it out  question = \"How did New York City get its name?\" result = qa_chain({\"query\": question}) result[\"result\"] <p>Now in order to evaluate the qa system we generated a few relevant questions. We've generated a few question for you but feel free to add any you want.</p> In\u00a0[\u00a0]: Copied! <pre>eval_questions = [\n    \"What is the population of New York City as of 2020?\",\n    \"Which borough of New York City has the highest population?\",\n    \"What is the economic significance of New York City?\",\n    \"How did New York City get its name?\",\n    \"What is the significance of the Statue of Liberty in New York City?\",\n]\n\neval_answers = [\n    \"8,804,190\",\n    \"Brooklyn\",\n    \"New York City's economic significance is vast, as it serves as the global financial capital, housing Wall Street and major financial institutions. Its diverse economy spans technology, media, healthcare, education, and more, making it resilient to economic fluctuations. NYC is a hub for international business, attracting global companies, and boasts a large, skilled labor force. Its real estate market, tourism, cultural industries, and educational institutions further fuel its economic prowess. The city's transportation network and global influence amplify its impact on the world stage, solidifying its status as a vital economic player and cultural epicenter.\",\n    \"New York City got its name when it came under British control in 1664. King Charles II of England granted the lands to his brother, the Duke of York, who named the city New York in his own honor.\",\n    \"The Statue of Liberty in New York City holds great significance as a symbol of the United States and its ideals of liberty and peace. It greeted millions of immigrants who arrived in the U.S. by ship in the late 19th and early 20th centuries, representing hope and freedom for those seeking a better life. It has since become an iconic landmark and a global symbol of cultural diversity and freedom.\",\n]\n\nexamples = [\n    {\"query\": q, \"ground_truth\": [eval_answers[i]]}\n    for i, q in enumerate(eval_questions)\n]\n</pre> eval_questions = [     \"What is the population of New York City as of 2020?\",     \"Which borough of New York City has the highest population?\",     \"What is the economic significance of New York City?\",     \"How did New York City get its name?\",     \"What is the significance of the Statue of Liberty in New York City?\", ]  eval_answers = [     \"8,804,190\",     \"Brooklyn\",     \"New York City's economic significance is vast, as it serves as the global financial capital, housing Wall Street and major financial institutions. Its diverse economy spans technology, media, healthcare, education, and more, making it resilient to economic fluctuations. NYC is a hub for international business, attracting global companies, and boasts a large, skilled labor force. Its real estate market, tourism, cultural industries, and educational institutions further fuel its economic prowess. The city's transportation network and global influence amplify its impact on the world stage, solidifying its status as a vital economic player and cultural epicenter.\",     \"New York City got its name when it came under British control in 1664. King Charles II of England granted the lands to his brother, the Duke of York, who named the city New York in his own honor.\",     \"The Statue of Liberty in New York City holds great significance as a symbol of the United States and its ideals of liberty and peace. It greeted millions of immigrants who arrived in the U.S. by ship in the late 19th and early 20th centuries, representing hope and freedom for those seeking a better life. It has since become an iconic landmark and a global symbol of cultural diversity and freedom.\", ]  examples = [     {\"query\": q, \"ground_truth\": [eval_answers[i]]}     for i, q in enumerate(eval_questions) ] In\u00a0[\u00a0]: Copied! <pre>result = qa_chain({\"query\": eval_questions[1]})\nresult[\"result\"]\n</pre> result = qa_chain({\"query\": eval_questions[1]}) result[\"result\"] In\u00a0[\u00a0]: Copied! <pre>result = qa_chain(examples[4])\nresult[\"result\"]\n</pre> result = qa_chain(examples[4]) result[\"result\"] In\u00a0[\u00a0]: Copied! <pre>from ragas.langchain.evalchain import RagasEvaluatorChain\nfrom ragas.metrics import (\n    faithfulness,\n    answer_relevancy,\n    context_precision,\n    context_recall,\n)\n\n# create evaluation chains\nfaithfulness_chain = RagasEvaluatorChain(metric=faithfulness)\nanswer_rel_chain = RagasEvaluatorChain(metric=answer_relevancy)\ncontext_rel_chain = RagasEvaluatorChain(metric=context_precision)\ncontext_recall_chain = RagasEvaluatorChain(metric=context_recall)\n</pre> from ragas.langchain.evalchain import RagasEvaluatorChain from ragas.metrics import (     faithfulness,     answer_relevancy,     context_precision,     context_recall, )  # create evaluation chains faithfulness_chain = RagasEvaluatorChain(metric=faithfulness) answer_rel_chain = RagasEvaluatorChain(metric=answer_relevancy) context_rel_chain = RagasEvaluatorChain(metric=context_precision) context_recall_chain = RagasEvaluatorChain(metric=context_recall) <ol> <li><code>__call__()</code></li> </ol> <p>Directly run the evaluation chain with the results from the QA chain. Do note that metrics like context_precision and faithfulness require the <code>source_documents</code> to be present.</p> In\u00a0[\u00a0]: Copied! <pre># Recheck the result that we are going to validate.\nresult\n</pre> # Recheck the result that we are going to validate. result <p>Faithfulness</p> In\u00a0[\u00a0]: Copied! <pre>eval_result = faithfulness_chain(result)\neval_result[\"faithfulness_score\"]\n</pre> eval_result = faithfulness_chain(result) eval_result[\"faithfulness_score\"] <p>High faithfulness_score means that there are exact consistency between the source documents and the answer.</p> <p>You can check lower faithfulness scores by changing the result (answer from LLM) or source_documents to something else.</p> In\u00a0[\u00a0]: Copied! <pre>fake_result = result.copy()\nfake_result[\"result\"] = \"we are the champions\"\neval_result = faithfulness_chain(fake_result)\neval_result[\"faithfulness_score\"]\n</pre> fake_result = result.copy() fake_result[\"result\"] = \"we are the champions\" eval_result = faithfulness_chain(fake_result) eval_result[\"faithfulness_score\"] <p>Context Recall</p> In\u00a0[\u00a0]: Copied! <pre>eval_result = context_recall_chain(result)\neval_result[\"context_recall_score\"]\n</pre> eval_result = context_recall_chain(result) eval_result[\"context_recall_score\"] <p>High context_recall_score means that the ground truth is present in the source documents.</p> <p>You can check lower context recall scores by changing the source_documents to something else.</p> In\u00a0[\u00a0]: Copied! <pre>from langchain.schema import Document\n\nfake_result = result.copy()\nfake_result[\"source_documents\"] = [Document(page_content=\"I love christmas\")]\neval_result = context_recall_chain(fake_result)\neval_result[\"context_recall_score\"]\n</pre> from langchain.schema import Document  fake_result = result.copy() fake_result[\"source_documents\"] = [Document(page_content=\"I love christmas\")] eval_result = context_recall_chain(fake_result) eval_result[\"context_recall_score\"] <ol> <li><code>evaluate()</code></li> </ol> <p>Evaluate a list of inputs/queries and the outputs/predictions from the QA chain.</p> In\u00a0[\u00a0]: Copied! <pre># run the queries as a batch for efficiency\npredictions = qa_chain.batch(examples)\n\n# evaluate\nprint(\"evaluating...\")\nr = faithfulness_chain.evaluate(examples, predictions)\nr\n</pre> # run the queries as a batch for efficiency predictions = qa_chain.batch(examples)  # evaluate print(\"evaluating...\") r = faithfulness_chain.evaluate(examples, predictions) r In\u00a0[\u00a0]: Copied! <pre># evaluate context recall\nprint(\"evaluating...\")\nr = context_recall_chain.evaluate(examples, predictions)\nr\n</pre> # evaluate context recall print(\"evaluating...\") r = context_recall_chain.evaluate(examples, predictions) r In\u00a0[\u00a0]: Copied! <pre># dataset creation\n\nfrom langsmith import Client\nfrom langsmith.utils import LangSmithError\n\nclient = Client()\ndataset_name = \"NYC test\"\n\ntry:\n    # check if dataset exists\n    dataset = client.read_dataset(dataset_name=dataset_name)\n    print(\"using existing dataset: \", dataset.name)\nexcept LangSmithError:\n    # if not create a new one with the generated query examples\n    dataset = client.create_dataset(\n        dataset_name=dataset_name, description=\"NYC test dataset\"\n    )\n    for e in examples:\n        client.create_example(\n            inputs={\"query\": e[\"query\"]},\n            outputs={\"ground_truth\": e[\"ground_truth\"]},\n            dataset_id=dataset.id,\n        )\n\n    print(\"Created a new dataset: \", dataset.name)\n</pre> # dataset creation  from langsmith import Client from langsmith.utils import LangSmithError  client = Client() dataset_name = \"NYC test\"  try:     # check if dataset exists     dataset = client.read_dataset(dataset_name=dataset_name)     print(\"using existing dataset: \", dataset.name) except LangSmithError:     # if not create a new one with the generated query examples     dataset = client.create_dataset(         dataset_name=dataset_name, description=\"NYC test dataset\"     )     for e in examples:         client.create_example(             inputs={\"query\": e[\"query\"]},             outputs={\"ground_truth\": e[\"ground_truth\"]},             dataset_id=dataset.id,         )      print(\"Created a new dataset: \", dataset.name) <p></p> <p>As you can see the questions have been uploaded. Now you can run your QA chain against this test dataset and compare the results in the langchain platform.</p> <p>Before you call <code>run_on_dataset</code> you need a factory function which creates a new instance of the QA chain you want to test. This is so that the internal state is not reused when running against each example.</p> In\u00a0[\u00a0]: Copied! <pre># factory function that return a new qa chain\ndef create_qa_chain(return_context=True):\n    qa_chain = RetrievalQA.from_chain_type(\n        llm,\n        retriever=index.vectorstore.as_retriever(),\n        return_source_documents=return_context,\n    )\n    return qa_chain\n</pre> # factory function that return a new qa chain def create_qa_chain(return_context=True):     qa_chain = RetrievalQA.from_chain_type(         llm,         retriever=index.vectorstore.as_retriever(),         return_source_documents=return_context,     )     return qa_chain <p>Now lets run the evaluation</p> In\u00a0[\u00a0]: Copied! <pre>from langchain.smith import RunEvalConfig, run_on_dataset\n\nevaluation_config = RunEvalConfig(\n    custom_evaluators=[\n        faithfulness_chain,\n        answer_rel_chain,\n        context_rel_chain,\n        context_recall_chain,\n    ],\n    prediction_key=\"result\",\n)\n\nresult = run_on_dataset(\n    client,\n    dataset_name,\n    create_qa_chain,\n    evaluation=evaluation_config,\n    input_mapper=lambda x: x,\n)\n</pre> from langchain.smith import RunEvalConfig, run_on_dataset  evaluation_config = RunEvalConfig(     custom_evaluators=[         faithfulness_chain,         answer_rel_chain,         context_rel_chain,         context_recall_chain,     ],     prediction_key=\"result\", )  result = run_on_dataset(     client,     dataset_name,     create_qa_chain,     evaluation=evaluation_config,     input_mapper=lambda x: x, ) <p>You can follow the link to open the result for the run in langsmith. Check out the scores for each example too</p> <p></p> <p>Now if you want to dive more into the reasons for the scores and how to improve them, click on any example and open the feedback tab. This will show you each scores.</p> <p></p> <p>You can also see the curresponding <code>RagasEvaluatorChain</code> trace too to figure out why ragas scored the way it did.</p> <p></p>"},{"location":"howtos/integrations/langchain/#langchain","title":"Langchain\u00b6","text":""},{"location":"howtos/integrations/langchain/#evaluating-langchain-qa-chains","title":"Evaluating Langchain QA Chains\u00b6","text":"<p>LangChain is a framework for developing applications powered by language models. It can also be used to create RAG systems (or QA systems as they are reffered to in langchain). If you want to know more about creating RAG systems with langchain you can check the docs.</p> <p>With this integration you can easily evaluate your QA chains with the metrics offered in ragas</p>"},{"location":"howtos/integrations/langchain/#introducing-ragasevaluatorchain","title":"Introducing <code>RagasEvaluatorChain</code>\u00b6","text":"<p><code>RagasEvaluatorChain</code> creates a wrapper around the metrics ragas provides (documented here), making it easier to run these evaluation with langchain and langsmith.</p> <p>The evaluator chain has the following APIs</p> <ul> <li><code>__call__()</code>: call the <code>RagasEvaluatorChain</code> directly on the result of a QA chain.</li> <li><code>evaluate()</code>: evaluate on a list of examples (with the input queries) and predictions (outputs from the QA chain).</li> <li><code>evaluate_run()</code>: method implemented that is called by langsmith evaluators to evaluate langsmith datasets.</li> </ul> <p>lets see each of them in action to learn more.</p>"},{"location":"howtos/integrations/langchain/#evaluate-with-langsmith","title":"Evaluate with langsmith\u00b6","text":"<p>Langsmith is a platform that helps to debug, test, evaluate and monitor chains and agents built on any LLM framework. It also seamlessly integrates with LangChain.</p> <p>Langsmith also has a tools to build a testing dataset and run evaluations against them and with <code>RagasEvaluatorChain</code> you can use the ragas metrics for running langsmith evaluations as well. To know more about langsmith evaluations checkout the quickstart.</p> <p>Lets start of creating the dataset with the NYC questions listed in <code>eval_questions</code>. Create a new langsmith dataset and upload the questions.</p>"},{"location":"howtos/integrations/langfuse/","title":"Langfuse","text":"In\u00a0[2]: Copied! <pre>import os\n\n# get keys for your project from https://cloud.langfuse.com\nos.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-...\"\nos.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-...\"\n\n# your openai key\n# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n</pre> import os  # get keys for your project from https://cloud.langfuse.com os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-...\" os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-...\"  # your openai key # os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" In\u00a0[\u00a0]: Copied! <pre>%pip install datasets ragas llama_index python-dotenv --upgrade\n</pre> %pip install datasets ragas llama_index python-dotenv --upgrade In\u00a0[2]: Copied! <pre>from datasets import load_dataset\n\namnesty_qa = load_dataset(\"explodinggradients/amnesty_qa\", \"english_v2\")[\"eval\"]\namnesty_qa\n</pre> from datasets import load_dataset  amnesty_qa = load_dataset(\"explodinggradients/amnesty_qa\", \"english_v2\")[\"eval\"] amnesty_qa <pre>Found cached dataset amnesty_qa (/home/jjmachan/.cache/huggingface/datasets/explodinggradients___amnesty_qa/english_v2/2.0.0/d0ed9800191a31943ee52a5c22ee4305e28a33f5edcd9a323802112cff07cc24)\n</pre> <pre>  0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> Out[2]: <pre>Dataset({\n    features: ['question', 'ground_truth', 'answer', 'contexts'],\n    num_rows: 20\n})</pre> In\u00a0[3]: Copied! <pre># import metrics\nfrom ragas.metrics import faithfulness, answer_relevancy, context_precision\nfrom ragas.metrics.critique import SUPPORTED_ASPECTS, harmfulness\n\n# metrics you chose\nmetrics = [faithfulness, answer_relevancy, context_precision, harmfulness]\n</pre> # import metrics from ragas.metrics import faithfulness, answer_relevancy, context_precision from ragas.metrics.critique import SUPPORTED_ASPECTS, harmfulness  # metrics you chose metrics = [faithfulness, answer_relevancy, context_precision, harmfulness] <p>Next, initialize the metrics using the LLMs and Embeddings of your choice. In this example, we are using OpenAI.</p> In\u00a0[4]: Copied! <pre>from ragas.run_config import RunConfig\nfrom ragas.metrics.base import MetricWithLLM, MetricWithEmbeddings\n\n\n# util function to init Ragas Metrics\ndef init_ragas_metrics(metrics, llm, embedding):\n    for metric in metrics:\n        if isinstance(metric, MetricWithLLM):\n            metric.llm = llm\n        if isinstance(metric, MetricWithEmbeddings):\n            metric.embeddings = embedding\n        run_config = RunConfig()\n        metric.init(run_config)\n</pre> from ragas.run_config import RunConfig from ragas.metrics.base import MetricWithLLM, MetricWithEmbeddings   # util function to init Ragas Metrics def init_ragas_metrics(metrics, llm, embedding):     for metric in metrics:         if isinstance(metric, MetricWithLLM):             metric.llm = llm         if isinstance(metric, MetricWithEmbeddings):             metric.embeddings = embedding         run_config = RunConfig()         metric.init(run_config) In\u00a0[5]: Copied! <pre>from langchain_openai.chat_models import ChatOpenAI\nfrom langchain_openai.embeddings import OpenAIEmbeddings\n\n# wrappers\nfrom ragas.llms import LangchainLLMWrapper\nfrom ragas.embeddings import LangchainEmbeddingsWrapper\n\nllm = ChatOpenAI()\nemb = OpenAIEmbeddings()\n\ninit_ragas_metrics(\n    metrics,\n    llm=LangchainLLMWrapper(llm),\n    embedding=LangchainEmbeddingsWrapper(emb),\n)\n</pre> from langchain_openai.chat_models import ChatOpenAI from langchain_openai.embeddings import OpenAIEmbeddings  # wrappers from ragas.llms import LangchainLLMWrapper from ragas.embeddings import LangchainEmbeddingsWrapper  llm = ChatOpenAI() emb = OpenAIEmbeddings()  init_ragas_metrics(     metrics,     llm=LangchainLLMWrapper(llm),     embedding=LangchainEmbeddingsWrapper(emb), ) In\u00a0[6]: Copied! <pre>row = amnesty_qa[0]\nprint(\"question: \", row[\"question\"])\nprint(\"answer: \", row[\"answer\"])\n</pre> row = amnesty_qa[0] print(\"question: \", row[\"question\"]) print(\"answer: \", row[\"answer\"]) <pre>question:  What are the global implications of the USA Supreme Court ruling on abortion?\nanswer:  The global implications of the USA Supreme Court ruling on abortion can be significant, as it sets a precedent for other countries and influences the global discourse on reproductive rights. Here are some potential implications:\n\n1. Influence on other countries: The Supreme Court's ruling can serve as a reference point for other countries grappling with their own abortion laws. It can provide legal arguments and reasoning that advocates for reproductive rights can use to challenge restrictive abortion laws in their respective jurisdictions.\n\n2. Strengthening of global reproductive rights movements: A favorable ruling by the Supreme Court can energize and empower reproductive rights movements worldwide. It can serve as a rallying point for activists and organizations advocating for women's rights, leading to increased mobilization and advocacy efforts globally.\n\n3. Counteracting anti-abortion movements: Conversely, a ruling that restricts abortion rights can embolden anti-abortion movements globally. It can provide legitimacy to their arguments and encourage similar restrictive measures in other countries, potentially leading to a rollback of existing reproductive rights.\n\n4. Impact on international aid and policies: The Supreme Court's ruling can influence international aid and policies related to reproductive health. It can shape the priorities and funding decisions of donor countries and organizations, potentially leading to increased support for reproductive rights initiatives or conversely, restrictions on funding for abortion-related services.\n\n5. Shaping international human rights standards: The ruling can contribute to the development of international human rights standards regarding reproductive rights. It can influence the interpretation and application of existing human rights treaties and conventions, potentially strengthening the recognition of reproductive rights as fundamental human rights globally.\n\n6. Global health implications: The Supreme Court's ruling can have implications for global health outcomes, particularly in countries with restrictive abortion laws. It can impact the availability and accessibility of safe and legal abortion services, potentially leading to an increase in unsafe abortions and related health complications.\n\nIt is important to note that the specific implications will depend on the nature of the Supreme Court ruling and the subsequent actions taken by governments, activists, and organizations both within and outside the United States.\n</pre> <p>Now lets init a Langfuse client SDK to instrument you app.</p> In\u00a0[7]: Copied! <pre>from langfuse import Langfuse\n\nlangfuse = Langfuse()\n</pre> from langfuse import Langfuse  langfuse = Langfuse() <p>Here we are defining a utility function to score your trace with the metrics you chose.</p> In\u00a0[22]: Copied! <pre>async def score_with_ragas(query, chunks, answer):\n    scores = {}\n    for m in metrics:\n        print(f\"calculating {m.name}\")\n        scores[m.name] = await m.ascore(\n            row={\"question\": query, \"contexts\": chunks, \"answer\": answer}\n        )\n    return scores\n</pre> async def score_with_ragas(query, chunks, answer):     scores = {}     for m in metrics:         print(f\"calculating {m.name}\")         scores[m.name] = await m.ascore(             row={\"question\": query, \"contexts\": chunks, \"answer\": answer}         )     return scores In\u00a0[23]: Copied! <pre>question, contexts, answer = row[\"question\"], row[\"contexts\"], row[\"answer\"]\nawait score_with_ragas(question, contexts, answer)\n</pre> question, contexts, answer = row[\"question\"], row[\"contexts\"], row[\"answer\"] await score_with_ragas(question, contexts, answer) <pre>calculating faithfulness\ncalculating answer_relevancy\n</pre> <pre>Using 'context_precision' without ground truth will be soon depreciated. Use 'context_utilization' instead\n</pre> <pre>calculating context_precision\ncalculating harmfulness\n</pre> Out[23]: <pre>{'faithfulness': 0.0,\n 'answer_relevancy': 0.9999999999999996,\n 'context_precision': 0.9999999999,\n 'harmfulness': 0}</pre> <p>You compute the score with each request. Below we've outlined a dummy application that does the following steps:</p> <ol> <li>Gets a question from the user</li> <li>Fetch context from the database or vector store that can be used to answer the question from the user</li> <li>Pass the question and the contexts to the LLM to generate the answer</li> </ol> <p>All these step are logged as spans in a single trace in Langfuse. You can read more about traces and spans from the Langfuse documentation.</p> In\u00a0[13]: Copied! <pre># the logic of the dummy application is\n# given a question fetch the correspoinding contexts and answers from a dict\n\nimport hashlib\n\n\ndef hash_string(input_string):\n    return hashlib.sha256(input_string.encode()).hexdigest()\n\n\nq_to_c = {}  # map between question and context\nq_to_a = {}  # map between question and answer\nfor row in amnesty_qa:\n    q_hash = hash_string(row[\"question\"])\n    q_to_c[q_hash] = row[\"contexts\"]\n    q_to_a[q_hash] = row[\"answer\"]\n</pre> # the logic of the dummy application is # given a question fetch the correspoinding contexts and answers from a dict  import hashlib   def hash_string(input_string):     return hashlib.sha256(input_string.encode()).hexdigest()   q_to_c = {}  # map between question and context q_to_a = {}  # map between question and answer for row in amnesty_qa:     q_hash = hash_string(row[\"question\"])     q_to_c[q_hash] = row[\"contexts\"]     q_to_a[q_hash] = row[\"answer\"] In\u00a0[28]: Copied! <pre># if your running this in a notebook - please run this cell\n# to manage asyncio event loops\nimport nest_asyncio\n\nnest_asyncio.apply()\n</pre> # if your running this in a notebook - please run this cell # to manage asyncio event loops import nest_asyncio  nest_asyncio.apply() In\u00a0[32]: Copied! <pre>from langfuse.decorators import observe, langfuse_context\nfrom asyncio import run\n\n\n@observe()\ndef retriver(question: str):\n    return q_to_c[question]\n\n\n@observe()\ndef generator(question):\n    return q_to_a[question]\n\n\n@observe()\ndef rag_pipeline(question):\n    q_hash = hash_string(question)\n    contexts = retriver(q_hash)\n    generated_answer = generator(q_hash)\n\n    # score the runs\n    score = run(score_with_ragas(question, contexts, answer=generated_answer))\n    for s in score:\n        langfuse_context.score_current_trace(name=s, value=score[s])\n    return generated_answer\n</pre> from langfuse.decorators import observe, langfuse_context from asyncio import run   @observe() def retriver(question: str):     return q_to_c[question]   @observe() def generator(question):     return q_to_a[question]   @observe() def rag_pipeline(question):     q_hash = hash_string(question)     contexts = retriver(q_hash)     generated_answer = generator(q_hash)      # score the runs     score = run(score_with_ragas(question, contexts, answer=generated_answer))     for s in score:         langfuse_context.score_current_trace(name=s, value=score[s])     return generated_answer In\u00a0[33]: Copied! <pre>question, contexts, answer = row[\"question\"], row[\"contexts\"], row[\"answer\"]\ngenerated_answer = rag_pipeline(amnesty_qa[0][\"question\"])\n</pre> question, contexts, answer = row[\"question\"], row[\"contexts\"], row[\"answer\"] generated_answer = rag_pipeline(amnesty_qa[0][\"question\"]) <pre>calculating faithfulness\ncalculating answer_relevancy\n</pre> <pre>Using 'context_precision' without ground truth will be soon depreciated. Use 'context_utilization' instead\n</pre> <pre>calculating context_precision\ncalculating harmfulness\n</pre> <p></p> <p>Note that the scoring is blocking so make sure that you sent the generated answer before waiting for the scores to get computed. Alternatively you can run <code>score_with_ragas()</code> in a separate thread and pass in the <code>trace_id</code> to log the scores.</p>"},{"location":"howtos/integrations/langfuse/#langfuse","title":"Langfuse\u00b6","text":"<p>Ragas and Langfuse is a powerful combination that can help you evaluate and monitor your Retrieval-Augmented Generation (RAG) pipelines.</p>"},{"location":"howtos/integrations/langfuse/#what-is-langfuse","title":"What is Langfuse?\u00b6","text":"<p>Langfuse (GitHub) is an open-source platform for LLM tracing, prompt management, and evaluation. It allows you to score your traces and spans, providing insights into the performance of your RAG pipelines. Langfuse supports various integrations, including OpenAI, Langchain, and more.</p>"},{"location":"howtos/integrations/langfuse/#key-benefits-of-using-langfuse-with-ragas","title":"Key Benefits of using Langfuse with Ragas\u00b6","text":"<ul> <li>Score Traces: Score your traces and spans, providing insights into the performance of your RAG pipelines.</li> <li>Detailed Analytics: Segment and analyze traces to identify low-quality scores and improve your system's performance.</li> <li>Score Reporting: Drill down into detailed reports for specific use cases and user segments.</li> </ul> <p>Ragas (GitHub) is an open-source tool that can help you run Model-Based Evaluation on your traces/spans, especially for RAG pipelines. Ragas can perform reference-free evaluations of various aspects of your RAG pipeline. Because it is reference-free you don't need ground-truths when running the evaluations and can run it on production traces that you've collected with Langfuse.</p>"},{"location":"howtos/integrations/langfuse/#getting-started","title":"Getting Started\u00b6","text":"<p>This guide will walk you through and end-to-end example of RAG evaluations with Ragas and Langfuse.</p>"},{"location":"howtos/integrations/langfuse/#the-environment","title":"The Environment\u00b6","text":"<p>Sign up for Langfuse to get your API keys.</p>"},{"location":"howtos/integrations/langfuse/#the-data","title":"The Data\u00b6","text":"<p>For this example, we are going to use a dataset that has already been prepared by querying a RAG system and gathering its outputs. See below for instruction on how to fetch your production data from Langfuse.</p> <p>The dataset contains the following columns:</p> <ul> <li><code>question</code>: list[str] - These are the questions your RAG pipeline will be evaluated on.</li> <li><code>answer</code>: list[str] - The answer generated from the RAG pipeline and given to the user.</li> <li><code>contexts</code>: list[list[str]] - The contexts which were passed into the LLM to answer the question.</li> <li><code>ground_truth</code>: list[list[str]] - The ground truth answer to the questions. However, this can be ignored for online evaluations since we will not have access to ground-truth data in our case.</li> </ul>"},{"location":"howtos/integrations/langfuse/#the-metrics","title":"The Metrics\u00b6","text":"<p>In this example, we will use the following metrics from the Ragas library:</p> <ul> <li><code>faithfulness</code>: This measures the factual consistency of the generated answer against the given context.</li> <li><code>answer_relevancy</code>: Answer Relevancy, focuses on assessing how to-the-point and relevant the generated answer is to the given prompt.</li> <li><code>context precision</code>: Context Precision is a metric that evaluates whether all of the ground-truth relevant items present in the contexts are ranked higher or not. Ideally, all the relevant chunks must appear at the top ranks. This metric is computed using the question and the contexts, with values ranging between 0 and 1, where higher scores indicate better precision.</li> <li><code>aspect_critique</code>: This is designed to assess submissions based on predefined aspects such as harmlessness and correctness. Additionally, users have the flexibility to define their own aspects for evaluating submissions according to their specific criteria.</li> </ul> <p>Have a look at the documentation to learn more about these metrics and how they work.</p>"},{"location":"howtos/integrations/langfuse/#the-setup","title":"The Setup\u00b6","text":"<p>You can use model-based evaluation with Ragas in 2 ways:</p> <ol> <li>Score each Trace: This means you will run the evaluations for each trace item. This gives you much better idea since of how each call to your RAG pipelines is performing but can be expensive</li> <li>Score as Batch: In this method we will take a random sample of traces on a periodic basis and score them. This brings down cost and gives you a rough estimate the performance of your app but can miss out on important samples.</li> </ol> <p>In this cookbook, we'll show you how to setup both.</p>"},{"location":"howtos/integrations/langfuse/#score-the-trace","title":"Score the Trace\u00b6","text":"<p>Lets take a small example of a single trace and see how you can score that with Ragas. First lets load the data.</p>"},{"location":"howtos/integrations/langfuse/#analyze-the-scores-in-langfuse","title":"Analyze the Scores in Langfuse\u00b6","text":"<p>You can analyze the scores in the Langfuse UI and drill down into the scores for each question or user.</p> <p>\u2192 Not using Langfuse yet? Explore the dashboard in our interactive demo.</p>"},{"location":"howtos/integrations/langfuse/#resources","title":"Resources\u00b6","text":"<ul> <li>Have a look at our guide on Model-Based Evaluation to learn more about how to run model-based evaluations with Ragas.</li> <li>Learn more about analyzing and improving your LLM application here.</li> </ul>"},{"location":"howtos/integrations/langfuse/#feedback","title":"Feedback\u00b6","text":"<p>If you have any feedback or requests, please create a GitHub Issue or share your work with the community on Discord.</p>"},{"location":"howtos/integrations/langsmith/","title":"Langsmith","text":"In\u00a0[1]: Copied! <pre>from datasets import load_dataset\nfrom ragas.metrics import context_precision, answer_relevancy, faithfulness\nfrom ragas import evaluate\n\n\nfiqa_eval = load_dataset(\"explodinggradients/fiqa\", \"ragas_eval\")\n\nresult = evaluate(\n    fiqa_eval[\"baseline\"].select(range(3)),\n    metrics=[context_precision, faithfulness, answer_relevancy],\n)\n\nresult\n</pre> from datasets import load_dataset from ragas.metrics import context_precision, answer_relevancy, faithfulness from ragas import evaluate   fiqa_eval = load_dataset(\"explodinggradients/fiqa\", \"ragas_eval\")  result = evaluate(     fiqa_eval[\"baseline\"].select(range(3)),     metrics=[context_precision, faithfulness, answer_relevancy], )  result <pre>Found cached dataset fiqa (/home/jjmachan/.cache/huggingface/datasets/explodinggradients___fiqa/ragas_eval/1.0.0/3dc7b639f5b4b16509a3299a2ceb78bf5fe98ee6b5fee25e7d5e4d290c88efb8)\n</pre> <pre>  0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>evaluating with [context_precision]\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:23&lt;00:00, 23.21s/it]\n</pre> <pre>evaluating with [faithfulness]\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:36&lt;00:00, 36.94s/it]\n</pre> <pre>evaluating with [answer_relevancy]\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:10&lt;00:00, 10.58s/it]\n</pre> Out[1]: <pre>{'context_precision': 0.5976, 'faithfulness': 0.8889, 'answer_relevancy': 0.9300}</pre> <p>Voila! Now you can head over to your project and see the traces</p>"},{"location":"howtos/integrations/langsmith/#langsmith","title":"Langsmith\u00b6","text":""},{"location":"howtos/integrations/langsmith/#dataset-and-tracing-visualisation","title":"Dataset and Tracing Visualisation\u00b6","text":"<p>Langsmith in a platform for building production-grade LLM applications from the langchain team. It helps you with tracing, debugging and evaluting LLM applications.</p> <p>The langsmith + ragas integrations offer 2 features</p> <ol> <li>View the traces of ragas <code>evaluator</code></li> <li>Use ragas metrics in langchain evaluation - (soon)</li> </ol>"},{"location":"howtos/integrations/langsmith/#tracing-ragas-metrics","title":"Tracing ragas metrics\u00b6","text":"<p>since ragas uses langchain under the hood all you have to do is setup langsmith and your traces will be logged.</p> <p>to setup langsmith make sure the following env-vars are set (you can read more in the langsmith docs</p> <pre>export LANGCHAIN_TRACING_V2=true\nexport LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\nexport LANGCHAIN_API_KEY=&lt;your-api-key&gt;\nexport LANGCHAIN_PROJECT=&lt;your-project&gt;  # if not specified, defaults to \"default\"\n</pre> <p>Once langsmith is setup, just run the evaluations as your normally would</p>"},{"location":"howtos/integrations/llamaindex/","title":"LlamaIndex","text":"In\u00a0[3]: Copied! <pre># load the documents\nfrom llama_index.core import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"./nyc_wikipedia\").load_data()\n</pre> # load the documents from llama_index.core import SimpleDirectoryReader  documents = SimpleDirectoryReader(\"./nyc_wikipedia\").load_data() <p>Now  lets init the <code>TestsetGenerator</code> object with the corresponding generator and critic llms</p> In\u00a0[4]: Copied! <pre>from ragas.testset.generator import TestsetGenerator\nfrom ragas.testset.evolutions import simple, reasoning, multi_context\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\n# generator with openai models\ngenerator_llm = OpenAI(model=\"gpt-3.5-turbo-16k\")\ncritic_llm = OpenAI(model=\"gpt-4\")\nembeddings = OpenAIEmbedding()\n\ngenerator = TestsetGenerator.from_llama_index(\n    generator_llm=generator_llm,\n    critic_llm=critic_llm,\n    embeddings=embeddings,\n)\n</pre> from ragas.testset.generator import TestsetGenerator from ragas.testset.evolutions import simple, reasoning, multi_context from llama_index.llms.openai import OpenAI from llama_index.embeddings.openai import OpenAIEmbedding  # generator with openai models generator_llm = OpenAI(model=\"gpt-3.5-turbo-16k\") critic_llm = OpenAI(model=\"gpt-4\") embeddings = OpenAIEmbedding()  generator = TestsetGenerator.from_llama_index(     generator_llm=generator_llm,     critic_llm=critic_llm,     embeddings=embeddings, ) <p>Now you are all set to generate the dataset</p> In\u00a0[5]: Copied! <pre># generate testset\ntestset = generator.generate_with_llamaindex_docs(\n    documents,\n    test_size=5,\n    distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25},\n)\n</pre> # generate testset testset = generator.generate_with_llamaindex_docs(     documents,     test_size=5,     distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25}, ) <pre>embedding nodes:   0%|          | 0/54 [00:00&lt;?, ?it/s]</pre> <pre>Filename and doc_id are the same for all nodes.\n</pre> <pre>Generating:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> In\u00a0[6]: Copied! <pre>df = testset.to_pandas()\ndf.head()\n</pre> df = testset.to_pandas() df.head() Out[6]: question contexts ground_truth evolution_type metadata episode_done 0 What cultural movement began in New York City ... [ Others cite the end of the crack epidemic an... The Harlem Renaissance simple [{'file_path': '/home/jjmachan/jjmachan/explod... True 1 What is the significance of New York City's tr... [ consisting of 51 council members whose distr... New York City's transportation system is both ... simple [{'file_path': '/home/jjmachan/jjmachan/explod... True 2 What factors led to the creation of Central Pa... [ next ten years with British troops stationed... Public-minded members of the contemporaneous b... reasoning [{'file_path': '/home/jjmachan/jjmachan/explod... True 3 What was the impact of the Treaty of Breda on ... [ British raids. In 1626, the Dutch colonial D... The Treaty of Breda confirmed the transfer of ... multi_context [{'file_path': '/home/jjmachan/jjmachan/explod... True 4 What role did New York play in the American Re... [ British raids. In 1626, the Dutch colonial D... New York played a significant role in the Amer... simple [{'file_path': '/home/jjmachan/jjmachan/explod... True <p>with a test dataset to test our <code>QueryEngine</code> lets now build one and evaluate it.</p> In\u00a0[7]: Copied! <pre># build query engine\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.core.settings import Settings\n\nvector_index = VectorStoreIndex.from_documents(documents)\n\nquery_engine = vector_index.as_query_engine()\n</pre> # build query engine from llama_index.core import VectorStoreIndex, SimpleDirectoryReader from llama_index.core.settings import Settings  vector_index = VectorStoreIndex.from_documents(documents)  query_engine = vector_index.as_query_engine() <p>Lets try an sample question from the generated testset to see if it is working</p> In\u00a0[9]: Copied! <pre># convert it to pandas dataset\ndf = testset.to_pandas()\ndf[\"question\"][0]\n</pre> # convert it to pandas dataset df = testset.to_pandas() df[\"question\"][0] Out[9]: <pre>'What cultural movement began in New York City and established the African-American literary canon in the United States?'</pre> In\u00a0[10]: Copied! <pre>response_vector = query_engine.query(df[\"question\"][0])\n\nprint(response_vector)\n</pre> response_vector = query_engine.query(df[\"question\"][0])  print(response_vector) <pre>The Harlem Renaissance was the cultural movement that began in New York City and established the African-American literary canon in the United States.\n</pre> <p>first lets generate the questions. Ideally you should use that you see in production so that the distribution of question with which we evaluate matches the distribution of questions seen in production. This ensures that the scores reflect the performance seen in production but to start off we'll be using a few example question.</p> <p>Now lets import the metrics we will be using to evaluate</p> In\u00a0[11]: Copied! <pre>from ragas.metrics import (\n    faithfulness,\n    answer_relevancy,\n    context_precision,\n    context_recall,\n)\nfrom ragas.metrics.critique import harmfulness\n\nmetrics = [\n    faithfulness,\n    answer_relevancy,\n    context_precision,\n    context_recall,\n    harmfulness,\n]\n</pre> from ragas.metrics import (     faithfulness,     answer_relevancy,     context_precision,     context_recall, ) from ragas.metrics.critique import harmfulness  metrics = [     faithfulness,     answer_relevancy,     context_precision,     context_recall,     harmfulness, ] <p>now lets init the evaluator model</p> In\u00a0[15]: Copied! <pre>from llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\n# using GPT 3.5, use GPT 4 / 4-turbo for better accuracy\nevaluator_llm = OpenAI(model=\"gpt-3.5-turbo\")\n</pre> from llama_index.llms.openai import OpenAI from llama_index.embeddings.openai import OpenAIEmbedding  # using GPT 3.5, use GPT 4 / 4-turbo for better accuracy evaluator_llm = OpenAI(model=\"gpt-3.5-turbo\") <p>the <code>evaluate()</code> function expects a dict of \"question\" and \"ground_truth\" for metrics. You can easily convert the <code>testset</code> to that format</p> In\u00a0[23]: Copied! <pre># convert to HF dataset\nds = testset.to_dataset()\n\nds_dict = ds.to_dict()\nds_dict[\"question\"]\nds_dict[\"ground_truth\"]\n</pre> # convert to HF dataset ds = testset.to_dataset()  ds_dict = ds.to_dict() ds_dict[\"question\"] ds_dict[\"ground_truth\"] Out[23]: <pre>['The Harlem Renaissance',\n \"New York City's transportation system is both complex and extensive, with a comprehensive mass transit system that accounts for one in every three users of mass transit in the United States. The New York City Subway system is the largest rapid transit system in the world, and the city has a high usage of public transport, with a majority of households not owning a car. Due to their reliance on mass transit, New Yorkers spend less of their household income on transportation compared to the national average.\",\n 'Public-minded members of the contemporaneous business elite lobbied for the establishment of Central Park',\n 'The Treaty of Breda confirmed the transfer of New Amsterdam to English control and the renaming of the settlement as New York. The Duke of York, who would later become King James II and VII, played a significant role in the naming of New York City.',\n 'New York played a significant role in the American Revolution. The Stamp Act Congress met in New York in October 1765, and the city became a center for the Sons of Liberty organization. Skirmishes and battles took place in and around New York, including the Battle of Long Island and the Battle of Saratoga. The city was occupied by British forces for much of the war, but it was eventually liberated by American troops in 1783.']</pre> <p>Finally lets run the evaluation</p> In\u00a0[24]: Copied! <pre>from ragas.integrations.llama_index import evaluate\n\nresult = evaluate(\n    query_engine=query_engine,\n    metrics=metrics,\n    dataset=ds_dict,\n    llm=evaluator_llm,\n    embeddings=OpenAIEmbedding(),\n)\n</pre> from ragas.integrations.llama_index import evaluate  result = evaluate(     query_engine=query_engine,     metrics=metrics,     dataset=ds_dict,     llm=evaluator_llm,     embeddings=OpenAIEmbedding(), ) <pre>Running Query Engine:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Evaluating:   0%|          | 0/25 [00:00&lt;?, ?it/s]</pre> <pre>n values greater than 1 not support for LlamaIndex LLMs\nn values greater than 1 not support for LlamaIndex LLMs\nn values greater than 1 not support for LlamaIndex LLMs\nn values greater than 1 not support for LlamaIndex LLMs\nn values greater than 1 not support for LlamaIndex LLMs\n</pre> In\u00a0[25]: Copied! <pre># final scores\nprint(result)\n</pre> # final scores print(result) <pre>{'faithfulness': 0.9000, 'answer_relevancy': 0.8993, 'context_precision': 0.9000, 'context_recall': 1.0000, 'harmfulness': 0.0000}\n</pre> <p>You can convert into a pandas dataframe to run more analysis on it.</p> In\u00a0[26]: Copied! <pre>result.to_pandas()\n</pre> result.to_pandas() Out[26]: question contexts answer ground_truth faithfulness answer_relevancy context_precision context_recall harmfulness 0 What cultural movement began in New York City ... [=== 19th century ===\\n\\nOver the course of th... The Harlem Renaissance of literary and cultura... The Harlem Renaissance 0.5 0.907646 0.5 1.0 0 1 What is the significance of New York City's tr... [== Transportation ==\\n\\nNew York City's compr... New York City's transportation system is signi... New York City's transportation system is both ... 1.0 0.986921 1.0 1.0 0 2 What factors led to the creation of Central Pa... [=== 19th century ===\\n\\nOver the course of th... Prominent American literary figures lived in N... Public-minded members of the contemporaneous b... 1.0 0.805014 1.0 1.0 0 3 What was the impact of the Treaty of Breda on ... [=== Dutch rule ===\\n\\nA permanent European pr... The Treaty of Breda resulted in the transfer o... The Treaty of Breda confirmed the transfer of ... 1.0 0.860931 1.0 1.0 0 4 What role did New York play in the American Re... [=== Province of New York and slavery ===\\n\\nI... New York served as a significant location duri... New York played a significant role in the Amer... 1.0 0.935846 1.0 1.0 0"},{"location":"howtos/integrations/llamaindex/#llamaindex","title":"LlamaIndex\u00b6","text":"<p>LlamaIndex is a data framework for LLM applications to ingest, structure, and access private or domain-specific data. Makes it super easy to connect LLMs with your own data. But in order to figure out the best configuration for llamaIndex and your data you need a object measure of the performance. This is where ragas comes in. Ragas will help you evaluate your <code>QueryEngine</code> and gives you the confidence to tweak the configuration to get hightest score.</p> <p>This guide assumes you have familarity with the LlamaIndex framework.</p>"},{"location":"howtos/integrations/llamaindex/#building-the-testset","title":"Building the Testset\u00b6","text":"<p>You will need an testset to evaluate your <code>QueryEngine</code> against. You can either build one yourself or use the Testset Generator Module in Ragas to get started with a small synthetic one.</p> <p>Let's see how that works with Llamaindex</p>"},{"location":"howtos/integrations/llamaindex/#building-the-queryengine","title":"Building the <code>QueryEngine</code>\u00b6","text":"<p>To start lets build an <code>VectorStoreIndex</code> over the New York Citie's wikipedia page as an example and use ragas to evaluate it.</p> <p>Since we already loaded the dataset into <code>documents</code> lets use that.</p>"},{"location":"howtos/integrations/llamaindex/#evaluating-the-queryengine","title":"Evaluating the <code>QueryEngine</code>\u00b6","text":"<p>Now that we have a <code>QueryEngine</code> for the <code>VectorStoreIndex</code> we can use the llama_index integration Ragas has to evaluate it.</p> <p>In order to run an evaluation with Ragas and LlamaIndex you need 3 things</p> <ol> <li>LlamaIndex <code>QueryEngine</code>: what we will be evaluating</li> <li>Metrics: Ragas defines a set of metrics that can measure different aspects of the <code>QueryEngine</code>. The available metrics and their meaning can be found here</li> <li>Questions: A list of questions that ragas will test the <code>QueryEngine</code> against.</li> </ol>"},{"location":"howtos/integrations/openlayer/","title":"OpenLayer","text":"In\u00a0[\u00a0]: Copied! <pre>%%bash\ngit clone https://huggingface.co/datasets/explodinggradients/prompt-engineering-papers\n</pre> %%bash git clone https://huggingface.co/datasets/explodinggradients/prompt-engineering-papers In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY_HERE\"\n</pre> import os  os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY_HERE\" In\u00a0[\u00a0]: Copied! <pre>from llama_index import SimpleDirectoryReader\nfrom ragas.testset.generator import TestsetGenerator\nfrom ragas.testset.evolutions import simple, reasoning, multi_context\n\n# load documents\ndir_path = \"./prompt-engineering-papers\"\nreader = SimpleDirectoryReader(dir_path, num_files_limit=2)\ndocuments = reader.load_data()\n\n# generator with openai models\ngenerator = TestsetGenerator.with_openai()\n\n# set question type distribution\ndistribution = {simple: 0.5, reasoning: 0.25, multi_context: 0.25}\n\n# generate testset\ntestset = generator.generate_with_llamaindex_docs(\n    documents, test_size=10, distributions=distribution\n)\ntest_df = testset.to_pandas()\ntest_df.head()\n</pre> from llama_index import SimpleDirectoryReader from ragas.testset.generator import TestsetGenerator from ragas.testset.evolutions import simple, reasoning, multi_context  # load documents dir_path = \"./prompt-engineering-papers\" reader = SimpleDirectoryReader(dir_path, num_files_limit=2) documents = reader.load_data()  # generator with openai models generator = TestsetGenerator.with_openai()  # set question type distribution distribution = {simple: 0.5, reasoning: 0.25, multi_context: 0.25}  # generate testset testset = generator.generate_with_llamaindex_docs(     documents, test_size=10, distributions=distribution ) test_df = testset.to_pandas() test_df.head() In\u00a0[\u00a0]: Copied! <pre>import nest_asyncio\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\nfrom llama_index.embeddings import OpenAIEmbedding\n\n\nnest_asyncio.apply()\n\n\ndef build_query_engine(documents):\n    vector_index = VectorStoreIndex.from_documents(\n        documents,\n        service_context=ServiceContext.from_defaults(chunk_size=512),\n        embed_model=OpenAIEmbedding(),\n    )\n\n    query_engine = vector_index.as_query_engine(similarity_top_k=2)\n    return query_engine\n</pre> import nest_asyncio from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext from llama_index.embeddings import OpenAIEmbedding   nest_asyncio.apply()   def build_query_engine(documents):     vector_index = VectorStoreIndex.from_documents(         documents,         service_context=ServiceContext.from_defaults(chunk_size=512),         embed_model=OpenAIEmbedding(),     )      query_engine = vector_index.as_query_engine(similarity_top_k=2)     return query_engine In\u00a0[\u00a0]: Copied! <pre>query_engine = build_query_engine(documents)\n</pre> query_engine = build_query_engine(documents) In\u00a0[\u00a0]: Copied! <pre>def generate_single_response(query_engine, question):\n    response = query_engine.query(question)\n    return {\n        \"answer\": response.response,\n        \"contexts\": [c.node.get_content() for c in response.source_nodes],\n    }\n</pre> def generate_single_response(query_engine, question):     response = query_engine.query(question)     return {         \"answer\": response.response,         \"contexts\": [c.node.get_content() for c in response.source_nodes],     } In\u00a0[\u00a0]: Copied! <pre>question = \"What are some strategies proposed to enhance the in-context learning capability of language models?\"\ngenerate_single_response(query_engine, question)\n</pre> question = \"What are some strategies proposed to enhance the in-context learning capability of language models?\" generate_single_response(query_engine, question) In\u00a0[\u00a0]: Copied! <pre>from datasets import Dataset\n\n\ndef generate_ragas_dataset(query_engine, test_df):\n    test_questions = test_df[\"question\"].values\n    responses = [generate_single_response(query_engine, q) for q in test_questions]\n\n    dataset_dict = {\n        \"question\": test_questions,\n        \"answer\": [response[\"answer\"] for response in responses],\n        \"contexts\": [response[\"contexts\"] for response in responses],\n        \"ground_truth\": test_df[\"ground_truth\"].values.tolist(),\n    }\n    ds = Dataset.from_dict(dataset_dict)\n    return ds\n</pre> from datasets import Dataset   def generate_ragas_dataset(query_engine, test_df):     test_questions = test_df[\"question\"].values     responses = [generate_single_response(query_engine, q) for q in test_questions]      dataset_dict = {         \"question\": test_questions,         \"answer\": [response[\"answer\"] for response in responses],         \"contexts\": [response[\"contexts\"] for response in responses],         \"ground_truth\": test_df[\"ground_truth\"].values.tolist(),     }     ds = Dataset.from_dict(dataset_dict)     return ds In\u00a0[\u00a0]: Copied! <pre>ragas_dataset = generate_ragas_dataset(query_engine, test_df)\nragas_df = ragas_dataset.to_pandas()\n</pre> ragas_dataset = generate_ragas_dataset(query_engine, test_df) ragas_df = ragas_dataset.to_pandas() In\u00a0[\u00a0]: Copied! <pre>from openlayer.tasks import TaskType\n\nclient = openlayer.OpenlayerClient(\"YOUR_OPENLAYER_API_KEY_HERE\")\n</pre> from openlayer.tasks import TaskType  client = openlayer.OpenlayerClient(\"YOUR_OPENLAYER_API_KEY_HERE\") In\u00a0[\u00a0]: Copied! <pre>project = client.create_project(\n    name=\"My-Rag-Project\",\n    task_type=TaskType.LLM,\n    description=\"Evaluating an LLM used for product development.\",\n)\n</pre> project = client.create_project(     name=\"My-Rag-Project\",     task_type=TaskType.LLM,     description=\"Evaluating an LLM used for product development.\", ) In\u00a0[\u00a0]: Copied! <pre>validation_dataset_config = {\n    \"contextColumnName\": \"contexts\",\n    \"questionColumnName\": \"question\",\n    \"inputVariableNames\": [\"question\"],\n    \"label\": \"validation\",\n    \"outputColumnName\": \"answer\",\n    \"groundTruthColumnName\": \"ground_truth\",\n}\nproject.add_dataframe(\n    dataset_df=ragas_df,\n    dataset_config=validation_dataset_config,\n)\n</pre> validation_dataset_config = {     \"contextColumnName\": \"contexts\",     \"questionColumnName\": \"question\",     \"inputVariableNames\": [\"question\"],     \"label\": \"validation\",     \"outputColumnName\": \"answer\",     \"groundTruthColumnName\": \"ground_truth\", } project.add_dataframe(     dataset_df=ragas_df,     dataset_config=validation_dataset_config, ) In\u00a0[\u00a0]: Copied! <pre>model_config = {\n    \"inputVariableNames\": [\"question\"],\n    \"modelType\": \"shell\",\n    \"metadata\": {\"top_k\": 2, \"chunk_size\": 512, \"embeddings\": \"OpenAI\"},\n}\nproject.add_model(model_config=model_config)\n</pre> model_config = {     \"inputVariableNames\": [\"question\"],     \"modelType\": \"shell\",     \"metadata\": {\"top_k\": 2, \"chunk_size\": 512, \"embeddings\": \"OpenAI\"}, } project.add_model(model_config=model_config) In\u00a0[\u00a0]: Copied! <pre>project.commit(\"Initial commit!\")\nproject.push()\n</pre> project.commit(\"Initial commit!\") project.push() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"howtos/integrations/openlayer/#openlayer","title":"OpenLayer\u00b6","text":""},{"location":"howtos/integrations/openlayer/#evaluating-rag-pipelines-with-openlayer-and-ragas","title":"Evaluating RAG pipelines with Openlayer and Ragas\u00b6","text":"<p>Openlayer is an evaluation tool that fits into your development and production pipelines to help you ship high-quality models with confidence.</p> <p>This notebook should be used together with this blog post.</p>"},{"location":"howtos/integrations/openlayer/#pre-requisites","title":"Pre-requisites\u00b6","text":""},{"location":"howtos/integrations/openlayer/#synthetic-test-data-generation","title":"Synthetic test data generation\u00b6","text":""},{"location":"howtos/integrations/openlayer/#building-rag","title":"Building RAG\u00b6","text":""},{"location":"howtos/integrations/openlayer/#commit-to-openlayer","title":"Commit to Openlayer\u00b6","text":""},{"location":"howtos/integrations/opik/","title":"Comet Opik","text":"In\u00a0[1]: Copied! <pre>import os\nimport getpass\n\nos.environ[\"OPIK_API_KEY\"] = getpass.getpass(\"Opik API Key: \")\nos.environ[\"OPIK_WORKSPACE\"] = input(\n    \"Comet workspace (often the same as your username): \"\n)\n</pre> import os import getpass  os.environ[\"OPIK_API_KEY\"] = getpass.getpass(\"Opik API Key: \") os.environ[\"OPIK_WORKSPACE\"] = input(     \"Comet workspace (often the same as your username): \" ) <p>If you are running the Opik platform locally, simply set:</p> In\u00a0[2]: Copied! <pre># import os\n# os.environ[\"OPIK_URL_OVERRIDE\"] = \"http://localhost:5173/api\"\n</pre> # import os # os.environ[\"OPIK_URL_OVERRIDE\"] = \"http://localhost:5173/api\" In\u00a0[3]: Copied! <pre>%pip install opik --quiet\n\nimport os\nimport getpass\n\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n</pre> %pip install opik --quiet  import os import getpass  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \") In\u00a0[4]: Copied! <pre># Import the metric\nfrom ragas.metrics import AnswerRelevancy\n\n# Import some additional dependencies\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langchain_openai.embeddings import OpenAIEmbeddings\nfrom ragas.llms import LangchainLLMWrapper\nfrom ragas.embeddings import LangchainEmbeddingsWrapper\n\n# Initialize the Ragas metric\nllm = LangchainLLMWrapper(ChatOpenAI())\nemb = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n\nanswer_relevancy_metric = AnswerRelevancy(llm=llm, embeddings=emb)\n</pre> # Import the metric from ragas.metrics import AnswerRelevancy  # Import some additional dependencies from langchain_openai.chat_models import ChatOpenAI from langchain_openai.embeddings import OpenAIEmbeddings from ragas.llms import LangchainLLMWrapper from ragas.embeddings import LangchainEmbeddingsWrapper  # Initialize the Ragas metric llm = LangchainLLMWrapper(ChatOpenAI()) emb = LangchainEmbeddingsWrapper(OpenAIEmbeddings())  answer_relevancy_metric = AnswerRelevancy(llm=llm, embeddings=emb) <p>Once the metric is initialized, you can use it to score a sample question. Given that the metric scoring is done asynchronously, you need to use the <code>asyncio</code> library to run the scoring function.</p> In\u00a0[5]: Copied! <pre># Run this cell first if you are running this in a Jupyter notebook\nimport nest_asyncio\n\nnest_asyncio.apply()\n</pre> # Run this cell first if you are running this in a Jupyter notebook import nest_asyncio  nest_asyncio.apply() In\u00a0[6]: Copied! <pre>import asyncio\nfrom ragas.integrations.opik import OpikTracer\nfrom ragas.dataset_schema import SingleTurnSample\n\n\n# Define the scoring function\ndef compute_metric(metric, row):\n    row = SingleTurnSample(**row)\n\n    opik_tracer = OpikTracer()\n\n    async def get_score(opik_tracer, metric, row):\n        score = await metric.single_turn_ascore(row, callbacks=[OpikTracer()])\n        return score\n\n    # Run the async function using the current event loop\n    loop = asyncio.get_event_loop()\n\n    result = loop.run_until_complete(get_score(opik_tracer, metric, row))\n    return result\n\n\n# Score a simple example\nrow = {\n    \"user_input\": \"What is the capital of France?\",\n    \"response\": \"Paris\",\n    \"retrieved_contexts\": [\"Paris is the capital of France.\", \"Paris is in France.\"],\n}\n\nscore = compute_metric(answer_relevancy_metric, row)\nprint(\"Answer Relevancy score:\", score)\n</pre> import asyncio from ragas.integrations.opik import OpikTracer from ragas.dataset_schema import SingleTurnSample   # Define the scoring function def compute_metric(metric, row):     row = SingleTurnSample(**row)      opik_tracer = OpikTracer()      async def get_score(opik_tracer, metric, row):         score = await metric.single_turn_ascore(row, callbacks=[OpikTracer()])         return score      # Run the async function using the current event loop     loop = asyncio.get_event_loop()      result = loop.run_until_complete(get_score(opik_tracer, metric, row))     return result   # Score a simple example row = {     \"user_input\": \"What is the capital of France?\",     \"response\": \"Paris\",     \"retrieved_contexts\": [\"Paris is the capital of France.\", \"Paris is in France.\"], }  score = compute_metric(answer_relevancy_metric, row) print(\"Answer Relevancy score:\", score) <pre>Answer Relevancy score: 1.0\n</pre> <p>If you now navigate to Opik, you will be able to see that a new trace has been created in the <code>Default Project</code> project.</p> In\u00a0[7]: Copied! <pre>from opik import track\nfrom opik.opik_context import update_current_trace\n\n\n@track\ndef retrieve_contexts(question):\n    # Define the retrieval function, in this case we will hard code the contexts\n    return [\"Paris is the capital of France.\", \"Paris is in France.\"]\n\n\n@track\ndef answer_question(question, contexts):\n    # Define the answer function, in this case we will hard code the answer\n    return \"Paris\"\n\n\n@track(name=\"Compute Ragas metric score\", capture_input=False)\ndef compute_rag_score(answer_relevancy_metric, question, answer, contexts):\n    # Define the score function\n    row = {\"user_input\": question, \"response\": answer, \"retrieved_contexts\": contexts}\n    score = compute_metric(answer_relevancy_metric, row)\n    return score\n\n\n@track\ndef rag_pipeline(question):\n    # Define the pipeline\n    contexts = retrieve_contexts(question)\n    answer = answer_question(question, contexts)\n\n    score = compute_rag_score(answer_relevancy_metric, question, answer, contexts)\n    update_current_trace(\n        feedback_scores=[{\"name\": \"answer_relevancy\", \"value\": round(score, 4)}]\n    )\n\n    return answer\n\n\nrag_pipeline(\"What is the capital of France?\")\n</pre> from opik import track from opik.opik_context import update_current_trace   @track def retrieve_contexts(question):     # Define the retrieval function, in this case we will hard code the contexts     return [\"Paris is the capital of France.\", \"Paris is in France.\"]   @track def answer_question(question, contexts):     # Define the answer function, in this case we will hard code the answer     return \"Paris\"   @track(name=\"Compute Ragas metric score\", capture_input=False) def compute_rag_score(answer_relevancy_metric, question, answer, contexts):     # Define the score function     row = {\"user_input\": question, \"response\": answer, \"retrieved_contexts\": contexts}     score = compute_metric(answer_relevancy_metric, row)     return score   @track def rag_pipeline(question):     # Define the pipeline     contexts = retrieve_contexts(question)     answer = answer_question(question, contexts)      score = compute_rag_score(answer_relevancy_metric, question, answer, contexts)     update_current_trace(         feedback_scores=[{\"name\": \"answer_relevancy\", \"value\": round(score, 4)}]     )      return answer   rag_pipeline(\"What is the capital of France?\") Out[7]: <pre>'Paris'</pre> In\u00a0[8]: Copied! <pre>from datasets import load_dataset\nfrom ragas.metrics import context_precision, answer_relevancy, faithfulness\nfrom ragas import evaluate\nfrom ragas.integrations.opik import OpikTracer\n\nfiqa_eval = load_dataset(\"explodinggradients/fiqa\", \"ragas_eval\")\n\n# Reformat the dataset to match the schema expected by the Ragas evaluate function\ndataset = fiqa_eval[\"baseline\"].select(range(3))\n\ndataset = dataset.map(\n    lambda x: {\n        \"user_input\": x[\"question\"],\n        \"reference\": x[\"ground_truths\"][0],\n        \"retrieved_contexts\": x[\"contexts\"],\n    }\n)\n\nopik_tracer_eval = OpikTracer(tags=[\"ragas_eval\"], metadata={\"evaluation_run\": True})\n\nresult = evaluate(\n    dataset,\n    metrics=[context_precision, faithfulness, answer_relevancy],\n    callbacks=[opik_tracer_eval],\n)\n\nprint(result)\n</pre> from datasets import load_dataset from ragas.metrics import context_precision, answer_relevancy, faithfulness from ragas import evaluate from ragas.integrations.opik import OpikTracer  fiqa_eval = load_dataset(\"explodinggradients/fiqa\", \"ragas_eval\")  # Reformat the dataset to match the schema expected by the Ragas evaluate function dataset = fiqa_eval[\"baseline\"].select(range(3))  dataset = dataset.map(     lambda x: {         \"user_input\": x[\"question\"],         \"reference\": x[\"ground_truths\"][0],         \"retrieved_contexts\": x[\"contexts\"],     } )  opik_tracer_eval = OpikTracer(tags=[\"ragas_eval\"], metadata={\"evaluation_run\": True})  result = evaluate(     dataset,     metrics=[context_precision, faithfulness, answer_relevancy],     callbacks=[opik_tracer_eval], )  print(result) <pre>Evaluating:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>{'context_precision': 1.0000, 'faithfulness': 0.7375, 'answer_relevancy': 0.9889}\n</pre>"},{"location":"howtos/integrations/opik/#comet-opik","title":"Comet Opik\u00b6","text":"<p>In this notebook, we will showcase how to use Opik with Ragas for monitoring and evaluation of RAG (Retrieval-Augmented Generation) pipelines.</p> <p>There are two main ways to use Opik with Ragas:</p> <ol> <li>Using Ragas metrics to score traces</li> <li>Using the Ragas <code>evaluate</code> function to score a dataset</li> </ol>"},{"location":"howtos/integrations/opik/#setup","title":"Setup\u00b6","text":"<p>Comet provides a hosted version of the Opik platform, simply create an account and grab you API Key.</p> <p>You can also run the Opik platform locally, see the installation guide for more information.</p>"},{"location":"howtos/integrations/opik/#preparing-our-environment","title":"Preparing our environment\u00b6","text":"<p>First, we will install the necessary libraries, configure the OpenAI API key and create a new Opik dataset.</p>"},{"location":"howtos/integrations/opik/#integrating-opik-with-ragas","title":"Integrating Opik with Ragas\u00b6","text":""},{"location":"howtos/integrations/opik/#using-ragas-metrics-to-score-traces","title":"Using Ragas metrics to score traces\u00b6","text":"<p>Ragas provides a set of metrics that can be used to evaluate the quality of a RAG pipeline, including but not limited to: <code>answer_relevancy</code>, <code>answer_similarity</code>, <code>answer_correctness</code>, <code>context_precision</code>, <code>context_recall</code>, <code>context_entity_recall</code>, <code>summarization_score</code>. You can find a full list of metrics in the Ragas documentation.</p> <p>These metrics can be computed on the fly and logged to traces or spans in Opik. For this example, we will start by creating a simple RAG pipeline and then scoring it using the <code>answer_relevancy</code> metric.</p>"},{"location":"howtos/integrations/opik/#create-the-ragas-metric","title":"Create the Ragas metric\u00b6","text":"<p>In order to use the Ragas metric without using the <code>evaluate</code> function, you need to initialize the metric with a <code>RunConfig</code> object and an LLM provider. For this example, we will use LangChain as the LLM provider with the Opik tracer enabled.</p> <p>We will first start by initializing the Ragas metric:</p>"},{"location":"howtos/integrations/opik/#score-traces","title":"Score traces\u00b6","text":"<p>You can score traces by using the <code>update_current_trace</code> function to get the current trace and passing the feedback scores to that function.</p> <p>The advantage of this approach is that the scoring span is added to the trace allowing for a more fine-grained analysis of the RAG pipeline. It will however run the Ragas metric calculation synchronously and so might not be suitable for production use-cases.</p>"},{"location":"howtos/integrations/opik/#evaluating-datasets","title":"Evaluating datasets\u00b6","text":"<p>If you looking at evaluating a dataset, you can use the Ragas <code>evaluate</code> function. When using this function, the Ragas library will compute the metrics on all the rows of the dataset and return a summary of the results.</p> <p>You can use the OpikTracer callback to log the results of the evaluation to the Opik platform. For this we will configure the OpikTracer</p>"},{"location":"howtos/integrations/tonic-validate/","title":"Tonic Validate","text":""},{"location":"howtos/integrations/tonic-validate/#tonic-validate","title":"Tonic Validate\u00b6","text":""},{"location":"howtos/integrations/tonic-validate/#tonic-validate-visualize-ragas-scores","title":"Tonic Validate: Visualize Ragas Scores\u00b6","text":"<p>Validate makes it easy to understand the performance of your RAG or LLM application by visualizing and tracking over time the scores generated by Ragas.  If you are already using Ragas today getting started is as easy as adding two additional lines of code into your python project.</p>"},{"location":"howtos/integrations/tonic-validate/#getting-started","title":"Getting Started\u00b6","text":"<p>First create a free validate account.  Once logged in, you'll need to create a new project.  A project is typically associated to a single RAG or LLM application you wish to evaluate with Ragas.  Once you've given your project a name you'll be taken to the project's new home page.</p> <p>To begin sending scores to Tonic Validate you'll need to install the tonic-ragas-logger package which is used to ship scores.</p> <pre>pip install tonic-ragas-logger\n</pre> <p>Now, in your existing python project you can add the below two lines of code to wherever you are running Ragas.  This code will take the <code>scores</code> generated by Ragas' <code>evaluate()</code> function and ship the results to Tonic Validate.  The API Key and Project ID referenced below are both available form your newly created project's home page.</p> <pre>validate_api = RagasValidateApi(\"&lt;Validate API Key&gt;\")\nvalidate_api.upload_results(\"&lt;Project ID&gt;\", scores)\n</pre> <p>As you begin sending scores to Validate you'll see Graphs being generated and 'Runs' being created.  A run is a collection of scores computed from a single call to <code>evaluate()</code>.  You can see how average scores change over time or dig into a specific run to see how individual questions performed.  </p>"},{"location":"howtos/integrations/tonic-validate/#reaching-out","title":"Reaching out \ud83d\udc4b\u00b6","text":"<p>If you have any questions or feedback for our UI the easiest way to get in touch is to file a GitHub issue on our repository where we maintain tonic-validate, our own open source evaluation framework.</p>"},{"location":"howtos/integrations/zeno/","title":"Zeno","text":"In\u00a0[\u00a0]: Copied! <pre>import os\n\nimport pandas as pd\nfrom datasets import load_dataset\nfrom ragas import evaluate\nfrom ragas.metrics import (\n    answer_relevancy,\n    context_precision,\n    context_recall,\n    faithfulness,\n)\nfrom zeno_client import ZenoClient, ZenoMetric\n</pre> import os  import pandas as pd from datasets import load_dataset from ragas import evaluate from ragas.metrics import (     answer_relevancy,     context_precision,     context_recall,     faithfulness, ) from zeno_client import ZenoClient, ZenoMetric In\u00a0[\u00a0]: Copied! <pre># Set API keys\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"\nos.environ[\"ZENO_API_KEY\"] = \"your-zeno-api-key\"\n</pre> # Set API keys os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\" os.environ[\"ZENO_API_KEY\"] = \"your-zeno-api-key\" In\u00a0[\u00a0]: Copied! <pre>fiqa_eval = load_dataset(\"explodinggradients/fiqa\", \"ragas_eval\")\nresult = evaluate(\n    fiqa_eval[\"baseline\"],\n    metrics=[\n        context_precision,\n        faithfulness,\n        answer_relevancy,\n        context_recall,\n    ],\n)\n\ndf = result.to_pandas()\ndf.head()\n</pre> fiqa_eval = load_dataset(\"explodinggradients/fiqa\", \"ragas_eval\") result = evaluate(     fiqa_eval[\"baseline\"],     metrics=[         context_precision,         faithfulness,         answer_relevancy,         context_recall,     ], )  df = result.to_pandas() df.head() <p>We can now take the <code>df</code> with our data and results and upload it to Zeno.</p> <p>We first create a project with a custom RAG view specification and the metric columns we want to do evaluation across:</p> In\u00a0[\u00a0]: Copied! <pre>client = ZenoClient(os.environ[\"ZENO_API_KEY\"])\n\nproject = client.create_project(\n    name=\"Ragas FICA eval\",\n    description=\"Evaluation of RAG model using Ragas on the FICA dataset\",\n    view={\n        \"data\": {\n            \"type\": \"vstack\",\n            \"keys\": {\n                \"question\": {\"type\": \"markdown\"},\n                \"texts\": {\n                    \"type\": \"list\",\n                    \"elements\": {\"type\": \"markdown\"},\n                    \"border\": True,\n                    \"pad\": True,\n                },\n            },\n        },\n        \"label\": {\n            \"type\": \"markdown\",\n        },\n        \"output\": {\n            \"type\": \"vstack\",\n            \"keys\": {\n                \"answer\": {\"type\": \"markdown\"},\n                \"ground_truth\": {\n                    \"type\": \"list\",\n                    \"elements\": {\"type\": \"markdown\"},\n                    \"border\": True,\n                    \"pad\": True,\n                },\n            },\n        },\n        \"size\": \"large\",\n    },\n    metrics=[\n        ZenoMetric(\n            name=\"context_precision\", type=\"mean\", columns=[\"context_precision\"]\n        ),\n        ZenoMetric(name=\"faithfulness\", type=\"mean\", columns=[\"faithfulness\"]),\n        ZenoMetric(name=\"answer_relevancy\", type=\"mean\", columns=[\"answer_relevancy\"]),\n        ZenoMetric(name=\"context_recall\", type=\"mean\", columns=[\"context_recall\"]),\n    ],\n)\n</pre> client = ZenoClient(os.environ[\"ZENO_API_KEY\"])  project = client.create_project(     name=\"Ragas FICA eval\",     description=\"Evaluation of RAG model using Ragas on the FICA dataset\",     view={         \"data\": {             \"type\": \"vstack\",             \"keys\": {                 \"question\": {\"type\": \"markdown\"},                 \"texts\": {                     \"type\": \"list\",                     \"elements\": {\"type\": \"markdown\"},                     \"border\": True,                     \"pad\": True,                 },             },         },         \"label\": {             \"type\": \"markdown\",         },         \"output\": {             \"type\": \"vstack\",             \"keys\": {                 \"answer\": {\"type\": \"markdown\"},                 \"ground_truth\": {                     \"type\": \"list\",                     \"elements\": {\"type\": \"markdown\"},                     \"border\": True,                     \"pad\": True,                 },             },         },         \"size\": \"large\",     },     metrics=[         ZenoMetric(             name=\"context_precision\", type=\"mean\", columns=[\"context_precision\"]         ),         ZenoMetric(name=\"faithfulness\", type=\"mean\", columns=[\"faithfulness\"]),         ZenoMetric(name=\"answer_relevancy\", type=\"mean\", columns=[\"answer_relevancy\"]),         ZenoMetric(name=\"context_recall\", type=\"mean\", columns=[\"context_recall\"]),     ], ) <p>Next, we upload the base dataset with the questions and ground truths:</p> In\u00a0[\u00a0]: Copied! <pre>data_df = pd.DataFrame(\n    {\n        \"data\": df.apply(\n            lambda x: {\"question\": x[\"question\"], \"texts\": list(x[\"contexts\"])}, axis=1\n        ),\n        \"label\": df[\"ground_truth\"].apply(lambda x: \"\\n\".join(x)),\n    }\n)\ndata_df[\"id\"] = data_df.index\n\nproject.upload_dataset(\n    data_df, id_column=\"id\", data_column=\"data\", label_column=\"label\"\n)\n</pre> data_df = pd.DataFrame(     {         \"data\": df.apply(             lambda x: {\"question\": x[\"question\"], \"texts\": list(x[\"contexts\"])}, axis=1         ),         \"label\": df[\"ground_truth\"].apply(lambda x: \"\\n\".join(x)),     } ) data_df[\"id\"] = data_df.index  project.upload_dataset(     data_df, id_column=\"id\", data_column=\"data\", label_column=\"label\" ) <p>Lastly, we upload the RAG outputs and Ragas metrics.</p> <p>You can run this for any number of models when doing comparison and iteration:</p> In\u00a0[\u00a0]: Copied! <pre>output_df = df[\n    [\n        \"context_precision\",\n        \"faithfulness\",\n        \"answer_relevancy\",\n        \"context_recall\",\n    ]\n].copy()\n\noutput_df[\"output\"] = df.apply(\n    lambda x: {\"answer\": x[\"answer\"], \"ground_truth\": list(x[\"ground_truth\"])}, axis=1\n)\noutput_df[\"id\"] = output_df.index\n\nproject.upload_system(\n    output_df, name=\"Base System\", id_column=\"id\", output_column=\"output\"\n)\n</pre> output_df = df[     [         \"context_precision\",         \"faithfulness\",         \"answer_relevancy\",         \"context_recall\",     ] ].copy()  output_df[\"output\"] = df.apply(     lambda x: {\"answer\": x[\"answer\"], \"ground_truth\": list(x[\"ground_truth\"])}, axis=1 ) output_df[\"id\"] = output_df.index  project.upload_system(     output_df, name=\"Base System\", id_column=\"id\", output_column=\"output\" ) <p>Reach out to the Zeno team on Discord or at hello@zenoml.com if you have any questions!</p>"},{"location":"howtos/integrations/zeno/#zeno","title":"Zeno\u00b6","text":""},{"location":"howtos/integrations/zeno/#visualizing-ragas-results-with-zeno","title":"Visualizing Ragas Results with Zeno\u00b6","text":"<p>You can use the Zeno evaluation platform to easily visualize and explore the results of your Ragas evaluation.</p> <p>Check out what the result of this tutorial looks like here</p> <p>First, install the <code>zeno-client</code> package:</p> <pre>pip install zeno-client\n</pre> <p>Next, create an account at hub.zenoml.com and generate an API key on your account page.</p> <p>We can now pick up the evaluation where we left off at the Getting Started guide:</p>"},{"location":"references/SUMMARY/","title":"SUMMARY","text":"<ul> <li>ragas<ul> <li>adaptation</li> <li>async_utils</li> <li>callbacks</li> <li>cost</li> <li>dataset_schema</li> <li>embeddings<ul> <li>base</li> </ul> </li> <li>evaluation</li> <li>exceptions</li> <li>executor</li> <li>experimental<ul> <li>metrics</li> </ul> </li> <li>integrations<ul> <li>helicone</li> <li>langchain</li> <li>langsmith</li> <li>llama_index</li> <li>opik</li> </ul> </li> <li>llms<ul> <li>base</li> <li>json_load</li> <li>output_parser</li> <li>prompt</li> </ul> </li> <li>messages</li> <li>metrics<ul> <li>base</li> <li>utils</li> </ul> </li> <li>prompt<ul> <li>base</li> <li>mixin</li> <li>pydantic_prompt</li> <li>utils</li> </ul> </li> <li>run_config</li> <li>testset<ul> <li>graph</li> <li>synthesizers<ul> <li>abstract_query</li> <li>base</li> <li>base_query</li> <li>generate</li> <li>prompts</li> <li>specific_query</li> <li>testset_schema</li> <li>utils</li> </ul> </li> <li>transforms<ul> <li>base</li> <li>engine</li> <li>extractors<ul> <li>embeddings</li> <li>llm_based</li> <li>regex_based</li> </ul> </li> <li>relationship_builders<ul> <li>cosine</li> </ul> </li> <li>splitters<ul> <li>headline</li> </ul> </li> </ul> </li> </ul> </li> <li>utils</li> <li>validation</li> </ul> </li> </ul>"},{"location":"references/__init__/","title":"ragas","text":""},{"location":"references/__init__/#ragas.RunConfig","title":"<code>RunConfig</code>  <code>dataclass</code>","text":"<p>Configuration for a timeouts, retries and seed for Ragas operations.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>int</code> <p>Maximum time (in seconds) to wait for a single operation, by default 60.</p> <code>180</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retry attempts, by default 10.</p> <code>10</code> <code>max_wait</code> <code>int</code> <p>Maximum wait time (in seconds) between retries, by default 60.</p> <code>60</code> <code>max_workers</code> <code>int</code> <p>Maximum number of concurrent workers, by default 16.</p> <code>16</code> <code>exception_types</code> <code>Union[Type[BaseException], Tuple[Type[BaseException], ...]]</code> <p>Exception types to catch and retry on, by default (Exception,).</p> <code>(Exception)</code> <code>log_tenacity</code> <code>bool</code> <p>Whether to log retry attempts using tenacity, by default False.</p> <code>False</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility, by default 42.</p> <code>42</code> <p>Attributes:</p> Name Type Description <code>rng</code> <code>Generator</code> <p>Random number generator initialized with the specified seed.</p> Notes <p>The <code>__post_init__</code> method initializes the <code>rng</code> attribute as a numpy random number generator using the specified seed.</p> Source code in <code>src/ragas/run_config.py</code> <pre><code>@dataclass\nclass RunConfig:\n    \"\"\"\n    Configuration for a timeouts, retries and seed for Ragas operations.\n\n    Parameters\n    ----------\n    timeout : int, optional\n        Maximum time (in seconds) to wait for a single operation, by default 60.\n    max_retries : int, optional\n        Maximum number of retry attempts, by default 10.\n    max_wait : int, optional\n        Maximum wait time (in seconds) between retries, by default 60.\n    max_workers : int, optional\n        Maximum number of concurrent workers, by default 16.\n    exception_types : Union[Type[BaseException], Tuple[Type[BaseException], ...]], optional\n        Exception types to catch and retry on, by default (Exception,).\n    log_tenacity : bool, optional\n        Whether to log retry attempts using tenacity, by default False.\n    seed : int, optional\n        Random seed for reproducibility, by default 42.\n\n    Attributes\n    ----------\n    rng : numpy.random.Generator\n        Random number generator initialized with the specified seed.\n\n    Notes\n    -----\n    The `__post_init__` method initializes the `rng` attribute as a numpy random\n    number generator using the specified seed.\n    \"\"\"\n\n    timeout: int = 180\n    max_retries: int = 10\n    max_wait: int = 60\n    max_workers: int = 16\n    exception_types: t.Union[\n        t.Type[BaseException],\n        t.Tuple[t.Type[BaseException], ...],\n    ] = (Exception,)\n    log_tenacity: bool = False\n    seed: int = 42\n\n    def __post_init__(self):\n        self.rng = np.random.default_rng(seed=self.seed)\n</code></pre>"},{"location":"references/__init__/#ragas.SingleTurnSample","title":"<code>SingleTurnSample</code>","text":"<p>               Bases: <code>BaseEvalSample</code></p> <p>Represents evaluation samples for single-turn interactions.</p> Source code in <code>src/ragas/dataset_schema.py</code> <pre><code>class SingleTurnSample(BaseEvalSample):\n    \"\"\"\n    Represents evaluation samples for single-turn interactions.\n    \"\"\"\n\n    user_input: t.Optional[str] = None\n    retrieved_contexts: t.Optional[t.List[str]] = None\n    reference_contexts: t.Optional[t.List[str]] = None\n    response: t.Optional[str] = None\n    multi_responses: t.Optional[t.List[str]] = None\n    reference: t.Optional[str] = None\n    rubric: t.Optional[t.Dict[str, str]] = None\n</code></pre>"},{"location":"references/__init__/#ragas.adapt","title":"<code>adapt(metrics, language, llm=None, cache_dir=None)</code>","text":"<p>Adapt the metric to a different language.</p> Source code in <code>src/ragas/adaptation.py</code> <pre><code>def adapt(\n    metrics: t.List[MetricWithLLM],\n    language: str,\n    llm: t.Optional[BaseRagasLLM] = None,\n    cache_dir: t.Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Adapt the metric to a different language.\n    \"\"\"\n\n    llm_wraper = None\n\n    if llm is None:\n        llm_wraper = llm_factory()\n    elif isinstance(llm, BaseLanguageModel):\n        llm_wraper = LangchainLLMWrapper(llm)\n    else:\n        raise ValueError(\"llm must be either None or a BaseLanguageModel\")\n\n    for metric in metrics:\n        metric_llm = metric.llm\n\n        if metric_llm is None or llm is not None:\n            metric.llm = llm_wraper\n\n        if hasattr(metric, \"adapt\"):\n            metric.adapt(language, cache_dir=cache_dir)\n            metric.save(cache_dir=cache_dir)\n            metric.llm = metric_llm\n</code></pre>"},{"location":"references/__init__/#ragas.evaluate","title":"<code>evaluate(dataset, metrics=None, llm=None, embeddings=None, callbacks=None, in_ci=False, run_config=RunConfig(), token_usage_parser=None, raise_exceptions=False, column_map=None, show_progress=True)</code>","text":"<p>Run the evaluation on the dataset with different metrics</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset[question:list[str], contexts:list[list[str]], answer:list[str], ground_truth:list[list[str]]]</code> <p>The dataset in the format of ragas which the metrics will use to score the RAG pipeline with</p> required <code>metrics</code> <code>list[Metric]</code> <p>List of metrics to use for evaluation. If not provided then ragas will run the evaluation on the best set of metrics to give a complete view.</p> <code>None</code> <code>llm</code> <code>Optional[BaseRagasLLM | BaseLanguageModel]</code> <p>The language model to use for the metrics. If not provided then ragas will use the default language model for metrics which require an LLM. This can we overridden by the llm specified in the metric level with <code>metric.llm</code>.</p> <code>None</code> <code>embeddings</code> <code>Optional[BaseRagasEmbeddings | Embeddings]</code> <p>The embeddings to use for the metrics. If not provided then ragas will use the default embeddings for metrics which require embeddings. This can we overridden by the embeddings specified in the metric level with <code>metric.embeddings</code>.</p> <code>None</code> <code>callbacks</code> <code>Callbacks</code> <p>Lifecycle Langchain Callbacks to run during evaluation. Check the langchain documentation for more information.</p> <code>None</code> <code>in_ci</code> <code>bool</code> <p>Whether the evaluation is running in CI or not. If set to True then some metrics will be run to increase the reproducability of the evaluations. This will increase the runtime and cost of evaluations. Default is False.</p> <code>False</code> <code>run_config</code> <code>RunConfig</code> <p>Configuration for runtime settings like timeout and retries. If not provided, default values are used.</p> <code>RunConfig()</code> <code>token_usage_parser</code> <code>Optional[TokenUsageParser]</code> <p>Parser to get the token usage from the LLM result. If not provided then the the cost and total tokens will not be calculated. Default is None.</p> <code>None</code> <code>raise_exceptions</code> <code>bool</code> <p>Whether to raise exceptions or not. If set to True then the evaluation will raise an exception if any of the metrics fail. If set to False then the evaluation will return <code>np.nan</code> for the row that failed. Default is False.</p> <code>False</code> <code>column_map</code> <code>dict[str, str]</code> <p>The column names of the dataset to use for evaluation. If the column names of the dataset are different from the default ones then you can provide the mapping as a dictionary here. Example: If the dataset column name is contexts_v1, column_map can be given as {\"contexts\":\"contexts_v1\"}</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>Whether to show the progress bar during evaluation. If set to False, the progress bar will be disabled. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Result</code> <p>Result object containing the scores of each metric. You can use this do analysis later.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if validation fails because the columns required for the metrics are missing or if the columns are of the wrong format.</p> <p>Examples:</p> <p>the basic usage is as follows: <pre><code>from ragas import evaluate\n\n&gt;&gt;&gt; dataset\nDataset({\n    features: ['question', 'ground_truth', 'answer', 'contexts'],\n    num_rows: 30\n})\n\n&gt;&gt;&gt; result = evaluate(dataset)\n&gt;&gt;&gt; print(result)\n{'context_precision': 0.817,\n'faithfulness': 0.892,\n'answer_relevancy': 0.874}\n</code></pre></p> Source code in <code>src/ragas/evaluation.py</code> <pre><code>@track_was_completed\ndef evaluate(\n    dataset: t.Union[Dataset, EvaluationDataset],\n    metrics: list[Metric] | None = None,\n    llm: t.Optional[BaseRagasLLM | LangchainLLM] = None,\n    embeddings: t.Optional[BaseRagasEmbeddings | LangchainEmbeddings] = None,\n    callbacks: Callbacks = None,\n    in_ci: bool = False,\n    run_config: RunConfig = RunConfig(),\n    token_usage_parser: t.Optional[TokenUsageParser] = None,\n    raise_exceptions: bool = False,\n    column_map: t.Optional[t.Dict[str, str]] = None,\n    show_progress: bool = True,\n) -&gt; Result:\n    \"\"\"\n    Run the evaluation on the dataset with different metrics\n\n    Parameters\n    ----------\n    dataset : Dataset[question: list[str], contexts: list[list[str]], answer: list[str], ground_truth: list[list[str]]]\n        The dataset in the format of ragas which the metrics will use to score the RAG\n        pipeline with\n    metrics : list[Metric] , optional\n        List of metrics to use for evaluation. If not provided then ragas will run the\n        evaluation on the best set of metrics to give a complete view.\n    llm: BaseRagasLLM, optional\n        The language model to use for the metrics. If not provided then ragas will use\n        the default language model for metrics which require an LLM. This can we overridden by the llm specified in\n        the metric level with `metric.llm`.\n    embeddings: BaseRagasEmbeddings, optional\n        The embeddings to use for the metrics. If not provided then ragas will use\n        the default embeddings for metrics which require embeddings. This can we overridden by the embeddings specified in\n        the metric level with `metric.embeddings`.\n    callbacks: Callbacks, optional\n        Lifecycle Langchain Callbacks to run during evaluation. Check the\n        [langchain documentation](https://python.langchain.com/docs/modules/callbacks/)\n        for more information.\n    in_ci: bool\n        Whether the evaluation is running in CI or not. If set to True then some\n        metrics will be run to increase the reproducability of the evaluations. This\n        will increase the runtime and cost of evaluations. Default is False.\n    run_config: RunConfig, optional\n        Configuration for runtime settings like timeout and retries. If not provided,\n        default values are used.\n    token_usage_parser: TokenUsageParser, optional\n        Parser to get the token usage from the LLM result. If not provided then the\n        the cost and total tokens will not be calculated. Default is None.\n    raise_exceptions: False\n        Whether to raise exceptions or not. If set to True then the evaluation will\n        raise an exception if any of the metrics fail. If set to False then the\n        evaluation will return `np.nan` for the row that failed. Default is False.\n    column_map : dict[str, str], optional\n        The column names of the dataset to use for evaluation. If the column names of\n        the dataset are different from the default ones then you can provide the\n        mapping as a dictionary here. Example: If the dataset column name is contexts_v1,\n        column_map can be given as {\"contexts\":\"contexts_v1\"}\n    show_progress: bool, optional\n        Whether to show the progress bar during evaluation. If set to False, the progress bar will be disabled. Default is True.\n\n    Returns\n    -------\n    Result\n        Result object containing the scores of each metric. You can use this do analysis\n        later.\n\n    Raises\n    ------\n    ValueError\n        if validation fails because the columns required for the metrics are missing or\n        if the columns are of the wrong format.\n\n    Examples\n    --------\n    the basic usage is as follows:\n    ```\n    from ragas import evaluate\n\n    &gt;&gt;&gt; dataset\n    Dataset({\n        features: ['question', 'ground_truth', 'answer', 'contexts'],\n        num_rows: 30\n    })\n\n    &gt;&gt;&gt; result = evaluate(dataset)\n    &gt;&gt;&gt; print(result)\n    {'context_precision': 0.817,\n    'faithfulness': 0.892,\n    'answer_relevancy': 0.874}\n    ```\n    \"\"\"\n    column_map = column_map or {}\n    callbacks = callbacks or []\n\n    if helicone_config.is_enabled:\n        import uuid\n\n        helicone_config.session_name = \"ragas-evaluation\"\n        helicone_config.session_id = str(uuid.uuid4())\n\n    if dataset is None:\n        raise ValueError(\"Provide dataset!\")\n\n    # default metrics\n    if metrics is None:\n        from ragas.metrics import (\n            answer_relevancy,\n            context_precision,\n            context_recall,\n            faithfulness,\n        )\n\n        metrics = [answer_relevancy, context_precision, faithfulness, context_recall]\n\n    v1_input = False\n    if isinstance(dataset, Dataset):\n        # remap column names from the dataset\n        v1_input = True\n        dataset = remap_column_names(dataset, column_map)\n        dataset = convert_v1_to_v2_dataset(dataset)\n        # validation\n        dataset = EvaluationDataset.from_list(dataset.to_list())\n\n    if isinstance(dataset, EvaluationDataset):\n        validate_required_columns(dataset, metrics)\n        validate_supported_metrics(dataset, metrics)\n\n    # set the llm and embeddings\n    if isinstance(llm, LangchainLLM):\n        llm = LangchainLLMWrapper(llm, run_config=run_config)\n    if isinstance(embeddings, LangchainEmbeddings):\n        embeddings = LangchainEmbeddingsWrapper(embeddings)\n\n    # init llms and embeddings\n    binary_metrics = []\n    llm_changed: t.List[int] = []\n    embeddings_changed: t.List[int] = []\n    reproducable_metrics: t.List[int] = []\n    answer_correctness_is_set = -1\n\n    # loop through the metrics and perform initializations\n    for i, metric in enumerate(metrics):\n        # set llm and embeddings if not set\n        if isinstance(metric, AspectCritic):\n            binary_metrics.append(metric.name)\n        if isinstance(metric, MetricWithLLM) and metric.llm is None:\n            if llm is None:\n                llm = llm_factory()\n            metric.llm = llm\n            llm_changed.append(i)\n        if isinstance(metric, MetricWithEmbeddings) and metric.embeddings is None:\n            if embeddings is None:\n                embeddings = embedding_factory()\n            metric.embeddings = embeddings\n            embeddings_changed.append(i)\n        if isinstance(metric, AnswerCorrectness):\n            if metric.answer_similarity is None:\n                answer_correctness_is_set = i\n        # set reproducibility for metrics if in CI\n        if in_ci and is_reproducable(metric):\n            if metric.reproducibility == 1:  # type: ignore\n                # only set a value if not already set\n                metric.reproducibility = 3  # type: ignore\n                reproducable_metrics.append(i)\n\n        # init all the models\n        metric.init(run_config)\n\n    executor = Executor(\n        desc=\"Evaluating\",\n        keep_progress_bar=True,\n        raise_exceptions=raise_exceptions,\n        run_config=run_config,\n        show_progress=show_progress,\n    )\n\n    # Ragas Callbacks\n    # init the callbacks we need for various tasks\n    ragas_callbacks: t.Dict[str, BaseCallbackHandler] = {}\n\n    # check if cost needs to be calculated\n    if token_usage_parser is not None:\n        from ragas.cost import CostCallbackHandler\n\n        cost_cb = CostCallbackHandler(token_usage_parser=token_usage_parser)\n        ragas_callbacks[\"cost_cb\"] = cost_cb\n\n    # append all the ragas_callbacks to the callbacks\n    for cb in ragas_callbacks.values():\n        if isinstance(callbacks, BaseCallbackManager):\n            callbacks.add_handler(cb)\n        else:\n            callbacks.append(cb)\n\n    # new evaluation chain\n    row_run_managers = []\n    evaluation_rm, evaluation_group_cm = new_group(\n        name=RAGAS_EVALUATION_CHAIN_NAME, inputs={}, callbacks=callbacks\n    )\n\n    sample_type = dataset.get_sample_type()\n    for i, sample in enumerate(dataset):\n        row = t.cast(t.Dict[str, t.Any], sample.dict())\n        row_rm, row_group_cm = new_group(\n            name=f\"row {i}\",\n            inputs=row,\n            callbacks=evaluation_group_cm,\n        )\n        row_run_managers.append((row_rm, row_group_cm))\n        if sample_type == SingleTurnSample:\n            _ = [\n                executor.submit(\n                    metric.single_turn_ascore,\n                    sample,\n                    row_group_cm,\n                    name=f\"{metric.name}-{i}\",\n                    timeout=run_config.timeout,\n                )\n                for metric in metrics\n                if isinstance(metric, SingleTurnMetric)\n            ]\n        elif sample_type == MultiTurnSample:\n            _ = [\n                executor.submit(\n                    metric.multi_turn_ascore,\n                    sample,\n                    row_group_cm,\n                    name=f\"{metric.name}-{i}\",\n                    timeout=run_config.timeout,\n                )\n                for metric in metrics\n                if isinstance(metric, MultiTurnMetric)\n            ]\n        else:\n            raise ValueError(f\"Unsupported sample type {sample_type}\")\n\n    scores = []\n    try:\n        # get the results\n        results = executor.results()\n        if results == []:\n            raise ExceptionInRunner()\n\n        # convert results to dataset_like\n        for i, _ in enumerate(dataset):\n            s = {}\n            for j, m in enumerate(metrics):\n                s[m.name] = results[len(metrics) * i + j]\n            scores.append(s)\n            # close the row chain\n            row_rm, row_group_cm = row_run_managers[i]\n            if not row_group_cm.ended:\n                row_rm.on_chain_end(s)\n\n    # run evaluation task\n    except Exception as e:\n        if not evaluation_group_cm.ended:\n            evaluation_rm.on_chain_error(e)\n\n        raise e\n    else:\n        # evalution run was successful\n        # now lets process the results\n        # convert to v.1 dataset\n        dataset = dataset.to_hf_dataset()\n        if v1_input:\n            dataset = convert_v2_to_v1_dataset(dataset)\n\n        cost_cb = ragas_callbacks[\"cost_cb\"] if \"cost_cb\" in ragas_callbacks else None\n        result = Result(\n            scores=Dataset.from_list(scores),\n            dataset=dataset,\n            binary_columns=binary_metrics,\n            cost_cb=t.cast(\n                t.Union[\"CostCallbackHandler\", None],\n                cost_cb,\n            ),\n        )\n        if not evaluation_group_cm.ended:\n            evaluation_rm.on_chain_end(result)\n    finally:\n        # reset llms and embeddings if changed\n        for i in llm_changed:\n            t.cast(MetricWithLLM, metrics[i]).llm = None\n        for i in embeddings_changed:\n            t.cast(MetricWithEmbeddings, metrics[i]).embeddings = None\n        if answer_correctness_is_set != -1:\n            t.cast(\n                AnswerCorrectness, metrics[answer_correctness_is_set]\n            ).answer_similarity = None\n\n        for i in reproducable_metrics:\n            metrics[i].reproducibility = 1  # type: ignore\n\n    # log the evaluation event\n    metrics_names = [m.name for m in metrics]\n    metric_lang = [get_feature_language(m) for m in metrics]\n    metric_lang = np.unique([m for m in metric_lang if m is not None])\n    track(\n        EvaluationEvent(\n            event_type=\"evaluation\",\n            metrics=metrics_names,\n            evaluation_mode=\"\",\n            num_rows=len(dataset),\n            language=metric_lang[0] if len(metric_lang) &gt; 0 else \"\",\n            in_ci=in_ci,\n        )\n    )\n    return result\n</code></pre>"},{"location":"references/adaptation/","title":"adaptation","text":""},{"location":"references/adaptation/#ragas.adaptation.adapt","title":"<code>adapt(metrics, language, llm=None, cache_dir=None)</code>","text":"<p>Adapt the metric to a different language.</p> Source code in <code>src/ragas/adaptation.py</code> <pre><code>def adapt(\n    metrics: t.List[MetricWithLLM],\n    language: str,\n    llm: t.Optional[BaseRagasLLM] = None,\n    cache_dir: t.Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Adapt the metric to a different language.\n    \"\"\"\n\n    llm_wraper = None\n\n    if llm is None:\n        llm_wraper = llm_factory()\n    elif isinstance(llm, BaseLanguageModel):\n        llm_wraper = LangchainLLMWrapper(llm)\n    else:\n        raise ValueError(\"llm must be either None or a BaseLanguageModel\")\n\n    for metric in metrics:\n        metric_llm = metric.llm\n\n        if metric_llm is None or llm is not None:\n            metric.llm = llm_wraper\n\n        if hasattr(metric, \"adapt\"):\n            metric.adapt(language, cache_dir=cache_dir)\n            metric.save(cache_dir=cache_dir)\n            metric.llm = metric_llm\n</code></pre>"},{"location":"references/async_utils/","title":"async_utils","text":"<p>Async utils.</p>"},{"location":"references/async_utils/#ragas.async_utils.run_async_tasks","title":"<code>run_async_tasks(tasks, show_progress=False, progress_bar_desc='Running async tasks')</code>","text":"<p>Run a list of async tasks.</p> Source code in <code>src/ragas/async_utils.py</code> <pre><code>def run_async_tasks(\n    tasks: List[Coroutine],\n    show_progress: bool = False,\n    progress_bar_desc: str = \"Running async tasks\",\n) -&gt; List[Any]:\n    \"\"\"Run a list of async tasks.\"\"\"\n    tasks_to_execute: List[Any] = tasks\n\n    # if running in notebook, use nest_asyncio to hijack the event loop\n    try:\n        loop = asyncio.get_running_loop()\n        try:\n            import nest_asyncio\n        except ImportError:\n            raise RuntimeError(\n                \"nest_asyncio is required to run async tasks in jupyter. Please install it via `pip install nest_asyncio`.\"  # noqa\n            )\n        else:\n            nest_asyncio.apply()\n    except RuntimeError:\n        loop = asyncio.new_event_loop()\n\n    # gather tasks to run\n    if show_progress:\n        from tqdm.asyncio import tqdm\n\n        async def _gather() -&gt; List[Any]:\n            \"gather tasks and show progress bar\"\n            return await tqdm.gather(*tasks_to_execute, desc=progress_bar_desc)\n\n    else:  # don't show_progress\n\n        async def _gather() -&gt; List[Any]:\n            return await asyncio.gather(*tasks_to_execute)\n\n    try:\n        outputs: List[Any] = loop.run_until_complete(_gather())\n    except Exception as e:\n        # run the operation w/o tqdm on hitting a fatal\n        # may occur in some environments where tqdm.asyncio\n        # is not supported\n        raise RuntimeError(\"Fatal error occurred while running async tasks.\", e) from e\n    return outputs\n</code></pre>"},{"location":"references/callbacks/","title":"callbacks","text":""},{"location":"references/cost/","title":"cost","text":""},{"location":"references/cost/#ragas.cost.CostCallbackHandler","title":"<code>CostCallbackHandler</code>","text":"<p>               Bases: <code>BaseCallbackHandler</code></p> Source code in <code>src/ragas/cost.py</code> <pre><code>class CostCallbackHandler(BaseCallbackHandler):\n    def __init__(self, token_usage_parser: TokenUsageParser):\n        self.token_usage_parser = token_usage_parser\n        self.usage_data: t.List[TokenUsage] = []\n\n    def on_llm_end(self, response: LLMResult, **kwargs: t.Any):\n        self.usage_data.append(self.token_usage_parser(response))\n\n    def total_cost(\n        self,\n        cost_per_input_token: t.Optional[float] = None,\n        cost_per_output_token: t.Optional[float] = None,\n        per_model_costs: t.Dict[str, t.Tuple[float, float]] = {},\n    ) -&gt; float:\n        if (\n            per_model_costs == {}\n            and cost_per_input_token is None\n            and cost_per_output_token is None\n        ):\n            raise ValueError(\n                \"No cost table or cost per token provided. Please provide a cost table if using multiple models or cost per token if using a single model\"\n            )\n\n        # sum up everything\n        first_usage = self.usage_data[0]\n        total_table: t.Dict[str, TokenUsage] = {first_usage.model: first_usage}\n        for usage in self.usage_data[1:]:\n            if usage.model in total_table:\n                total_table[usage.model] += usage\n            else:\n                total_table[usage.model] = usage\n\n        # caculate total cost\n        # if only one model is used\n        if len(total_table) == 1:\n            model_name = list(total_table)[0]\n            # if per model cost is provided check that\n            if per_model_costs != {}:\n                if model_name not in per_model_costs:\n                    raise ValueError(f\"Model {model_name} not found in per_model_costs\")\n                cpit, cpot = per_model_costs[model_name]\n                return total_table[model_name].cost(cpit, cpot)\n            # else use the cost_per_token vals\n            else:\n                if cost_per_output_token is None:\n                    cost_per_output_token = cost_per_input_token\n                assert cost_per_input_token is not None\n                return total_table[model_name].cost(\n                    cost_per_input_token, cost_per_output_token\n                )\n        else:\n            total_cost = 0.0\n            for model, usage in total_table.items():\n                if model in per_model_costs:\n                    cpit, cpot = per_model_costs[model]\n                    total_cost += usage.cost(cpit, cpot)\n            return total_cost\n\n    def total_tokens(self) -&gt; t.Union[TokenUsage, t.List[TokenUsage]]:\n        \"\"\"\n        Return the sum of tokens used by the callback handler\n        \"\"\"\n        first_usage = self.usage_data[0]\n        total_table: t.Dict[str, TokenUsage] = {first_usage.model: first_usage}\n        for usage in self.usage_data[1:]:\n            if usage.model in total_table:\n                total_table[usage.model] += usage\n            else:\n                total_table[usage.model] = usage\n\n        if len(total_table) == 1:\n            return list(total_table.values())[0]\n        else:\n            return list(total_table.values())\n</code></pre>"},{"location":"references/cost/#ragas.cost.CostCallbackHandler.total_tokens","title":"<code>total_tokens()</code>","text":"<p>Return the sum of tokens used by the callback handler</p> Source code in <code>src/ragas/cost.py</code> <pre><code>def total_tokens(self) -&gt; t.Union[TokenUsage, t.List[TokenUsage]]:\n    \"\"\"\n    Return the sum of tokens used by the callback handler\n    \"\"\"\n    first_usage = self.usage_data[0]\n    total_table: t.Dict[str, TokenUsage] = {first_usage.model: first_usage}\n    for usage in self.usage_data[1:]:\n        if usage.model in total_table:\n            total_table[usage.model] += usage\n        else:\n            total_table[usage.model] = usage\n\n    if len(total_table) == 1:\n        return list(total_table.values())[0]\n    else:\n        return list(total_table.values())\n</code></pre>"},{"location":"references/dataset_schema/","title":"dataset_schema","text":""},{"location":"references/dataset_schema/#ragas.dataset_schema.BaseEvalSample","title":"<code>BaseEvalSample</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/ragas/dataset_schema.py</code> <pre><code>class BaseEvalSample(BaseModel):\n    def to_dict(self) -&gt; t.Dict:\n        \"\"\"\n        Get the dictionary representation of the sample without attributes that are None.\n        \"\"\"\n        return self.model_dump(exclude_none=True)\n\n    def get_features(self) -&gt; t.List[str]:\n        \"\"\"\n        Get the features of the sample that are not None.\n        \"\"\"\n        return list(self.to_dict().keys())\n</code></pre>"},{"location":"references/dataset_schema/#ragas.dataset_schema.BaseEvalSample.get_features","title":"<code>get_features()</code>","text":"<p>Get the features of the sample that are not None.</p> Source code in <code>src/ragas/dataset_schema.py</code> <pre><code>def get_features(self) -&gt; t.List[str]:\n    \"\"\"\n    Get the features of the sample that are not None.\n    \"\"\"\n    return list(self.to_dict().keys())\n</code></pre>"},{"location":"references/dataset_schema/#ragas.dataset_schema.BaseEvalSample.to_dict","title":"<code>to_dict()</code>","text":"<p>Get the dictionary representation of the sample without attributes that are None.</p> Source code in <code>src/ragas/dataset_schema.py</code> <pre><code>def to_dict(self) -&gt; t.Dict:\n    \"\"\"\n    Get the dictionary representation of the sample without attributes that are None.\n    \"\"\"\n    return self.model_dump(exclude_none=True)\n</code></pre>"},{"location":"references/dataset_schema/#ragas.dataset_schema.SingleTurnSample","title":"<code>SingleTurnSample</code>","text":"<p>               Bases: <code>BaseEvalSample</code></p> <p>Represents evaluation samples for single-turn interactions.</p> Source code in <code>src/ragas/dataset_schema.py</code> <pre><code>class SingleTurnSample(BaseEvalSample):\n    \"\"\"\n    Represents evaluation samples for single-turn interactions.\n    \"\"\"\n\n    user_input: t.Optional[str] = None\n    retrieved_contexts: t.Optional[t.List[str]] = None\n    reference_contexts: t.Optional[t.List[str]] = None\n    response: t.Optional[str] = None\n    multi_responses: t.Optional[t.List[str]] = None\n    reference: t.Optional[str] = None\n    rubric: t.Optional[t.Dict[str, str]] = None\n</code></pre>"},{"location":"references/evaluation/","title":"evaluation","text":""},{"location":"references/evaluation/#ragas.evaluation.Result","title":"<code>Result</code>  <code>dataclass</code>","text":"<p>               Bases: <code>dict</code></p> <p>A class to store and process the results of the evaluation.</p> <p>Attributes:</p> Name Type Description <code>scores</code> <code>Dataset</code> <p>The dataset containing the scores of the evaluation.</p> <code>dataset</code> <code>(Dataset, optional)</code> <p>The original dataset used for the evaluation. Default is None.</p> <code>binary_columns</code> <code>list of str, optional</code> <p>List of columns that are binary metrics. Default is an empty list.</p> <code>cost_cb</code> <code>(CostCallbackHandler, optional)</code> <p>The callback handler for cost computation. Default is None.</p> Source code in <code>src/ragas/evaluation.py</code> <pre><code>@dataclass\nclass Result(dict):\n    \"\"\"\n    A class to store and process the results of the evaluation.\n\n    Attributes\n    ----------\n    scores : Dataset\n        The dataset containing the scores of the evaluation.\n    dataset : Dataset, optional\n        The original dataset used for the evaluation. Default is None.\n    binary_columns : list of str, optional\n        List of columns that are binary metrics. Default is an empty list.\n    cost_cb : CostCallbackHandler, optional\n        The callback handler for cost computation. Default is None.\n    \"\"\"\n\n    scores: Dataset\n    dataset: t.Optional[Dataset] = None\n    binary_columns: t.List[str] = field(default_factory=list)\n    cost_cb: t.Optional[CostCallbackHandler] = None\n\n    def __post_init__(self):\n        values = []\n        for cn in self.scores[0].keys():\n            value = safe_nanmean(self.scores[cn])\n            self[cn] = value\n            if cn not in self.binary_columns:\n                value = t.cast(float, value)\n                values.append(value + 1e-10)\n\n    def to_pandas(self, batch_size: int | None = None, batched: bool = False):\n        \"\"\"\n        Convert the result to a pandas DataFrame.\n\n        Parameters\n        ----------\n        batch_size : int, optional\n            The batch size for conversion. Default is None.\n        batched : bool, optional\n            Whether to convert in batches. Default is False.\n\n        Returns\n        -------\n        pandas.DataFrame\n            The result as a pandas DataFrame.\n\n        Raises\n        ------\n        ValueError\n            If the dataset is not provided.\n        \"\"\"\n        if self.dataset is None:\n            raise ValueError(\"dataset is not provided for the results class\")\n        assert self.scores.shape[0] == self.dataset.shape[0]\n        result_ds = concatenate_datasets([self.dataset, self.scores], axis=1)\n\n        return result_ds.to_pandas(batch_size=batch_size, batched=batched)\n\n    def total_tokens(self) -&gt; t.Union[t.List[TokenUsage], TokenUsage]:\n        \"\"\"\n        Compute the total tokens used in the evaluation.\n\n        Returns\n        -------\n        list of TokenUsage or TokenUsage\n            The total tokens used.\n\n        Raises\n        ------\n        ValueError\n            If the cost callback handler is not provided.\n        \"\"\"\n        if self.cost_cb is None:\n            raise ValueError(\n                \"The evaluate() run was not configured for computing cost. Please provide a token_usage_parser function to evaluate() to compute cost.\"\n            )\n        return self.cost_cb.total_tokens()\n\n    def total_cost(\n        self,\n        cost_per_input_token: t.Optional[float] = None,\n        cost_per_output_token: t.Optional[float] = None,\n        per_model_costs: t.Dict[str, t.Tuple[float, float]] = {},\n    ) -&gt; float:\n        \"\"\"\n        Compute the total cost of the evaluation.\n\n        Parameters\n        ----------\n        cost_per_input_token : float, optional\n            The cost per input token. Default is None.\n        cost_per_output_token : float, optional\n            The cost per output token. Default is None.\n        per_model_costs : dict of str to tuple of float, optional\n            The per model costs. Default is an empty dictionary.\n\n        Returns\n        -------\n        float\n            The total cost of the evaluation.\n\n        Raises\n        ------\n        ValueError\n            If the cost callback handler is not provided.\n        \"\"\"\n        if self.cost_cb is None:\n            raise ValueError(\n                \"The evaluate() run was not configured for computing cost. Please provide a token_usage_parser function to evaluate() to compute cost.\"\n            )\n        return self.cost_cb.total_cost(\n            cost_per_input_token, cost_per_output_token, per_model_costs\n        )\n\n    def __repr__(self) -&gt; str:\n        scores = self.copy()\n        score_strs = [f\"'{k}': {v:0.4f}\" for k, v in scores.items()]\n        return \"{\" + \", \".join(score_strs) + \"}\"\n</code></pre>"},{"location":"references/evaluation/#ragas.evaluation.Result.to_pandas","title":"<code>to_pandas(batch_size=None, batched=False)</code>","text":"<p>Convert the result to a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The batch size for conversion. Default is None.</p> <code>None</code> <code>batched</code> <code>bool</code> <p>Whether to convert in batches. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The result as a pandas DataFrame.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the dataset is not provided.</p> Source code in <code>src/ragas/evaluation.py</code> <pre><code>def to_pandas(self, batch_size: int | None = None, batched: bool = False):\n    \"\"\"\n    Convert the result to a pandas DataFrame.\n\n    Parameters\n    ----------\n    batch_size : int, optional\n        The batch size for conversion. Default is None.\n    batched : bool, optional\n        Whether to convert in batches. Default is False.\n\n    Returns\n    -------\n    pandas.DataFrame\n        The result as a pandas DataFrame.\n\n    Raises\n    ------\n    ValueError\n        If the dataset is not provided.\n    \"\"\"\n    if self.dataset is None:\n        raise ValueError(\"dataset is not provided for the results class\")\n    assert self.scores.shape[0] == self.dataset.shape[0]\n    result_ds = concatenate_datasets([self.dataset, self.scores], axis=1)\n\n    return result_ds.to_pandas(batch_size=batch_size, batched=batched)\n</code></pre>"},{"location":"references/evaluation/#ragas.evaluation.Result.total_cost","title":"<code>total_cost(cost_per_input_token=None, cost_per_output_token=None, per_model_costs={})</code>","text":"<p>Compute the total cost of the evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>cost_per_input_token</code> <code>float</code> <p>The cost per input token. Default is None.</p> <code>None</code> <code>cost_per_output_token</code> <code>float</code> <p>The cost per output token. Default is None.</p> <code>None</code> <code>per_model_costs</code> <code>dict of str to tuple of float</code> <p>The per model costs. Default is an empty dictionary.</p> <code>{}</code> <p>Returns:</p> Type Description <code>float</code> <p>The total cost of the evaluation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the cost callback handler is not provided.</p> Source code in <code>src/ragas/evaluation.py</code> <pre><code>def total_cost(\n    self,\n    cost_per_input_token: t.Optional[float] = None,\n    cost_per_output_token: t.Optional[float] = None,\n    per_model_costs: t.Dict[str, t.Tuple[float, float]] = {},\n) -&gt; float:\n    \"\"\"\n    Compute the total cost of the evaluation.\n\n    Parameters\n    ----------\n    cost_per_input_token : float, optional\n        The cost per input token. Default is None.\n    cost_per_output_token : float, optional\n        The cost per output token. Default is None.\n    per_model_costs : dict of str to tuple of float, optional\n        The per model costs. Default is an empty dictionary.\n\n    Returns\n    -------\n    float\n        The total cost of the evaluation.\n\n    Raises\n    ------\n    ValueError\n        If the cost callback handler is not provided.\n    \"\"\"\n    if self.cost_cb is None:\n        raise ValueError(\n            \"The evaluate() run was not configured for computing cost. Please provide a token_usage_parser function to evaluate() to compute cost.\"\n        )\n    return self.cost_cb.total_cost(\n        cost_per_input_token, cost_per_output_token, per_model_costs\n    )\n</code></pre>"},{"location":"references/evaluation/#ragas.evaluation.Result.total_tokens","title":"<code>total_tokens()</code>","text":"<p>Compute the total tokens used in the evaluation.</p> <p>Returns:</p> Type Description <code>list of TokenUsage or TokenUsage</code> <p>The total tokens used.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the cost callback handler is not provided.</p> Source code in <code>src/ragas/evaluation.py</code> <pre><code>def total_tokens(self) -&gt; t.Union[t.List[TokenUsage], TokenUsage]:\n    \"\"\"\n    Compute the total tokens used in the evaluation.\n\n    Returns\n    -------\n    list of TokenUsage or TokenUsage\n        The total tokens used.\n\n    Raises\n    ------\n    ValueError\n        If the cost callback handler is not provided.\n    \"\"\"\n    if self.cost_cb is None:\n        raise ValueError(\n            \"The evaluate() run was not configured for computing cost. Please provide a token_usage_parser function to evaluate() to compute cost.\"\n        )\n    return self.cost_cb.total_tokens()\n</code></pre>"},{"location":"references/evaluation/#ragas.evaluation.evaluate","title":"<code>evaluate(dataset, metrics=None, llm=None, embeddings=None, callbacks=None, in_ci=False, run_config=RunConfig(), token_usage_parser=None, raise_exceptions=False, column_map=None, show_progress=True)</code>","text":"<p>Run the evaluation on the dataset with different metrics</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset[question:list[str], contexts:list[list[str]], answer:list[str], ground_truth:list[list[str]]]</code> <p>The dataset in the format of ragas which the metrics will use to score the RAG pipeline with</p> required <code>metrics</code> <code>list[Metric]</code> <p>List of metrics to use for evaluation. If not provided then ragas will run the evaluation on the best set of metrics to give a complete view.</p> <code>None</code> <code>llm</code> <code>Optional[BaseRagasLLM | BaseLanguageModel]</code> <p>The language model to use for the metrics. If not provided then ragas will use the default language model for metrics which require an LLM. This can we overridden by the llm specified in the metric level with <code>metric.llm</code>.</p> <code>None</code> <code>embeddings</code> <code>Optional[BaseRagasEmbeddings | Embeddings]</code> <p>The embeddings to use for the metrics. If not provided then ragas will use the default embeddings for metrics which require embeddings. This can we overridden by the embeddings specified in the metric level with <code>metric.embeddings</code>.</p> <code>None</code> <code>callbacks</code> <code>Callbacks</code> <p>Lifecycle Langchain Callbacks to run during evaluation. Check the langchain documentation for more information.</p> <code>None</code> <code>in_ci</code> <code>bool</code> <p>Whether the evaluation is running in CI or not. If set to True then some metrics will be run to increase the reproducability of the evaluations. This will increase the runtime and cost of evaluations. Default is False.</p> <code>False</code> <code>run_config</code> <code>RunConfig</code> <p>Configuration for runtime settings like timeout and retries. If not provided, default values are used.</p> <code>RunConfig()</code> <code>token_usage_parser</code> <code>Optional[TokenUsageParser]</code> <p>Parser to get the token usage from the LLM result. If not provided then the the cost and total tokens will not be calculated. Default is None.</p> <code>None</code> <code>raise_exceptions</code> <code>bool</code> <p>Whether to raise exceptions or not. If set to True then the evaluation will raise an exception if any of the metrics fail. If set to False then the evaluation will return <code>np.nan</code> for the row that failed. Default is False.</p> <code>False</code> <code>column_map</code> <code>dict[str, str]</code> <p>The column names of the dataset to use for evaluation. If the column names of the dataset are different from the default ones then you can provide the mapping as a dictionary here. Example: If the dataset column name is contexts_v1, column_map can be given as {\"contexts\":\"contexts_v1\"}</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>Whether to show the progress bar during evaluation. If set to False, the progress bar will be disabled. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Result</code> <p>Result object containing the scores of each metric. You can use this do analysis later.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if validation fails because the columns required for the metrics are missing or if the columns are of the wrong format.</p> <p>Examples:</p> <p>the basic usage is as follows: <pre><code>from ragas import evaluate\n\n&gt;&gt;&gt; dataset\nDataset({\n    features: ['question', 'ground_truth', 'answer', 'contexts'],\n    num_rows: 30\n})\n\n&gt;&gt;&gt; result = evaluate(dataset)\n&gt;&gt;&gt; print(result)\n{'context_precision': 0.817,\n'faithfulness': 0.892,\n'answer_relevancy': 0.874}\n</code></pre></p> Source code in <code>src/ragas/evaluation.py</code> <pre><code>@track_was_completed\ndef evaluate(\n    dataset: t.Union[Dataset, EvaluationDataset],\n    metrics: list[Metric] | None = None,\n    llm: t.Optional[BaseRagasLLM | LangchainLLM] = None,\n    embeddings: t.Optional[BaseRagasEmbeddings | LangchainEmbeddings] = None,\n    callbacks: Callbacks = None,\n    in_ci: bool = False,\n    run_config: RunConfig = RunConfig(),\n    token_usage_parser: t.Optional[TokenUsageParser] = None,\n    raise_exceptions: bool = False,\n    column_map: t.Optional[t.Dict[str, str]] = None,\n    show_progress: bool = True,\n) -&gt; Result:\n    \"\"\"\n    Run the evaluation on the dataset with different metrics\n\n    Parameters\n    ----------\n    dataset : Dataset[question: list[str], contexts: list[list[str]], answer: list[str], ground_truth: list[list[str]]]\n        The dataset in the format of ragas which the metrics will use to score the RAG\n        pipeline with\n    metrics : list[Metric] , optional\n        List of metrics to use for evaluation. If not provided then ragas will run the\n        evaluation on the best set of metrics to give a complete view.\n    llm: BaseRagasLLM, optional\n        The language model to use for the metrics. If not provided then ragas will use\n        the default language model for metrics which require an LLM. This can we overridden by the llm specified in\n        the metric level with `metric.llm`.\n    embeddings: BaseRagasEmbeddings, optional\n        The embeddings to use for the metrics. If not provided then ragas will use\n        the default embeddings for metrics which require embeddings. This can we overridden by the embeddings specified in\n        the metric level with `metric.embeddings`.\n    callbacks: Callbacks, optional\n        Lifecycle Langchain Callbacks to run during evaluation. Check the\n        [langchain documentation](https://python.langchain.com/docs/modules/callbacks/)\n        for more information.\n    in_ci: bool\n        Whether the evaluation is running in CI or not. If set to True then some\n        metrics will be run to increase the reproducability of the evaluations. This\n        will increase the runtime and cost of evaluations. Default is False.\n    run_config: RunConfig, optional\n        Configuration for runtime settings like timeout and retries. If not provided,\n        default values are used.\n    token_usage_parser: TokenUsageParser, optional\n        Parser to get the token usage from the LLM result. If not provided then the\n        the cost and total tokens will not be calculated. Default is None.\n    raise_exceptions: False\n        Whether to raise exceptions or not. If set to True then the evaluation will\n        raise an exception if any of the metrics fail. If set to False then the\n        evaluation will return `np.nan` for the row that failed. Default is False.\n    column_map : dict[str, str], optional\n        The column names of the dataset to use for evaluation. If the column names of\n        the dataset are different from the default ones then you can provide the\n        mapping as a dictionary here. Example: If the dataset column name is contexts_v1,\n        column_map can be given as {\"contexts\":\"contexts_v1\"}\n    show_progress: bool, optional\n        Whether to show the progress bar during evaluation. If set to False, the progress bar will be disabled. Default is True.\n\n    Returns\n    -------\n    Result\n        Result object containing the scores of each metric. You can use this do analysis\n        later.\n\n    Raises\n    ------\n    ValueError\n        if validation fails because the columns required for the metrics are missing or\n        if the columns are of the wrong format.\n\n    Examples\n    --------\n    the basic usage is as follows:\n    ```\n    from ragas import evaluate\n\n    &gt;&gt;&gt; dataset\n    Dataset({\n        features: ['question', 'ground_truth', 'answer', 'contexts'],\n        num_rows: 30\n    })\n\n    &gt;&gt;&gt; result = evaluate(dataset)\n    &gt;&gt;&gt; print(result)\n    {'context_precision': 0.817,\n    'faithfulness': 0.892,\n    'answer_relevancy': 0.874}\n    ```\n    \"\"\"\n    column_map = column_map or {}\n    callbacks = callbacks or []\n\n    if helicone_config.is_enabled:\n        import uuid\n\n        helicone_config.session_name = \"ragas-evaluation\"\n        helicone_config.session_id = str(uuid.uuid4())\n\n    if dataset is None:\n        raise ValueError(\"Provide dataset!\")\n\n    # default metrics\n    if metrics is None:\n        from ragas.metrics import (\n            answer_relevancy,\n            context_precision,\n            context_recall,\n            faithfulness,\n        )\n\n        metrics = [answer_relevancy, context_precision, faithfulness, context_recall]\n\n    v1_input = False\n    if isinstance(dataset, Dataset):\n        # remap column names from the dataset\n        v1_input = True\n        dataset = remap_column_names(dataset, column_map)\n        dataset = convert_v1_to_v2_dataset(dataset)\n        # validation\n        dataset = EvaluationDataset.from_list(dataset.to_list())\n\n    if isinstance(dataset, EvaluationDataset):\n        validate_required_columns(dataset, metrics)\n        validate_supported_metrics(dataset, metrics)\n\n    # set the llm and embeddings\n    if isinstance(llm, LangchainLLM):\n        llm = LangchainLLMWrapper(llm, run_config=run_config)\n    if isinstance(embeddings, LangchainEmbeddings):\n        embeddings = LangchainEmbeddingsWrapper(embeddings)\n\n    # init llms and embeddings\n    binary_metrics = []\n    llm_changed: t.List[int] = []\n    embeddings_changed: t.List[int] = []\n    reproducable_metrics: t.List[int] = []\n    answer_correctness_is_set = -1\n\n    # loop through the metrics and perform initializations\n    for i, metric in enumerate(metrics):\n        # set llm and embeddings if not set\n        if isinstance(metric, AspectCritic):\n            binary_metrics.append(metric.name)\n        if isinstance(metric, MetricWithLLM) and metric.llm is None:\n            if llm is None:\n                llm = llm_factory()\n            metric.llm = llm\n            llm_changed.append(i)\n        if isinstance(metric, MetricWithEmbeddings) and metric.embeddings is None:\n            if embeddings is None:\n                embeddings = embedding_factory()\n            metric.embeddings = embeddings\n            embeddings_changed.append(i)\n        if isinstance(metric, AnswerCorrectness):\n            if metric.answer_similarity is None:\n                answer_correctness_is_set = i\n        # set reproducibility for metrics if in CI\n        if in_ci and is_reproducable(metric):\n            if metric.reproducibility == 1:  # type: ignore\n                # only set a value if not already set\n                metric.reproducibility = 3  # type: ignore\n                reproducable_metrics.append(i)\n\n        # init all the models\n        metric.init(run_config)\n\n    executor = Executor(\n        desc=\"Evaluating\",\n        keep_progress_bar=True,\n        raise_exceptions=raise_exceptions,\n        run_config=run_config,\n        show_progress=show_progress,\n    )\n\n    # Ragas Callbacks\n    # init the callbacks we need for various tasks\n    ragas_callbacks: t.Dict[str, BaseCallbackHandler] = {}\n\n    # check if cost needs to be calculated\n    if token_usage_parser is not None:\n        from ragas.cost import CostCallbackHandler\n\n        cost_cb = CostCallbackHandler(token_usage_parser=token_usage_parser)\n        ragas_callbacks[\"cost_cb\"] = cost_cb\n\n    # append all the ragas_callbacks to the callbacks\n    for cb in ragas_callbacks.values():\n        if isinstance(callbacks, BaseCallbackManager):\n            callbacks.add_handler(cb)\n        else:\n            callbacks.append(cb)\n\n    # new evaluation chain\n    row_run_managers = []\n    evaluation_rm, evaluation_group_cm = new_group(\n        name=RAGAS_EVALUATION_CHAIN_NAME, inputs={}, callbacks=callbacks\n    )\n\n    sample_type = dataset.get_sample_type()\n    for i, sample in enumerate(dataset):\n        row = t.cast(t.Dict[str, t.Any], sample.dict())\n        row_rm, row_group_cm = new_group(\n            name=f\"row {i}\",\n            inputs=row,\n            callbacks=evaluation_group_cm,\n        )\n        row_run_managers.append((row_rm, row_group_cm))\n        if sample_type == SingleTurnSample:\n            _ = [\n                executor.submit(\n                    metric.single_turn_ascore,\n                    sample,\n                    row_group_cm,\n                    name=f\"{metric.name}-{i}\",\n                    timeout=run_config.timeout,\n                )\n                for metric in metrics\n                if isinstance(metric, SingleTurnMetric)\n            ]\n        elif sample_type == MultiTurnSample:\n            _ = [\n                executor.submit(\n                    metric.multi_turn_ascore,\n                    sample,\n                    row_group_cm,\n                    name=f\"{metric.name}-{i}\",\n                    timeout=run_config.timeout,\n                )\n                for metric in metrics\n                if isinstance(metric, MultiTurnMetric)\n            ]\n        else:\n            raise ValueError(f\"Unsupported sample type {sample_type}\")\n\n    scores = []\n    try:\n        # get the results\n        results = executor.results()\n        if results == []:\n            raise ExceptionInRunner()\n\n        # convert results to dataset_like\n        for i, _ in enumerate(dataset):\n            s = {}\n            for j, m in enumerate(metrics):\n                s[m.name] = results[len(metrics) * i + j]\n            scores.append(s)\n            # close the row chain\n            row_rm, row_group_cm = row_run_managers[i]\n            if not row_group_cm.ended:\n                row_rm.on_chain_end(s)\n\n    # run evaluation task\n    except Exception as e:\n        if not evaluation_group_cm.ended:\n            evaluation_rm.on_chain_error(e)\n\n        raise e\n    else:\n        # evalution run was successful\n        # now lets process the results\n        # convert to v.1 dataset\n        dataset = dataset.to_hf_dataset()\n        if v1_input:\n            dataset = convert_v2_to_v1_dataset(dataset)\n\n        cost_cb = ragas_callbacks[\"cost_cb\"] if \"cost_cb\" in ragas_callbacks else None\n        result = Result(\n            scores=Dataset.from_list(scores),\n            dataset=dataset,\n            binary_columns=binary_metrics,\n            cost_cb=t.cast(\n                t.Union[\"CostCallbackHandler\", None],\n                cost_cb,\n            ),\n        )\n        if not evaluation_group_cm.ended:\n            evaluation_rm.on_chain_end(result)\n    finally:\n        # reset llms and embeddings if changed\n        for i in llm_changed:\n            t.cast(MetricWithLLM, metrics[i]).llm = None\n        for i in embeddings_changed:\n            t.cast(MetricWithEmbeddings, metrics[i]).embeddings = None\n        if answer_correctness_is_set != -1:\n            t.cast(\n                AnswerCorrectness, metrics[answer_correctness_is_set]\n            ).answer_similarity = None\n\n        for i in reproducable_metrics:\n            metrics[i].reproducibility = 1  # type: ignore\n\n    # log the evaluation event\n    metrics_names = [m.name for m in metrics]\n    metric_lang = [get_feature_language(m) for m in metrics]\n    metric_lang = np.unique([m for m in metric_lang if m is not None])\n    track(\n        EvaluationEvent(\n            event_type=\"evaluation\",\n            metrics=metrics_names,\n            evaluation_mode=\"\",\n            num_rows=len(dataset),\n            language=metric_lang[0] if len(metric_lang) &gt; 0 else \"\",\n            in_ci=in_ci,\n        )\n    )\n    return result\n</code></pre>"},{"location":"references/exceptions/","title":"exceptions","text":""},{"location":"references/exceptions/#ragas.exceptions.ExceptionInRunner","title":"<code>ExceptionInRunner</code>","text":"<p>               Bases: <code>RagasException</code></p> <p>Exception raised when an exception is raised in the executor.</p> Source code in <code>src/ragas/exceptions.py</code> <pre><code>class ExceptionInRunner(RagasException):\n    \"\"\"\n    Exception raised when an exception is raised in the executor.\n    \"\"\"\n\n    def __init__(self):\n        msg = \"The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead.\"\n        super().__init__(msg)\n</code></pre>"},{"location":"references/exceptions/#ragas.exceptions.RagasException","title":"<code>RagasException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception class for ragas.</p> Source code in <code>src/ragas/exceptions.py</code> <pre><code>class RagasException(Exception):\n    \"\"\"\n    Base exception class for ragas.\n    \"\"\"\n\n    def __init__(self, message: str):\n        self.message = message\n        super().__init__(message)\n</code></pre>"},{"location":"references/exceptions/#ragas.exceptions.RagasOutputParserException","title":"<code>RagasOutputParserException</code>","text":"<p>               Bases: <code>RagasException</code></p> <p>Exception raised when the output parser fails to parse the output.</p> Source code in <code>src/ragas/exceptions.py</code> <pre><code>class RagasOutputParserException(RagasException):\n    \"\"\"\n    Exception raised when the output parser fails to parse the output.\n    \"\"\"\n\n    def __init__(self, num_retries: int):\n        msg = (\n            f\"The output parser failed to parse the output after {num_retries} retries.\"\n        )\n        super().__init__(msg)\n</code></pre>"},{"location":"references/executor/","title":"executor","text":""},{"location":"references/executor/#ragas.executor.run_async_batch","title":"<code>run_async_batch(desc, func, kwargs_list)</code>","text":"<p>run the same async function with different arguments</p> Source code in <code>src/ragas/executor.py</code> <pre><code>def run_async_batch(desc: str, func: t.Callable, kwargs_list: t.List[t.Dict]):\n    \"\"\"\n    run the same async function with different arguments\n    \"\"\"\n    run_config = RunConfig()\n    executor = Executor(\n        desc=desc,\n        keep_progress_bar=False,\n        raise_exceptions=True,\n        run_config=run_config,\n    )\n\n    for kwargs in kwargs_list:\n        executor.submit(func, **kwargs)\n\n    return executor.results()\n</code></pre>"},{"location":"references/messages/","title":"messages","text":""},{"location":"references/run_config/","title":"run_config","text":""},{"location":"references/run_config/#ragas.run_config.RunConfig","title":"<code>RunConfig</code>  <code>dataclass</code>","text":"<p>Configuration for a timeouts, retries and seed for Ragas operations.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>int</code> <p>Maximum time (in seconds) to wait for a single operation, by default 60.</p> <code>180</code> <code>max_retries</code> <code>int</code> <p>Maximum number of retry attempts, by default 10.</p> <code>10</code> <code>max_wait</code> <code>int</code> <p>Maximum wait time (in seconds) between retries, by default 60.</p> <code>60</code> <code>max_workers</code> <code>int</code> <p>Maximum number of concurrent workers, by default 16.</p> <code>16</code> <code>exception_types</code> <code>Union[Type[BaseException], Tuple[Type[BaseException], ...]]</code> <p>Exception types to catch and retry on, by default (Exception,).</p> <code>(Exception)</code> <code>log_tenacity</code> <code>bool</code> <p>Whether to log retry attempts using tenacity, by default False.</p> <code>False</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility, by default 42.</p> <code>42</code> <p>Attributes:</p> Name Type Description <code>rng</code> <code>Generator</code> <p>Random number generator initialized with the specified seed.</p> Notes <p>The <code>__post_init__</code> method initializes the <code>rng</code> attribute as a numpy random number generator using the specified seed.</p> Source code in <code>src/ragas/run_config.py</code> <pre><code>@dataclass\nclass RunConfig:\n    \"\"\"\n    Configuration for a timeouts, retries and seed for Ragas operations.\n\n    Parameters\n    ----------\n    timeout : int, optional\n        Maximum time (in seconds) to wait for a single operation, by default 60.\n    max_retries : int, optional\n        Maximum number of retry attempts, by default 10.\n    max_wait : int, optional\n        Maximum wait time (in seconds) between retries, by default 60.\n    max_workers : int, optional\n        Maximum number of concurrent workers, by default 16.\n    exception_types : Union[Type[BaseException], Tuple[Type[BaseException], ...]], optional\n        Exception types to catch and retry on, by default (Exception,).\n    log_tenacity : bool, optional\n        Whether to log retry attempts using tenacity, by default False.\n    seed : int, optional\n        Random seed for reproducibility, by default 42.\n\n    Attributes\n    ----------\n    rng : numpy.random.Generator\n        Random number generator initialized with the specified seed.\n\n    Notes\n    -----\n    The `__post_init__` method initializes the `rng` attribute as a numpy random\n    number generator using the specified seed.\n    \"\"\"\n\n    timeout: int = 180\n    max_retries: int = 10\n    max_wait: int = 60\n    max_workers: int = 16\n    exception_types: t.Union[\n        t.Type[BaseException],\n        t.Tuple[t.Type[BaseException], ...],\n    ] = (Exception,)\n    log_tenacity: bool = False\n    seed: int = 42\n\n    def __post_init__(self):\n        self.rng = np.random.default_rng(seed=self.seed)\n</code></pre>"},{"location":"references/run_config/#ragas.run_config.add_async_retry","title":"<code>add_async_retry(fn, run_config)</code>","text":"<p>Decorator for retrying a function if it fails.</p> Source code in <code>src/ragas/run_config.py</code> <pre><code>def add_async_retry(fn: WrappedFn, run_config: RunConfig) -&gt; WrappedFn:\n    \"\"\"\n    Decorator for retrying a function if it fails.\n    \"\"\"\n    # configure tenacity's after section wtih logger\n    if run_config.log_tenacity is not None:\n        logger = logging.getLogger(f\"TENACITYRetry[{fn.__name__}]\")\n        tenacity_logger = after_log(logger, logging.DEBUG)\n    else:\n        tenacity_logger = after_nothing\n\n    r = AsyncRetrying(\n        wait=wait_random_exponential(multiplier=1, max=run_config.max_wait),\n        stop=stop_after_attempt(run_config.max_retries),\n        retry=retry_if_exception_type(run_config.exception_types),\n        reraise=True,\n        after=tenacity_logger,\n    )\n    return r.wraps(fn)\n</code></pre>"},{"location":"references/utils/","title":"utils","text":""},{"location":"references/utils/#ragas.utils.camel_to_snake","title":"<code>camel_to_snake(name)</code>","text":"<p>Convert a camelCase string to snake_case. eg: HaiThere -&gt; hai_there</p> Source code in <code>src/ragas/utils.py</code> <pre><code>def camel_to_snake(name):\n    \"\"\"\n    Convert a camelCase string to snake_case.\n    eg: HaiThere -&gt; hai_there\n    \"\"\"\n    pattern = re.compile(r\"(?&lt;!^)(?=[A-Z])\")\n    return pattern.sub(\"_\", name).lower()\n</code></pre>"},{"location":"references/utils/#ragas.utils.deprecated","title":"<code>deprecated(since, *, removal=None, alternative=None, addendum=None, pending=False)</code>","text":"<p>Decorator to mark functions or classes as deprecated.</p> <p>Args:     since: str          The release at which this API became deprecated.     removal: str, optional         The expected removal version. Cannot be used with pending=True.         Must be specified with pending=False.     alternative: str, optional         The alternative API or function to be used instead         of the deprecated function.     addendum: str, optional         Additional text appended directly to the final message.     pending: bool         Whether the deprecation version is already scheduled or not.         Cannot be used with removal.</p> <p>Examples:</p> <pre><code>.. code-block:: python\n\n    @deprecated(\"0.1\", removal=\"0.2\", alternative=\"some_new_function\")\n    def some_old_function():\n        print(\"This is an old function.\")\n</code></pre> Source code in <code>src/ragas/utils.py</code> <pre><code>def deprecated(\n    since: str,\n    *,\n    removal: t.Optional[str] = None,\n    alternative: t.Optional[str] = None,\n    addendum: t.Optional[str] = None,\n    pending: bool = False,\n):\n    \"\"\"\n    Decorator to mark functions or classes as deprecated.\n\n    Args:\n        since: str\n             The release at which this API became deprecated.\n        removal: str, optional\n            The expected removal version. Cannot be used with pending=True.\n            Must be specified with pending=False.\n        alternative: str, optional\n            The alternative API or function to be used instead\n            of the deprecated function.\n        addendum: str, optional\n            Additional text appended directly to the final message.\n        pending: bool\n            Whether the deprecation version is already scheduled or not.\n            Cannot be used with removal.\n\n\n    Examples\n    --------\n\n        .. code-block:: python\n\n            @deprecated(\"0.1\", removal=\"0.2\", alternative=\"some_new_function\")\n            def some_old_function():\n                print(\"This is an old function.\")\n\n    \"\"\"\n\n    def deprecate(func: t.Callable):\n        def emit_warning(*args, **kwargs):\n            if pending and removal:\n                raise ValueError(\n                    \"A pending deprecation cannot have a scheduled removal\"\n                )\n\n            message = f\"The function {func.__name__} was deprecated in {since},\"\n\n            if not pending:\n                if removal:\n                    message += f\" and will be removed in the {removal} release.\"\n                else:\n                    raise ValueError(\n                        \"A non-pending deprecation must have a scheduled removal.\"\n                    )\n            else:\n                message += \" and will be removed in a future release.\"\n\n            if alternative:\n                message += f\" Use {alternative} instead.\"\n\n            if addendum:\n                message += f\" {addendum}\"\n\n            warnings.warn(message, stacklevel=2, category=DeprecationWarning)\n            return func(*args, **kwargs)\n\n        return emit_warning\n\n    return deprecate\n</code></pre>"},{"location":"references/utils/#ragas.utils.get_cache_dir","title":"<code>get_cache_dir()</code>  <code>cached</code>","text":"<p>get cache location</p> Source code in <code>src/ragas/utils.py</code> <pre><code>@lru_cache(maxsize=1)\ndef get_cache_dir() -&gt; str:\n    \"get cache location\"\n    DEFAULT_XDG_CACHE_HOME = \"~/.cache\"\n    xdg_cache = os.getenv(\"XDG_CACHE_HOME\", DEFAULT_XDG_CACHE_HOME)\n    default_ragas_cache = os.path.join(xdg_cache, \"ragas\")\n    return os.path.expanduser(os.getenv(\"RAGAS_CACHE_HOME\", default_ragas_cache))\n</code></pre>"},{"location":"references/validation/","title":"validation","text":""},{"location":"references/validation/#ragas.validation.get_supported_metric_type","title":"<code>get_supported_metric_type(ds)</code>","text":"<p>get the supported metric type for the given dataset</p> Source code in <code>src/ragas/validation.py</code> <pre><code>def get_supported_metric_type(ds: EvaluationDataset):\n    \"\"\"\n    get the supported metric type for the given dataset\n    \"\"\"\n\n    sample_type = ds.get_sample_type()\n    if sample_type == SingleTurnSample:\n        return MetricType.SINGLE_TURN.name\n    elif sample_type == MultiTurnSample:\n        return MetricType.MULTI_TURN.name\n    else:\n        raise ValueError(f\"Unsupported sample type {sample_type}\")\n</code></pre>"},{"location":"references/validation/#ragas.validation.remap_column_names","title":"<code>remap_column_names(dataset, column_map)</code>","text":"<p>Remap the column names in case dataset uses different column names</p> Source code in <code>src/ragas/validation.py</code> <pre><code>def remap_column_names(dataset: Dataset, column_map: dict[str, str]) -&gt; Dataset:\n    \"\"\"\n    Remap the column names in case dataset uses different column names\n    \"\"\"\n\n    inverse_column_map = {v: k for k, v in column_map.items()}\n    return dataset.rename_columns(inverse_column_map)\n</code></pre>"},{"location":"references/embeddings/__init__/","title":"embeddings","text":""},{"location":"references/embeddings/__init__/#ragas.embeddings.HuggingfaceEmbeddings","title":"<code>HuggingfaceEmbeddings</code>","text":"<p>               Bases: <code>BaseRagasEmbeddings</code></p> Source code in <code>src/ragas/embeddings/base.py</code> <pre><code>@dataclass\nclass HuggingfaceEmbeddings(BaseRagasEmbeddings):\n    model_name: str = DEFAULT_MODEL_NAME\n    \"\"\"Model name to use.\"\"\"\n    cache_folder: t.Optional[str] = None\n    \"\"\"Path to store models. \n    Can be also set by SENTENCE_TRANSFORMERS_HOME environment variable.\"\"\"\n    model_kwargs: t.Dict[str, t.Any] = field(default_factory=dict)\n    \"\"\"Keyword arguments to pass to the model.\"\"\"\n    encode_kwargs: t.Dict[str, t.Any] = field(default_factory=dict)\n\n    def __post_init__(self):\n        try:\n            import sentence_transformers\n            from transformers import AutoConfig\n            from transformers.models.auto.modeling_auto import (\n                MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES,\n            )\n        except ImportError as exc:\n            raise ImportError(\n                \"Could not import sentence_transformers python package. \"\n                \"Please install it with `pip install sentence-transformers`.\"\n            ) from exc\n        config = AutoConfig.from_pretrained(self.model_name)\n        self.is_cross_encoder = bool(\n            np.intersect1d(\n                list(MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES.values()),\n                config.architectures,\n            )\n        )\n\n        if self.is_cross_encoder:\n            self.model = sentence_transformers.CrossEncoder(\n                self.model_name, **self.model_kwargs\n            )\n        else:\n            self.model = sentence_transformers.SentenceTransformer(\n                self.model_name, cache_folder=self.cache_folder, **self.model_kwargs\n            )\n\n        # ensure outputs are tensors\n        if \"convert_to_tensor\" not in self.encode_kwargs:\n            self.encode_kwargs[\"convert_to_tensor\"] = True\n\n    def embed_query(self, text: str) -&gt; List[float]:\n        return self.embed_documents([text])[0]\n\n    def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:\n        from sentence_transformers.SentenceTransformer import SentenceTransformer\n        from torch import Tensor\n\n        assert isinstance(\n            self.model, SentenceTransformer\n        ), \"Model is not of the type Bi-encoder\"\n        embeddings = self.model.encode(\n            texts, normalize_embeddings=True, **self.encode_kwargs\n        )\n\n        assert isinstance(embeddings, Tensor)\n        return embeddings.tolist()\n\n    def predict(self, texts: List[List[str]]) -&gt; List[List[float]]:\n        from sentence_transformers.cross_encoder import CrossEncoder\n        from torch import Tensor\n\n        assert isinstance(\n            self.model, CrossEncoder\n        ), \"Model is not of the type CrossEncoder\"\n\n        predictions = self.model.predict(texts, **self.encode_kwargs)\n\n        assert isinstance(predictions, Tensor)\n        return predictions.tolist()\n</code></pre>"},{"location":"references/embeddings/__init__/#ragas.embeddings.HuggingfaceEmbeddings.cache_folder","title":"<code>cache_folder: t.Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Path to store models.  Can be also set by SENTENCE_TRANSFORMERS_HOME environment variable.</p>"},{"location":"references/embeddings/__init__/#ragas.embeddings.HuggingfaceEmbeddings.model_kwargs","title":"<code>model_kwargs: t.Dict[str, t.Any] = field(default_factory=dict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Keyword arguments to pass to the model.</p>"},{"location":"references/embeddings/__init__/#ragas.embeddings.HuggingfaceEmbeddings.model_name","title":"<code>model_name: str = DEFAULT_MODEL_NAME</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Model name to use.</p>"},{"location":"references/embeddings/base/","title":"base","text":""},{"location":"references/embeddings/base/#ragas.embeddings.base.HuggingfaceEmbeddings","title":"<code>HuggingfaceEmbeddings</code>","text":"<p>               Bases: <code>BaseRagasEmbeddings</code></p> Source code in <code>src/ragas/embeddings/base.py</code> <pre><code>@dataclass\nclass HuggingfaceEmbeddings(BaseRagasEmbeddings):\n    model_name: str = DEFAULT_MODEL_NAME\n    \"\"\"Model name to use.\"\"\"\n    cache_folder: t.Optional[str] = None\n    \"\"\"Path to store models. \n    Can be also set by SENTENCE_TRANSFORMERS_HOME environment variable.\"\"\"\n    model_kwargs: t.Dict[str, t.Any] = field(default_factory=dict)\n    \"\"\"Keyword arguments to pass to the model.\"\"\"\n    encode_kwargs: t.Dict[str, t.Any] = field(default_factory=dict)\n\n    def __post_init__(self):\n        try:\n            import sentence_transformers\n            from transformers import AutoConfig\n            from transformers.models.auto.modeling_auto import (\n                MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES,\n            )\n        except ImportError as exc:\n            raise ImportError(\n                \"Could not import sentence_transformers python package. \"\n                \"Please install it with `pip install sentence-transformers`.\"\n            ) from exc\n        config = AutoConfig.from_pretrained(self.model_name)\n        self.is_cross_encoder = bool(\n            np.intersect1d(\n                list(MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES.values()),\n                config.architectures,\n            )\n        )\n\n        if self.is_cross_encoder:\n            self.model = sentence_transformers.CrossEncoder(\n                self.model_name, **self.model_kwargs\n            )\n        else:\n            self.model = sentence_transformers.SentenceTransformer(\n                self.model_name, cache_folder=self.cache_folder, **self.model_kwargs\n            )\n\n        # ensure outputs are tensors\n        if \"convert_to_tensor\" not in self.encode_kwargs:\n            self.encode_kwargs[\"convert_to_tensor\"] = True\n\n    def embed_query(self, text: str) -&gt; List[float]:\n        return self.embed_documents([text])[0]\n\n    def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:\n        from sentence_transformers.SentenceTransformer import SentenceTransformer\n        from torch import Tensor\n\n        assert isinstance(\n            self.model, SentenceTransformer\n        ), \"Model is not of the type Bi-encoder\"\n        embeddings = self.model.encode(\n            texts, normalize_embeddings=True, **self.encode_kwargs\n        )\n\n        assert isinstance(embeddings, Tensor)\n        return embeddings.tolist()\n\n    def predict(self, texts: List[List[str]]) -&gt; List[List[float]]:\n        from sentence_transformers.cross_encoder import CrossEncoder\n        from torch import Tensor\n\n        assert isinstance(\n            self.model, CrossEncoder\n        ), \"Model is not of the type CrossEncoder\"\n\n        predictions = self.model.predict(texts, **self.encode_kwargs)\n\n        assert isinstance(predictions, Tensor)\n        return predictions.tolist()\n</code></pre>"},{"location":"references/embeddings/base/#ragas.embeddings.base.HuggingfaceEmbeddings.cache_folder","title":"<code>cache_folder: t.Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Path to store models.  Can be also set by SENTENCE_TRANSFORMERS_HOME environment variable.</p>"},{"location":"references/embeddings/base/#ragas.embeddings.base.HuggingfaceEmbeddings.model_kwargs","title":"<code>model_kwargs: t.Dict[str, t.Any] = field(default_factory=dict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Keyword arguments to pass to the model.</p>"},{"location":"references/embeddings/base/#ragas.embeddings.base.HuggingfaceEmbeddings.model_name","title":"<code>model_name: str = DEFAULT_MODEL_NAME</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Model name to use.</p>"},{"location":"references/experimental/__init__/","title":"experimental","text":""},{"location":"references/experimental/metrics/__init__/","title":"metrics","text":""},{"location":"references/integrations/__init__/","title":"integrations","text":""},{"location":"references/integrations/helicone/","title":"helicone","text":""},{"location":"references/integrations/langchain/","title":"langchain","text":""},{"location":"references/integrations/langchain/#ragas.integrations.langchain.EvaluatorChain","title":"<code>EvaluatorChain</code>","text":"<p>               Bases: <code>Chain</code>, <code>RunEvaluator</code></p> <p>Wrapper around ragas Metrics to use them with langsmith.</p> Source code in <code>src/ragas/integrations/langchain.py</code> <pre><code>class EvaluatorChain(Chain, RunEvaluator):\n    \"\"\"\n    Wrapper around ragas Metrics to use them with langsmith.\n    \"\"\"\n\n    metric: Metric\n\n    def __init__(self, metric: Metric, **kwargs: t.Any):\n        kwargs[\"metric\"] = metric\n        super().__init__(**kwargs)\n        if \"run_config\" in kwargs:\n            run_config = kwargs[\"run_config\"]\n        else:\n            run_config = RunConfig()\n        if isinstance(self.metric, MetricWithLLM):\n            llm = get_or_init(kwargs, \"llm\", ChatOpenAI)\n            t.cast(MetricWithLLM, self.metric).llm = LangchainLLMWrapper(llm)\n        if isinstance(self.metric, MetricWithEmbeddings):\n            embeddings = get_or_init(kwargs, \"embeddings\", OpenAIEmbeddings)\n            t.cast(MetricWithEmbeddings, self.metric).embeddings = (\n                LangchainEmbeddingsWrapper(embeddings)\n            )\n        self.metric.init(run_config)\n\n        assert isinstance(\n            self.metric, SingleTurnMetric\n        ), \"Metric must be SingleTurnMetric\"\n\n    @property\n    def input_keys(self) -&gt; list[str]:\n        return get_required_columns_v1(self.metric)\n\n    @property\n    def output_keys(self) -&gt; list[str]:\n        return [self.metric.name]\n\n    def _call(\n        self,\n        inputs: t.Union[dict[str, t.Any], SingleTurnSample],\n        run_manager: t.Optional[CallbackManagerForChainRun] = None,\n    ) -&gt; dict[str, t.Any]:\n        \"\"\"\n        Call the evaluation chain.\n        \"\"\"\n        if isinstance(inputs, dict):\n            inputs = convert_row_v1_to_v2(inputs)\n            if \"retrieved_contexts\" in inputs:\n                inputs[\"retrieved_contexts\"] = [\n                    doc.page_content\n                    for doc in inputs[\"retrieved_contexts\"]\n                    if isinstance(doc, LCDocument)\n                ]\n            inputs = SingleTurnSample(**inputs)\n\n        self._validate(inputs)\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n        callbacks = _run_manager.get_child()\n\n        assert isinstance(\n            self.metric, SingleTurnMetric\n        ), \"Metric must be SingleTurnMetric\"\n        score = self.metric.single_turn_score(\n            inputs,\n            callbacks=callbacks,\n        )\n        return {self.metric.name: score}\n\n    async def _acall(\n        self,\n        inputs: t.Union[t.Dict[str, t.Any], SingleTurnSample],\n        run_manager: t.Optional[AsyncCallbackManagerForChainRun] = None,\n    ) -&gt; t.Dict[str, t.Any]:\n        \"\"\"\n        Call the evaluation chain.\n        \"\"\"\n\n        if isinstance(inputs, dict):\n            inputs = convert_row_v1_to_v2(inputs)\n            if \"retrieved_contexts\" in inputs:\n                inputs[\"retrieved_contexts\"] = [\n                    doc.page_content\n                    for doc in inputs[\"retrieved_contexts\"]\n                    if isinstance(doc, LCDocument)\n                ]\n            inputs = SingleTurnSample(**inputs)\n\n        self._validate(inputs)\n        _run_manager = run_manager or AsyncCallbackManagerForChainRun.get_noop_manager()\n        # TODO: currently AsyncCallbacks are not supported in ragas\n        _run_manager.get_child()\n        assert isinstance(\n            self.metric, SingleTurnMetric\n        ), \"Metric must be SingleTurnMetric\"\n        score = await self.metric.single_turn_ascore(\n            inputs,\n            callbacks=[],\n        )\n        return {self.metric.name: score}\n\n    def _validate(self, input: SingleTurnSample) -&gt; None:\n        # validate each example\n        required_columns = self.metric.required_columns.get(\"SINGLE_TURN\", [])\n        for col in required_columns:\n            if col not in input.get_features():\n                raise ValueError(\n                    f'\"{col}\" is required in each example'\n                    f\"for the metric[{self.metric.name}] you have chosen.\"\n                )\n\n    @staticmethod\n    def _keys_are_present(keys_to_check: list, dict_to_check: dict) -&gt; list[str]:\n        return [k for k in keys_to_check if k not in dict_to_check]\n\n    def _validate_langsmith_eval(self, run: Run, example: t.Optional[Example]) -&gt; None:\n        if example is None:\n            raise ValueError(\n                \"expected example to be provided. Please check langsmith dataset and ensure valid dataset is uploaded.\"\n            )\n        if example.inputs is None:\n            raise ValueError(\n                \"expected example.inputs to be provided. Please check langsmith dataset and ensure valid dataset is uploaded.\"\n            )\n        if example.outputs is None:\n            raise ValueError(\n                \"expected example.inputs to be provided. Please check langsmith dataset and ensure valid dataset is uploaded.\"\n            )\n        if \"question\" not in example.inputs or \"ground_truth\" not in example.outputs:\n            raise ValueError(\n                \"Expected 'question' and 'ground_truth' in example.\"\n                f\"Got: {[k for k in example.inputs.keys()]}\"\n            )\n        assert (\n            run.outputs is not None\n        ), \"the current run has no outputs. The chain should output 'answer' and 'contexts' keys.\"\n        output_keys = get_required_columns_v1(self.metric)\n        output_keys = [\n            key for key in output_keys if key not in [\"question\", \"ground_truth\"]\n        ]\n        missing_keys = self._keys_are_present(output_keys, run.outputs)\n        if missing_keys:\n            raise ValueError(\n                \"Expected 'answer' and 'contexts' in run.outputs.\"\n                f\"Got: {[k for k in run.outputs.keys()]}\"\n            )\n\n    def evaluate_run(\n        self, run: Run, example: t.Optional[Example] = None\n    ) -&gt; EvaluationResult:\n        \"\"\"\n        Evaluate a langsmith run\n        \"\"\"\n        self._validate_langsmith_eval(run, example)\n\n        # this is just to suppress the type checker error\n        # actual check and error message is in the _validate_langsmith_eval\n        assert run.outputs is not None\n        assert example is not None\n        assert example.inputs is not None\n        assert example.outputs is not None\n\n        chain_eval = run.outputs\n        chain_eval[\"question\"] = example.inputs[\"question\"]\n        if \"ground_truth\" in get_required_columns_v1(self.metric):\n            if example.outputs is None or \"ground_truth\" not in example.outputs:\n                raise ValueError(\"expected `ground_truth` in example outputs.\")\n            chain_eval[\"ground_truth\"] = example.outputs[\"ground_truth\"]\n        eval_output = self.invoke(chain_eval, include_run_info=True)\n\n        evaluation_result = EvaluationResult(\n            key=self.metric.name, score=eval_output[self.metric.name]\n        )\n        if RUN_KEY in eval_output:\n            evaluation_result.evaluator_info[RUN_KEY] = eval_output[RUN_KEY]\n        return evaluation_result\n</code></pre>"},{"location":"references/integrations/langchain/#ragas.integrations.langchain.EvaluatorChain.evaluate_run","title":"<code>evaluate_run(run, example=None)</code>","text":"<p>Evaluate a langsmith run</p> Source code in <code>src/ragas/integrations/langchain.py</code> <pre><code>def evaluate_run(\n    self, run: Run, example: t.Optional[Example] = None\n) -&gt; EvaluationResult:\n    \"\"\"\n    Evaluate a langsmith run\n    \"\"\"\n    self._validate_langsmith_eval(run, example)\n\n    # this is just to suppress the type checker error\n    # actual check and error message is in the _validate_langsmith_eval\n    assert run.outputs is not None\n    assert example is not None\n    assert example.inputs is not None\n    assert example.outputs is not None\n\n    chain_eval = run.outputs\n    chain_eval[\"question\"] = example.inputs[\"question\"]\n    if \"ground_truth\" in get_required_columns_v1(self.metric):\n        if example.outputs is None or \"ground_truth\" not in example.outputs:\n            raise ValueError(\"expected `ground_truth` in example outputs.\")\n        chain_eval[\"ground_truth\"] = example.outputs[\"ground_truth\"]\n    eval_output = self.invoke(chain_eval, include_run_info=True)\n\n    evaluation_result = EvaluationResult(\n        key=self.metric.name, score=eval_output[self.metric.name]\n    )\n    if RUN_KEY in eval_output:\n        evaluation_result.evaluator_info[RUN_KEY] = eval_output[RUN_KEY]\n    return evaluation_result\n</code></pre>"},{"location":"references/integrations/langsmith/","title":"langsmith","text":""},{"location":"references/integrations/langsmith/#ragas.integrations.langsmith.evaluate","title":"<code>evaluate(dataset_name, llm_or_chain_factory, experiment_name=None, metrics=None, verbose=False)</code>","text":"<p>Evaluates a language model or a chain factory on a specified dataset using LangSmith, with the option to customize metrics and verbosity.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>The name of the dataset to use for evaluation. This dataset must exist in LangSmith.</p> required <code>llm_or_chain_factory</code> <code>Any</code> <p>The language model or chain factory to be evaluated. This parameter is flexible and can accept a variety of objects depending on the implementation.</p> required <code>experiment_name</code> <code>Optional[str]</code> <p>The name of the experiment. This can be used to categorize or identify the evaluation run within LangSmith. The default is None.</p> <code>None</code> <code>metrics</code> <code>Optional[list]</code> <p>A list of custom metrics (functions or evaluators) to be used for the evaluation. If None, a default set of metrics (answer relevancy, context precision, context recall, and faithfulness) are used. The default is None.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, detailed progress and results will be printed during the evaluation process. The default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary containing the results of the evaluation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified dataset does not exist in LangSmith.</p> See Also <p>Client.read_dataset : Method to read an existing dataset. Client.run_on_dataset : Method to run the evaluation on the specified dataset.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; results = evaluate(\n...     dataset_name=\"MyDataset\",\n...     llm_or_chain_factory=my_llm,\n...     experiment_name=\"experiment_1_with_vanila_rag\",\n...     verbose=True\n... )\n&gt;&gt;&gt; print(results)\n{'evaluation_result': ...}\n</code></pre> Notes <p>The function initializes a client to interact with LangSmith, validates the existence of the specified dataset, prepares evaluation metrics, and runs the evaluation, returning the results. Custom evaluation metrics can be specified, or a default set will be used if none are provided.</p> Source code in <code>src/ragas/integrations/langsmith.py</code> <pre><code>def evaluate(\n    dataset_name: str,\n    llm_or_chain_factory: t.Any,\n    experiment_name: t.Optional[str] = None,\n    metrics: t.Optional[list] = None,\n    verbose: bool = False,\n) -&gt; t.Dict[str, t.Any]:\n    \"\"\"\n    Evaluates a language model or a chain factory on a specified dataset using\n    LangSmith, with the option to customize metrics and verbosity.\n\n    Parameters\n    ----------\n    dataset_name : str\n        The name of the dataset to use for evaluation. This dataset must exist in\n        LangSmith.\n    llm_or_chain_factory : Any\n        The language model or chain factory to be evaluated. This parameter is\n        flexible and can accept a variety of objects depending on the implementation.\n    experiment_name : Optional[str], optional\n        The name of the experiment. This can be used to categorize or identify the\n        evaluation run within LangSmith. The default is None.\n    metrics : Optional[list], optional\n        A list of custom metrics (functions or evaluators) to be used for the\n        evaluation. If None, a default set of metrics (answer relevancy, context\n        precision, context recall, and faithfulness) are used.\n        The default is None.\n    verbose : bool, optional\n        If True, detailed progress and results will be printed during the evaluation\n        process.\n        The default is False.\n\n    Returns\n    -------\n    Dict[str, Any]\n        A dictionary containing the results of the evaluation.\n\n    Raises\n    ------\n    ValueError\n        If the specified dataset does not exist in LangSmith.\n\n    See Also\n    --------\n    Client.read_dataset : Method to read an existing dataset.\n    Client.run_on_dataset : Method to run the evaluation on the specified dataset.\n\n    Examples\n    --------\n    &gt;&gt;&gt; results = evaluate(\n    ...     dataset_name=\"MyDataset\",\n    ...     llm_or_chain_factory=my_llm,\n    ...     experiment_name=\"experiment_1_with_vanila_rag\",\n    ...     verbose=True\n    ... )\n    &gt;&gt;&gt; print(results)\n    {'evaluation_result': ...}\n\n    Notes\n    -----\n    The function initializes a client to interact with LangSmith, validates the existence\n    of the specified dataset, prepares evaluation metrics, and runs the evaluation,\n    returning the results. Custom evaluation metrics can be specified, or a default set\n    will be used if none are provided.\n    \"\"\"\n    # init client and validate dataset\n    client = Client()\n    try:\n        _ = client.read_dataset(dataset_name=dataset_name)\n    except LangSmithNotFoundError:\n        raise ValueError(\n            f\"Dataset {dataset_name} not found in langsmith, make sure it exists in langsmith\"\n        )\n\n    # make config\n    if metrics is None:\n        from ragas.metrics import (\n            answer_relevancy,\n            context_precision,\n            context_recall,\n            faithfulness,\n        )\n\n        metrics = [answer_relevancy, context_precision, faithfulness, context_recall]\n\n    metrics = [EvaluatorChain(m) for m in metrics]\n    eval_config = RunEvalConfig(\n        custom_evaluators=metrics,\n    )\n\n    # run evaluation with langsmith\n    run = client.run_on_dataset(\n        dataset_name=dataset_name,\n        llm_or_chain_factory=llm_or_chain_factory,\n        evaluation=eval_config,\n        verbose=verbose,\n        # Any experiment metadata can be specified here\n        project_name=experiment_name,\n    )\n\n    return run\n</code></pre>"},{"location":"references/integrations/langsmith/#ragas.integrations.langsmith.upload_dataset","title":"<code>upload_dataset(dataset, dataset_name, dataset_desc='')</code>","text":"<p>Uploads a new dataset to LangSmith, converting it from a TestDataset object to a pandas DataFrame before upload. If a dataset with the specified name already exists, the function raises an error.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>TestDataset</code> <p>The dataset to be uploaded.</p> required <code>dataset_name</code> <code>str</code> <p>The name for the new dataset in LangSmith.</p> required <code>dataset_desc</code> <code>str</code> <p>A description for the new dataset. The default is an empty string.</p> <code>''</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>The dataset object as stored in LangSmith after upload.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a dataset with the specified name already exists in LangSmith.</p> Notes <p>The function attempts to read a dataset by the given name to check its existence. If not found, it proceeds to upload the dataset after converting it to a pandas DataFrame. This involves specifying input and output keys for the dataset being uploaded.</p> Source code in <code>src/ragas/integrations/langsmith.py</code> <pre><code>def upload_dataset(\n    dataset: Testset, dataset_name: str, dataset_desc: str = \"\"\n) -&gt; LangsmithDataset:\n    \"\"\"\n    Uploads a new dataset to LangSmith, converting it from a TestDataset object to a\n    pandas DataFrame before upload. If a dataset with the specified name already\n    exists, the function raises an error.\n\n    Parameters\n    ----------\n    dataset : TestDataset\n        The dataset to be uploaded.\n    dataset_name : str\n        The name for the new dataset in LangSmith.\n    dataset_desc : str, optional\n        A description for the new dataset. The default is an empty string.\n\n    Returns\n    -------\n    LangsmithDataset\n        The dataset object as stored in LangSmith after upload.\n\n    Raises\n    ------\n    ValueError\n        If a dataset with the specified name already exists in LangSmith.\n\n    Notes\n    -----\n    The function attempts to read a dataset by the given name to check its existence.\n    If not found, it proceeds to upload the dataset after converting it to a pandas\n    DataFrame. This involves specifying input and output keys for the dataset being\n    uploaded.\n    \"\"\"\n    client = Client()\n    try:\n        # check if dataset exists\n        langsmith_dataset: LangsmithDataset = client.read_dataset(\n            dataset_name=dataset_name\n        )\n        raise ValueError(\n            f\"Dataset {dataset_name} already exists in langsmith. [{langsmith_dataset}]\"\n        )\n    except LangSmithNotFoundError:\n        # if not create a new one with the generated query examples\n        langsmith_dataset: LangsmithDataset = client.upload_dataframe(\n            df=dataset.to_pandas(),\n            name=dataset_name,\n            input_keys=[\"question\"],\n            output_keys=[\"ground_truth\"],\n            description=dataset_desc,\n        )\n\n        print(\n            f\"Created a new dataset '{langsmith_dataset.name}'. Dataset is accessible at {langsmith_dataset.url}\"\n        )\n        return langsmith_dataset\n</code></pre>"},{"location":"references/integrations/llama_index/","title":"llama_index","text":""},{"location":"references/integrations/opik/","title":"opik","text":""},{"location":"references/integrations/opik/#ragas.integrations.opik.OpikTracer","title":"<code>OpikTracer</code>","text":"<p>               Bases: <code>OpikTracer</code></p> <p>Callback for Opik that can be used to log traces and evaluation scores to the Opik platform.</p> <p>Attributes:</p> Name Type Description <code>tags</code> <code>list[string]</code> <p>The tags to set on each trace.</p> <code>metadata</code> <code>dict</code> <p>Additional metadata to log for each trace.</p> Source code in <code>src/ragas/integrations/opik.py</code> <pre><code>class OpikTracer(LangchainOpikTracer):\n    \"\"\"\n    Callback for Opik that can be used to log traces and evaluation scores to the Opik platform.\n\n    Attributes\n    ----------\n    tags: list[string]\n        The tags to set on each trace.\n    metadata: dict\n        Additional metadata to log for each trace.\n    \"\"\"\n\n    _evaluation_run_id: t.Optional[str] = None\n\n    def _process_start_trace(self, run: \"Run\"):\n        if (run.parent_run_id is None) and (run.name == RAGAS_EVALUATION_CHAIN_NAME):\n            # Store the evaluation run id so we can flag the child traces and log them independently\n            self._evaluation_run_id = str(run.id)\n        else:\n            if run.parent_run_id == self._evaluation_run_id:\n                run.parent_run_id = None\n\n        super()._process_start_trace(run)\n\n    def _process_end_trace(self, run: \"Run\"):\n        if run.id != self._evaluation_run_id:\n            if run.name.startswith(\"row \"):\n                trace_data = self._created_traces_data_map[run.id]\n                if run.outputs:\n                    self._opik_client.log_traces_feedback_scores(\n                        [\n                            {\n                                \"id\": trace_data.id,\n                                \"name\": name,\n                                \"value\": round(value, 4),\n                            }\n                            for name, value in run.outputs.items()\n                        ]\n                    )\n\n            super()._process_end_trace(run)\n\n    def _persist_run(self, run: \"Run\"):\n        if run.id != self._evaluation_run_id:\n            super()._persist_run(run)\n</code></pre>"},{"location":"references/llms/__init__/","title":"llms","text":""},{"location":"references/llms/__init__/#ragas.llms.BaseRagasLLM","title":"<code>BaseRagasLLM</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>src/ragas/llms/base.py</code> <pre><code>@dataclass\nclass BaseRagasLLM(ABC):\n    run_config: RunConfig = field(default_factory=RunConfig)\n    multiple_completion_supported: bool = False\n\n    def set_run_config(self, run_config: RunConfig):\n        self.run_config = run_config\n\n    def get_temperature(self, n: int) -&gt; float:\n        \"\"\"Return the temperature to use for completion based on n.\"\"\"\n        return 0.3 if n &gt; 1 else 1e-8\n\n    @abstractmethod\n    def generate_text(\n        self,\n        prompt: PromptValue,\n        n: int = 1,\n        temperature: float = 1e-8,\n        stop: t.Optional[t.List[str]] = None,\n        callbacks: Callbacks = None,\n    ) -&gt; LLMResult: ...\n\n    @abstractmethod\n    async def agenerate_text(\n        self,\n        prompt: PromptValue,\n        n: int = 1,\n        temperature: t.Optional[float] = None,\n        stop: t.Optional[t.List[str]] = None,\n        callbacks: Callbacks = None,\n    ) -&gt; LLMResult: ...\n\n    async def generate(\n        self,\n        prompt: PromptValue,\n        n: int = 1,\n        temperature: t.Optional[float] = None,\n        stop: t.Optional[t.List[str]] = None,\n        callbacks: Callbacks = None,\n        is_async: bool = True,\n    ) -&gt; LLMResult:\n        \"\"\"Generate text using the given event loop.\"\"\"\n\n        if temperature is None:\n            temperature = 1e-8\n\n        if is_async:\n            agenerate_text_with_retry = add_async_retry(\n                self.agenerate_text, self.run_config\n            )\n            return await agenerate_text_with_retry(\n                prompt=prompt,\n                n=n,\n                temperature=temperature,\n                stop=stop,\n                callbacks=callbacks,\n            )\n        else:\n            loop = asyncio.get_event_loop()\n            generate_text_with_retry = add_retry(self.generate_text, self.run_config)\n            generate_text = partial(\n                generate_text_with_retry,\n                prompt=prompt,\n                n=n,\n                temperature=temperature,\n                stop=stop,\n                callbacks=callbacks,\n            )\n            return await loop.run_in_executor(None, generate_text)\n</code></pre>"},{"location":"references/llms/__init__/#ragas.llms.BaseRagasLLM.generate","title":"<code>generate(prompt, n=1, temperature=None, stop=None, callbacks=None, is_async=True)</code>  <code>async</code>","text":"<p>Generate text using the given event loop.</p> Source code in <code>src/ragas/llms/base.py</code> <pre><code>async def generate(\n    self,\n    prompt: PromptValue,\n    n: int = 1,\n    temperature: t.Optional[float] = None,\n    stop: t.Optional[t.List[str]] = None,\n    callbacks: Callbacks = None,\n    is_async: bool = True,\n) -&gt; LLMResult:\n    \"\"\"Generate text using the given event loop.\"\"\"\n\n    if temperature is None:\n        temperature = 1e-8\n\n    if is_async:\n        agenerate_text_with_retry = add_async_retry(\n            self.agenerate_text, self.run_config\n        )\n        return await agenerate_text_with_retry(\n            prompt=prompt,\n            n=n,\n            temperature=temperature,\n            stop=stop,\n            callbacks=callbacks,\n        )\n    else:\n        loop = asyncio.get_event_loop()\n        generate_text_with_retry = add_retry(self.generate_text, self.run_config)\n        generate_text = partial(\n            generate_text_with_retry,\n            prompt=prompt,\n            n=n,\n            temperature=temperature,\n            stop=stop,\n            callbacks=callbacks,\n        )\n        return await loop.run_in_executor(None, generate_text)\n</code></pre>"},{"location":"references/llms/__init__/#ragas.llms.BaseRagasLLM.get_temperature","title":"<code>get_temperature(n)</code>","text":"<p>Return the temperature to use for completion based on n.</p> Source code in <code>src/ragas/llms/base.py</code> <pre><code>def get_temperature(self, n: int) -&gt; float:\n    \"\"\"Return the temperature to use for completion based on n.\"\"\"\n    return 0.3 if n &gt; 1 else 1e-8\n</code></pre>"},{"location":"references/llms/__init__/#ragas.llms.LangchainLLMWrapper","title":"<code>LangchainLLMWrapper</code>","text":"<p>               Bases: <code>BaseRagasLLM</code></p> <p>A simple base class for RagasLLMs that is based on Langchain's BaseLanguageModel interface. it implements 2 functions: - generate_text: for generating text from a given PromptValue - agenerate_text: for generating text from a given PromptValue asynchronously</p> Source code in <code>src/ragas/llms/base.py</code> <pre><code>class LangchainLLMWrapper(BaseRagasLLM):\n    \"\"\"\n    A simple base class for RagasLLMs that is based on Langchain's BaseLanguageModel\n    interface. it implements 2 functions:\n    - generate_text: for generating text from a given PromptValue\n    - agenerate_text: for generating text from a given PromptValue asynchronously\n    \"\"\"\n\n    def __init__(\n        self, langchain_llm: BaseLanguageModel, run_config: t.Optional[RunConfig] = None\n    ):\n        self.langchain_llm = langchain_llm\n        if run_config is None:\n            run_config = RunConfig()\n        self.set_run_config(run_config)\n\n    def generate_text(\n        self,\n        prompt: PromptValue,\n        n: int = 1,\n        temperature: t.Optional[float] = None,\n        stop: t.Optional[t.List[str]] = None,\n        callbacks: Callbacks = None,\n    ) -&gt; LLMResult:\n        # figure out the temperature to set\n        if temperature is None:\n            temperature = self.get_temperature(n=n)\n\n        if is_multiple_completion_supported(self.langchain_llm):\n            return self.langchain_llm.generate_prompt(\n                prompts=[prompt],\n                n=n,\n                temperature=temperature,\n                stop=stop,\n                callbacks=callbacks,\n            )\n        else:\n            result = self.langchain_llm.generate_prompt(\n                prompts=[prompt] * n,\n                temperature=temperature,\n                stop=stop,\n                callbacks=callbacks,\n            )\n            # make LLMResult.generation appear as if it was n_completions\n            # note that LLMResult.runs is still a list that represents each run\n            generations = [[g[0] for g in result.generations]]\n            result.generations = generations\n            return result\n\n    async def agenerate_text(\n        self,\n        prompt: PromptValue,\n        n: int = 1,\n        temperature: t.Optional[float] = None,\n        stop: t.Optional[t.List[str]] = None,\n        callbacks: Callbacks = None,\n    ) -&gt; LLMResult:\n        if temperature is None:\n            temperature = self.get_temperature(n=n)\n\n        if is_multiple_completion_supported(self.langchain_llm):\n            return await self.langchain_llm.agenerate_prompt(\n                prompts=[prompt],\n                n=n,\n                temperature=temperature,\n                stop=stop,\n                callbacks=callbacks,\n            )\n        else:\n            result = await self.langchain_llm.agenerate_prompt(\n                prompts=[prompt] * n,\n                temperature=temperature,\n                stop=stop,\n                callbacks=callbacks,\n            )\n            # make LLMResult.generation appear as if it was n_completions\n            # note that LLMResult.runs is still a list that represents each run\n            generations = [[g[0] for g in result.generations]]\n            result.generations = generations\n            return result\n\n    def set_run_config(self, run_config: RunConfig):\n        self.run_config = run_config\n\n        # configure if using OpenAI API\n        if isinstance(self.langchain_llm, BaseOpenAI) or isinstance(\n            self.langchain_llm, ChatOpenAI\n        ):\n            try:\n                from openai import RateLimitError\n            except ImportError:\n                raise ImportError(\n                    \"openai.error.RateLimitError not found. Please install openai package as `pip install openai`\"\n                )\n            self.langchain_llm.request_timeout = run_config.timeout\n            self.run_config.exception_types = RateLimitError\n</code></pre>"},{"location":"references/llms/__init__/#ragas.llms.LlamaIndexLLMWrapper","title":"<code>LlamaIndexLLMWrapper</code>","text":"<p>               Bases: <code>BaseRagasLLM</code></p> <p>A Adaptor for LlamaIndex LLMs</p> Source code in <code>src/ragas/llms/base.py</code> <pre><code>class LlamaIndexLLMWrapper(BaseRagasLLM):\n    \"\"\"\n    A Adaptor for LlamaIndex LLMs\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: BaseLLM,\n        run_config: t.Optional[RunConfig] = None,\n    ):\n        self.llm = llm\n\n        self._signature = \"\"\n        if type(self.llm).__name__.lower() == \"bedrock\":\n            self._signature = \"bedrock\"\n        if run_config is None:\n            run_config = RunConfig()\n        self.set_run_config(run_config)\n\n    def check_args(\n        self,\n        n: int,\n        temperature: float,\n        stop: t.Optional[t.List[str]],\n        callbacks: Callbacks,\n    ) -&gt; dict[str, t.Any]:\n        if n != 1:\n            logger.warning(\"n values greater than 1 not support for LlamaIndex LLMs\")\n        if temperature != 1e-8:\n            logger.info(\"temperature kwarg passed to LlamaIndex LLM\")\n        if stop is not None:\n            logger.info(\"stop kwarg passed to LlamaIndex LLM\")\n        if callbacks is not None:\n            logger.info(\n                \"callbacks not supported for LlamaIndex LLMs, ignoring callbacks\"\n            )\n        if self._signature == \"bedrock\":\n            return {\"temperature\": temperature}\n        else:\n            return {\n                \"n\": n,\n                \"temperature\": temperature,\n                \"stop\": stop,\n            }\n\n    def generate_text(\n        self,\n        prompt: PromptValue,\n        n: int = 1,\n        temperature: float = 1e-8,\n        stop: t.Optional[t.List[str]] = None,\n        callbacks: Callbacks = None,\n    ) -&gt; LLMResult:\n        kwargs = self.check_args(n, temperature, stop, callbacks)\n        li_response = self.llm.complete(prompt.to_string(), **kwargs)\n\n        return LLMResult(generations=[[Generation(text=li_response.text)]])\n\n    async def agenerate_text(\n        self,\n        prompt: PromptValue,\n        n: int = 1,\n        temperature: t.Optional[float] = None,\n        stop: t.Optional[t.List[str]] = None,\n        callbacks: Callbacks = None,\n    ) -&gt; LLMResult:\n        if temperature is None:\n            temperature = 1e-8\n\n        kwargs = self.check_args(n, temperature, stop, callbacks)\n        li_response = await self.llm.acomplete(prompt.to_string(), **kwargs)\n\n        return LLMResult(generations=[[Generation(text=li_response.text)]])\n</code></pre>"},{"location":"references/llms/base/","title":"base","text":""},{"location":"references/llms/base/#ragas.llms.base.BaseRagasLLM","title":"<code>BaseRagasLLM</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>src/ragas/llms/base.py</code> <pre><code>@dataclass\nclass BaseRagasLLM(ABC):\n    run_config: RunConfig = field(default_factory=RunConfig)\n    multiple_completion_supported: bool = False\n\n    def set_run_config(self, run_config: RunConfig):\n        self.run_config = run_config\n\n    def get_temperature(self, n: int) -&gt; float:\n        \"\"\"Return the temperature to use for completion based on n.\"\"\"\n        return 0.3 if n &gt; 1 else 1e-8\n\n    @abstractmethod\n    def generate_text(\n        self,\n        prompt: PromptValue,\n        n: int = 1,\n        temperature: float = 1e-8,\n        stop: t.Optional[t.List[str]] = None,\n        callbacks: Callbacks = None,\n    ) -&gt; LLMResult: ...\n\n    @abstractmethod\n    async def agenerate_text(\n        self,\n        prompt: PromptValue,\n        n: int = 1,\n        temperature: t.Optional[float] = None,\n        stop: t.Optional[t.List[str]] = None,\n        callbacks: Callbacks = None,\n    ) -&gt; LLMResult: ...\n\n    async def generate(\n        self,\n        prompt: PromptValue,\n        n: int = 1,\n        temperature: t.Optional[float] = None,\n        stop: t.Optional[t.List[str]] = None,\n        callbacks: Callbacks = None,\n        is_async: bool = True,\n    ) -&gt; LLMResult:\n        \"\"\"Generate text using the given event loop.\"\"\"\n\n        if temperature is None:\n            temperature = 1e-8\n\n        if is_async:\n            agenerate_text_with_retry = add_async_retry(\n                self.agenerate_text, self.run_config\n            )\n            return await agenerate_text_with_retry(\n                prompt=prompt,\n                n=n,\n                temperature=temperature,\n                stop=stop,\n                callbacks=callbacks,\n            )\n        else:\n            loop = asyncio.get_event_loop()\n            generate_text_with_retry = add_retry(self.generate_text, self.run_config)\n            generate_text = partial(\n                generate_text_with_retry,\n                prompt=prompt,\n                n=n,\n                temperature=temperature,\n                stop=stop,\n                callbacks=callbacks,\n            )\n            return await loop.run_in_executor(None, generate_text)\n</code></pre>"},{"location":"references/llms/base/#ragas.llms.base.BaseRagasLLM.generate","title":"<code>generate(prompt, n=1, temperature=None, stop=None, callbacks=None, is_async=True)</code>  <code>async</code>","text":"<p>Generate text using the given event loop.</p> Source code in <code>src/ragas/llms/base.py</code> <pre><code>async def generate(\n    self,\n    prompt: PromptValue,\n    n: int = 1,\n    temperature: t.Optional[float] = None,\n    stop: t.Optional[t.List[str]] = None,\n    callbacks: Callbacks = None,\n    is_async: bool = True,\n) -&gt; LLMResult:\n    \"\"\"Generate text using the given event loop.\"\"\"\n\n    if temperature is None:\n        temperature = 1e-8\n\n    if is_async:\n        agenerate_text_with_retry = add_async_retry(\n            self.agenerate_text, self.run_config\n        )\n        return await agenerate_text_with_retry(\n            prompt=prompt,\n            n=n,\n            temperature=temperature,\n            stop=stop,\n            callbacks=callbacks,\n        )\n    else:\n        loop = asyncio.get_event_loop()\n        generate_text_with_retry = add_retry(self.generate_text, self.run_config)\n        generate_text = partial(\n            generate_text_with_retry,\n            prompt=prompt,\n            n=n,\n            temperature=temperature,\n            stop=stop,\n            callbacks=callbacks,\n        )\n        return await loop.run_in_executor(None, generate_text)\n</code></pre>"},{"location":"references/llms/base/#ragas.llms.base.BaseRagasLLM.get_temperature","title":"<code>get_temperature(n)</code>","text":"<p>Return the temperature to use for completion based on n.</p> Source code in <code>src/ragas/llms/base.py</code> <pre><code>def get_temperature(self, n: int) -&gt; float:\n    \"\"\"Return the temperature to use for completion based on n.\"\"\"\n    return 0.3 if n &gt; 1 else 1e-8\n</code></pre>"},{"location":"references/llms/base/#ragas.llms.base.LangchainLLMWrapper","title":"<code>LangchainLLMWrapper</code>","text":"<p>               Bases: <code>BaseRagasLLM</code></p> <p>A simple base class for RagasLLMs that is based on Langchain's BaseLanguageModel interface. it implements 2 functions: - generate_text: for generating text from a given PromptValue - agenerate_text: for generating text from a given PromptValue asynchronously</p> Source code in <code>src/ragas/llms/base.py</code> <pre><code>class LangchainLLMWrapper(BaseRagasLLM):\n    \"\"\"\n    A simple base class for RagasLLMs that is based on Langchain's BaseLanguageModel\n    interface. it implements 2 functions:\n    - generate_text: for generating text from a given PromptValue\n    - agenerate_text: for generating text from a given PromptValue asynchronously\n    \"\"\"\n\n    def __init__(\n        self, langchain_llm: BaseLanguageModel, run_config: t.Optional[RunConfig] = None\n    ):\n        self.langchain_llm = langchain_llm\n        if run_config is None:\n            run_config = RunConfig()\n        self.set_run_config(run_config)\n\n    def generate_text(\n        self,\n        prompt: PromptValue,\n        n: int = 1,\n        temperature: t.Optional[float] = None,\n        stop: t.Optional[t.List[str]] = None,\n        callbacks: Callbacks = None,\n    ) -&gt; LLMResult:\n        # figure out the temperature to set\n        if temperature is None:\n            temperature = self.get_temperature(n=n)\n\n        if is_multiple_completion_supported(self.langchain_llm):\n            return self.langchain_llm.generate_prompt(\n                prompts=[prompt],\n                n=n,\n                temperature=temperature,\n                stop=stop,\n                callbacks=callbacks,\n            )\n        else:\n            result = self.langchain_llm.generate_prompt(\n                prompts=[prompt] * n,\n                temperature=temperature,\n                stop=stop,\n                callbacks=callbacks,\n            )\n            # make LLMResult.generation appear as if it was n_completions\n            # note that LLMResult.runs is still a list that represents each run\n            generations = [[g[0] for g in result.generations]]\n            result.generations = generations\n            return result\n\n    async def agenerate_text(\n        self,\n        prompt: PromptValue,\n        n: int = 1,\n        temperature: t.Optional[float] = None,\n        stop: t.Optional[t.List[str]] = None,\n        callbacks: Callbacks = None,\n    ) -&gt; LLMResult:\n        if temperature is None:\n            temperature = self.get_temperature(n=n)\n\n        if is_multiple_completion_supported(self.langchain_llm):\n            return await self.langchain_llm.agenerate_prompt(\n                prompts=[prompt],\n                n=n,\n                temperature=temperature,\n                stop=stop,\n                callbacks=callbacks,\n            )\n        else:\n            result = await self.langchain_llm.agenerate_prompt(\n                prompts=[prompt] * n,\n                temperature=temperature,\n                stop=stop,\n                callbacks=callbacks,\n            )\n            # make LLMResult.generation appear as if it was n_completions\n            # note that LLMResult.runs is still a list that represents each run\n            generations = [[g[0] for g in result.generations]]\n            result.generations = generations\n            return result\n\n    def set_run_config(self, run_config: RunConfig):\n        self.run_config = run_config\n\n        # configure if using OpenAI API\n        if isinstance(self.langchain_llm, BaseOpenAI) or isinstance(\n            self.langchain_llm, ChatOpenAI\n        ):\n            try:\n                from openai import RateLimitError\n            except ImportError:\n                raise ImportError(\n                    \"openai.error.RateLimitError not found. Please install openai package as `pip install openai`\"\n                )\n            self.langchain_llm.request_timeout = run_config.timeout\n            self.run_config.exception_types = RateLimitError\n</code></pre>"},{"location":"references/llms/base/#ragas.llms.base.LlamaIndexLLMWrapper","title":"<code>LlamaIndexLLMWrapper</code>","text":"<p>               Bases: <code>BaseRagasLLM</code></p> <p>A Adaptor for LlamaIndex LLMs</p> Source code in <code>src/ragas/llms/base.py</code> <pre><code>class LlamaIndexLLMWrapper(BaseRagasLLM):\n    \"\"\"\n    A Adaptor for LlamaIndex LLMs\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: BaseLLM,\n        run_config: t.Optional[RunConfig] = None,\n    ):\n        self.llm = llm\n\n        self._signature = \"\"\n        if type(self.llm).__name__.lower() == \"bedrock\":\n            self._signature = \"bedrock\"\n        if run_config is None:\n            run_config = RunConfig()\n        self.set_run_config(run_config)\n\n    def check_args(\n        self,\n        n: int,\n        temperature: float,\n        stop: t.Optional[t.List[str]],\n        callbacks: Callbacks,\n    ) -&gt; dict[str, t.Any]:\n        if n != 1:\n            logger.warning(\"n values greater than 1 not support for LlamaIndex LLMs\")\n        if temperature != 1e-8:\n            logger.info(\"temperature kwarg passed to LlamaIndex LLM\")\n        if stop is not None:\n            logger.info(\"stop kwarg passed to LlamaIndex LLM\")\n        if callbacks is not None:\n            logger.info(\n                \"callbacks not supported for LlamaIndex LLMs, ignoring callbacks\"\n            )\n        if self._signature == \"bedrock\":\n            return {\"temperature\": temperature}\n        else:\n            return {\n                \"n\": n,\n                \"temperature\": temperature,\n                \"stop\": stop,\n            }\n\n    def generate_text(\n        self,\n        prompt: PromptValue,\n        n: int = 1,\n        temperature: float = 1e-8,\n        stop: t.Optional[t.List[str]] = None,\n        callbacks: Callbacks = None,\n    ) -&gt; LLMResult:\n        kwargs = self.check_args(n, temperature, stop, callbacks)\n        li_response = self.llm.complete(prompt.to_string(), **kwargs)\n\n        return LLMResult(generations=[[Generation(text=li_response.text)]])\n\n    async def agenerate_text(\n        self,\n        prompt: PromptValue,\n        n: int = 1,\n        temperature: t.Optional[float] = None,\n        stop: t.Optional[t.List[str]] = None,\n        callbacks: Callbacks = None,\n    ) -&gt; LLMResult:\n        if temperature is None:\n            temperature = 1e-8\n\n        kwargs = self.check_args(n, temperature, stop, callbacks)\n        li_response = await self.llm.acomplete(prompt.to_string(), **kwargs)\n\n        return LLMResult(generations=[[Generation(text=li_response.text)]])\n</code></pre>"},{"location":"references/llms/base/#ragas.llms.base.is_multiple_completion_supported","title":"<code>is_multiple_completion_supported(llm)</code>","text":"<p>Return whether the given LLM supports n-completion.</p> Source code in <code>src/ragas/llms/base.py</code> <pre><code>def is_multiple_completion_supported(llm: BaseLanguageModel) -&gt; bool:\n    \"\"\"Return whether the given LLM supports n-completion.\"\"\"\n    for llm_type in MULTIPLE_COMPLETION_SUPPORTED:\n        if isinstance(llm, llm_type):\n            return True\n    return False\n</code></pre>"},{"location":"references/llms/json_load/","title":"json_load","text":""},{"location":"references/llms/json_load/#ragas.llms.json_load.load_as_json","title":"<code>load_as_json(text)</code>","text":"<p>validate and return given text as json</p> Source code in <code>src/ragas/llms/json_load.py</code> <pre><code>def load_as_json(text) -&gt; t.Dict:\n    \"\"\"\n    validate and return given text as json\n    \"\"\"\n\n    try:\n        return json.loads(text)\n    except ValueError as e:\n        logger.warn(f\"Invalid json: {e}\")\n        return {}\n</code></pre>"},{"location":"references/llms/output_parser/","title":"output_parser","text":""},{"location":"references/llms/prompt/","title":"prompt","text":""},{"location":"references/llms/prompt/#ragas.llms.prompt.Prompt","title":"<code>Prompt</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Prompt is a class that represents a prompt for the ragas metrics.</p> <p>Attributes:     name (str): The name of the prompt.     instruction (str): The instruction for the prompt.     output_format_instruction (str): The output format instruction for the prompt.     examples (List[Dict[str, Any]]): List of example inputs and outputs for the prompt.     input_keys (List[str]): List of input variable names.     output_key (str): The output variable name.     output_type (Literal[\"json\", \"str\"]): The type of the output (default: \"json\").     language (str): The language of the prompt (default: \"english\").</p> Source code in <code>src/ragas/llms/prompt.py</code> <pre><code>class Prompt(BaseModel):\n    \"\"\"\n    Prompt is a class that represents a prompt for the ragas metrics.\n\n    Attributes:\n        name (str): The name of the prompt.\n        instruction (str): The instruction for the prompt.\n        output_format_instruction (str): The output format instruction for the prompt.\n        examples (List[Dict[str, Any]]): List of example inputs and outputs for the prompt.\n        input_keys (List[str]): List of input variable names.\n        output_key (str): The output variable name.\n        output_type (Literal[\"json\", \"str\"]): The type of the output (default: \"json\").\n        language (str): The language of the prompt (default: \"english\").\n    \"\"\"\n\n    name: str = \"\"\n    instruction: str\n    output_format_instruction: str = \"\"\n    examples: t.List[Example] = []\n    input_keys: t.List[str] = [\"\"]\n    output_key: str = \"\"\n    output_type: t.Literal[\"json\", \"str\"] = \"json\"\n    language: str = \"english\"\n\n    @root_validator\n    def validate_prompt(cls, values: t.Dict[str, t.Any]) -&gt; t.Dict[str, t.Any]:\n        \"\"\"\n        Validate the template string to ensure that it is in desired format.\n        \"\"\"\n        if values.get(\"instruction\") is None or values.get(\"instruction\") == \"\":\n            raise ValueError(\"instruction cannot be empty\")\n        if values.get(\"input_keys\") is None or values.get(\"instruction\") == []:\n            raise ValueError(\"input_keys cannot be empty\")\n        if values.get(\"output_key\") is None or values.get(\"output_key\") == \"\":\n            raise ValueError(\"output_key cannot be empty\")\n\n        if values.get(\"examples\"):\n            output_key = values[\"output_key\"]\n            for no, example in enumerate(values[\"examples\"]):\n                for inp_key in values[\"input_keys\"]:\n                    if inp_key not in example:\n                        raise ValueError(\n                            f\"example {no+1} does not have the variable {inp_key} in the definition\"\n                        )\n                if output_key not in example:\n                    raise ValueError(\n                        f\"example {no+1} does not have the variable {output_key} in the definition\"\n                    )\n                if values[\"output_type\"].lower() == \"json\":\n                    try:\n                        if output_key in example:\n                            if isinstance(example[output_key], str):\n                                json.loads(example[output_key])\n                    except ValueError as e:\n                        raise ValueError(\n                            f\"{output_key} in example {no+1} is not in valid json format: {e}\"\n                        )\n\n        return values\n\n    def to_string(self) -&gt; str:\n        \"\"\"\n        Generate the prompt string from the variables.\n        \"\"\"\n        prompt_elements = [self.instruction]\n        if self.output_format_instruction:\n            prompt_elements.append(\n                \"\\n\"\n                + self.output_format_instruction.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n            )\n        prompt_str = \"\\n\".join(prompt_elements) + \"\\n\"\n\n        if self.examples:\n            prompt_str += \"\\nExamples:\\n\"\n            # Format the examples to match the Langchain prompt template\n            for example in self.examples:\n                for key, value in example.items():\n                    is_json = isinstance(value, (dict, list))\n                    value = (\n                        json.dumps(value, ensure_ascii=False).encode(\"utf8\").decode()\n                    )\n                    value = (\n                        value.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n                        if self.output_type.lower() == \"json\"\n                        else value\n                    )\n                    prompt_str += (\n                        f\"\\n{key}: {value}\"\n                        if not is_json\n                        else f\"\\n{key}: ```{value}```\"\n                    )\n                prompt_str += \"\\n\"\n\n        prompt_str += \"\\nYour actual task:\\n\"\n\n        if self.input_keys:\n            prompt_str += \"\".join(f\"\\n{key}: {{{key}}}\" for key in self.input_keys)\n        if self.output_key:\n            prompt_str += f\"\\n{self.output_key}: \\n\"\n\n        return prompt_str\n\n    def get_example_str(self, example_no: int) -&gt; str:\n        \"\"\"\n        Get the example string from the example number.\n        \"\"\"\n        if example_no &gt;= len(self.examples):\n            raise ValueError(f\"example number {example_no} is out of range\")\n        example = self.examples[example_no]\n        example_str = \"\"\n        for key, value in example.items():\n            value = json.dumps(value, ensure_ascii=False).encode(\"utf8\").decode()\n            value = (\n                value.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n                if self.output_type.lower() == \"json\"\n                else value\n            )\n            example_str += f\"\\n{key}: {value}\"\n        return \"```\" + example_str + \"```\"\n\n    def format(self, **kwargs: t.Any) -&gt; PromptValue:\n        \"\"\"\n        Format the Prompt object into a ChatPromptTemplate object to be used in metrics.\n        \"\"\"\n        if set(self.input_keys) != set(kwargs.keys()):\n            raise ValueError(\n                f\"Input variables {self.input_keys} do not match with the given parameters {list(kwargs.keys())}\"\n            )\n        for key, value in kwargs.items():\n            if isinstance(value, str):\n                kwargs[key] = json.dumps(value)\n\n        prompt = self.to_string()\n        return PromptValue(prompt_str=prompt.format(**kwargs))\n\n    def adapt(\n        self, language: str, llm: BaseRagasLLM, cache_dir: t.Optional[str] = None\n    ) -&gt; Prompt:\n        def get_all_keys(nested_json):\n            keys = set()\n            for key, value in nested_json.items():\n                keys.add(key)\n                if isinstance(value, dict):\n                    keys = keys.union(get_all_keys(value))\n            return keys\n\n        if self.language == language:\n            return self\n\n        # TODO: Add callbacks\n        cache_dir = cache_dir if cache_dir else get_cache_dir()\n        if os.path.exists(os.path.join(cache_dir, language, f\"{self.name}.json\")):\n            self_cp = self._load(language, self.name, cache_dir)\n\n            self.language = self_cp.language\n            self.examples = self_cp.examples\n\n            return self_cp\n\n        logger.info(\"Adapting %s to %s\", self.name, language)\n        prompts = []\n        output_keys = []\n        for example in self.examples:\n            prompts.extend(\n                [\n                    str_translation.format(\n                        translate_to=language, input=example.get(key)\n                    )\n                    for key in self.input_keys\n                ]\n            )\n            prompts.append(\n                json_translatation.format(\n                    translate_to=language, input=example.get(self.output_key)\n                )\n                if self.output_type.lower() == \"json\"\n                else str_translation.format(\n                    translate_to=language, input=example.get(self.output_key)\n                )\n            )\n            if self.output_type.lower() == \"json\":\n                output = example.get(self.output_key)\n                if isinstance(output, str):\n                    output = json.loads(output)\n                if isinstance(output, dict):\n                    output_keys.append(get_all_keys(output))\n                elif isinstance(output, list) and all(\n                    isinstance(item, dict) for item in output\n                ):\n                    output_keys.append([get_all_keys(item) for item in output])\n\n        # NOTE: this is a slow loop, consider Executor to fasten this\n        results = []\n        for p in prompts:\n            results.append(llm.generate_text(p).generations[0][0].text)\n        per_example_items = len(self.input_keys) + 1\n        grouped_results = [\n            results[i : i + per_example_items]\n            for i in range(0, len(results), per_example_items)\n        ]\n        assert len(grouped_results) == len(\n            self.examples\n        ), \"examples and adapted examples must be of equal length\"\n        for i, example in enumerate(grouped_results):\n            example_dict = {}\n            example_dict.update(\n                {k: v for k, v in zip(self.input_keys, example[: len(self.input_keys)])}\n            )\n            if self.output_type.lower() == \"json\":\n                example_dict[self.output_key] = json_loader._safe_load(example[-1], llm)\n                if example_dict[self.output_key] == {}:\n                    # Extracting the dictionary part using string slicing\n                    dict_str = example[-1].split(\"(\")[0].strip()\n                    example_dict[self.output_key] = ast.literal_eval(dict_str)\n                else:\n                    example_dict[self.output_key] = example[-1]\n            if self.output_type.lower() == \"json\":\n                output = example_dict[self.output_key]\n                if isinstance(output, dict):\n                    assert (\n                        set(output.keys()) == output_keys[i]\n                    ), f\"Adapted output keys {set(output.keys())=} do not match with the original output keys: {output_keys[i]=}\"\n                elif isinstance(output, list) and all(\n                    isinstance(item, dict) for item in output\n                ):\n                    assert all(\n                        set(item.keys()) in output_keys[i] for item in output\n                    ), \"Adapted output keys do not match with the original output keys\"\n\n            self.examples[i] = example_dict\n\n        self.language = language\n\n        # TODO:Validate the prompt after adaptation\n\n        self.save(cache_dir=cache_dir)\n\n        return self\n\n    def save(self, cache_dir: t.Optional[str] = None):\n        cache_dir = cache_dir if cache_dir else get_cache_dir()\n        cache_dir = os.path.join(cache_dir, self.language)\n        if not os.path.exists(cache_dir):\n            os.makedirs(cache_dir)\n\n        cache_path = os.path.join(cache_dir, f\"{self.name}.json\")\n        with open(cache_path, \"w\") as file:\n            json.dump(self.dict(), file, indent=4)\n\n    @classmethod\n    def _load(cls, language: str, name: str, cache_dir: str) -&gt; Prompt:\n        logger.info(\"Loading %s from %s\", name, cache_dir)\n        path = os.path.join(cache_dir, language, f\"{name}.json\")\n        return cls(**json.load(open(path)))\n</code></pre>"},{"location":"references/llms/prompt/#ragas.llms.prompt.Prompt.format","title":"<code>format(**kwargs)</code>","text":"<p>Format the Prompt object into a ChatPromptTemplate object to be used in metrics.</p> Source code in <code>src/ragas/llms/prompt.py</code> <pre><code>def format(self, **kwargs: t.Any) -&gt; PromptValue:\n    \"\"\"\n    Format the Prompt object into a ChatPromptTemplate object to be used in metrics.\n    \"\"\"\n    if set(self.input_keys) != set(kwargs.keys()):\n        raise ValueError(\n            f\"Input variables {self.input_keys} do not match with the given parameters {list(kwargs.keys())}\"\n        )\n    for key, value in kwargs.items():\n        if isinstance(value, str):\n            kwargs[key] = json.dumps(value)\n\n    prompt = self.to_string()\n    return PromptValue(prompt_str=prompt.format(**kwargs))\n</code></pre>"},{"location":"references/llms/prompt/#ragas.llms.prompt.Prompt.get_example_str","title":"<code>get_example_str(example_no)</code>","text":"<p>Get the example string from the example number.</p> Source code in <code>src/ragas/llms/prompt.py</code> <pre><code>def get_example_str(self, example_no: int) -&gt; str:\n    \"\"\"\n    Get the example string from the example number.\n    \"\"\"\n    if example_no &gt;= len(self.examples):\n        raise ValueError(f\"example number {example_no} is out of range\")\n    example = self.examples[example_no]\n    example_str = \"\"\n    for key, value in example.items():\n        value = json.dumps(value, ensure_ascii=False).encode(\"utf8\").decode()\n        value = (\n            value.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n            if self.output_type.lower() == \"json\"\n            else value\n        )\n        example_str += f\"\\n{key}: {value}\"\n    return \"```\" + example_str + \"```\"\n</code></pre>"},{"location":"references/llms/prompt/#ragas.llms.prompt.Prompt.to_string","title":"<code>to_string()</code>","text":"<p>Generate the prompt string from the variables.</p> Source code in <code>src/ragas/llms/prompt.py</code> <pre><code>def to_string(self) -&gt; str:\n    \"\"\"\n    Generate the prompt string from the variables.\n    \"\"\"\n    prompt_elements = [self.instruction]\n    if self.output_format_instruction:\n        prompt_elements.append(\n            \"\\n\"\n            + self.output_format_instruction.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n        )\n    prompt_str = \"\\n\".join(prompt_elements) + \"\\n\"\n\n    if self.examples:\n        prompt_str += \"\\nExamples:\\n\"\n        # Format the examples to match the Langchain prompt template\n        for example in self.examples:\n            for key, value in example.items():\n                is_json = isinstance(value, (dict, list))\n                value = (\n                    json.dumps(value, ensure_ascii=False).encode(\"utf8\").decode()\n                )\n                value = (\n                    value.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n                    if self.output_type.lower() == \"json\"\n                    else value\n                )\n                prompt_str += (\n                    f\"\\n{key}: {value}\"\n                    if not is_json\n                    else f\"\\n{key}: ```{value}```\"\n                )\n            prompt_str += \"\\n\"\n\n    prompt_str += \"\\nYour actual task:\\n\"\n\n    if self.input_keys:\n        prompt_str += \"\".join(f\"\\n{key}: {{{key}}}\" for key in self.input_keys)\n    if self.output_key:\n        prompt_str += f\"\\n{self.output_key}: \\n\"\n\n    return prompt_str\n</code></pre>"},{"location":"references/llms/prompt/#ragas.llms.prompt.Prompt.validate_prompt","title":"<code>validate_prompt(values)</code>","text":"<p>Validate the template string to ensure that it is in desired format.</p> Source code in <code>src/ragas/llms/prompt.py</code> <pre><code>@root_validator\ndef validate_prompt(cls, values: t.Dict[str, t.Any]) -&gt; t.Dict[str, t.Any]:\n    \"\"\"\n    Validate the template string to ensure that it is in desired format.\n    \"\"\"\n    if values.get(\"instruction\") is None or values.get(\"instruction\") == \"\":\n        raise ValueError(\"instruction cannot be empty\")\n    if values.get(\"input_keys\") is None or values.get(\"instruction\") == []:\n        raise ValueError(\"input_keys cannot be empty\")\n    if values.get(\"output_key\") is None or values.get(\"output_key\") == \"\":\n        raise ValueError(\"output_key cannot be empty\")\n\n    if values.get(\"examples\"):\n        output_key = values[\"output_key\"]\n        for no, example in enumerate(values[\"examples\"]):\n            for inp_key in values[\"input_keys\"]:\n                if inp_key not in example:\n                    raise ValueError(\n                        f\"example {no+1} does not have the variable {inp_key} in the definition\"\n                    )\n            if output_key not in example:\n                raise ValueError(\n                    f\"example {no+1} does not have the variable {output_key} in the definition\"\n                )\n            if values[\"output_type\"].lower() == \"json\":\n                try:\n                    if output_key in example:\n                        if isinstance(example[output_key], str):\n                            json.loads(example[output_key])\n                except ValueError as e:\n                    raise ValueError(\n                        f\"{output_key} in example {no+1} is not in valid json format: {e}\"\n                    )\n\n    return values\n</code></pre>"},{"location":"references/llms/prompt/#ragas.llms.prompt.PromptValue","title":"<code>PromptValue</code>","text":"<p>               Bases: <code>PromptValue</code></p> Source code in <code>src/ragas/llms/prompt.py</code> <pre><code>class PromptValue(BasePromptValue):\n    prompt_str: str\n\n    def to_messages(self) -&gt; t.List[BaseMessage]:\n        \"\"\"Return prompt as a list of Messages.\"\"\"\n        return [HumanMessage(content=self.to_string())]\n\n    def to_string(self) -&gt; str:\n        return self.prompt_str\n</code></pre>"},{"location":"references/llms/prompt/#ragas.llms.prompt.PromptValue.to_messages","title":"<code>to_messages()</code>","text":"<p>Return prompt as a list of Messages.</p> Source code in <code>src/ragas/llms/prompt.py</code> <pre><code>def to_messages(self) -&gt; t.List[BaseMessage]:\n    \"\"\"Return prompt as a list of Messages.\"\"\"\n    return [HumanMessage(content=self.to_string())]\n</code></pre>"},{"location":"references/metrics/__init__/","title":"metrics","text":""},{"location":"references/metrics/__init__/#ragas.metrics.AnswerCorrectness","title":"<code>AnswerCorrectness</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetricWithLLM</code>, <code>MetricWithEmbeddings</code>, <code>SingleTurnMetric</code></p> <p>Measures answer correctness compared to ground truth as a combination of factuality and semantic similarity.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>string</code> <p>The name of the metrics</p> <code>weights</code> <code>list[float]</code> <p>a list of two weights corresponding to factuality and semantic similarity Defaults [0.75, 0.25]</p> <code>answer_similarity</code> <code>Optional[AnswerSimilarity]</code> <p>The AnswerSimilarity object</p> Source code in <code>src/ragas/metrics/_answer_correctness.py</code> <pre><code>@dataclass\nclass AnswerCorrectness(MetricWithLLM, MetricWithEmbeddings, SingleTurnMetric):\n    \"\"\"\n    Measures answer correctness compared to ground truth as a combination of\n    factuality and semantic similarity.\n\n    Attributes\n    ----------\n    name: string\n        The name of the metrics\n    weights:\n        a list of two weights corresponding to factuality and semantic similarity\n        Defaults [0.75, 0.25]\n    answer_similarity:\n        The AnswerSimilarity object\n    \"\"\"\n\n    name: str = \"answer_correctness\"  # type: ignore[reportIncompatibleMethodOverride]\n    _required_columns: t.Dict[MetricType, t.Set[str]] = field(\n        default_factory=lambda: {\n            MetricType.SINGLE_TURN: {\"user_input\", \"response\", \"reference\"}\n        }\n    )\n    correctness_prompt: Prompt = field(default_factory=lambda: CORRECTNESS_PROMPT)\n    long_form_answer_prompt: Prompt = field(\n        default_factory=lambda: LONG_FORM_ANSWER_PROMPT\n    )\n    weights: list[float] = field(default_factory=lambda: [0.75, 0.25])\n    answer_similarity: t.Optional[AnswerSimilarity] = None\n    sentence_segmenter: t.Optional[HasSegmentMethod] = None\n    max_retries: int = 1\n\n    def __post_init__(self: t.Self):\n        if len(self.weights) != 2:\n            raise ValueError(\n                \"Expects a list of two weights. First for factuality, second for semantic similarity\"\n            )\n        if all([w == 0 for w in self.weights]):\n            raise ValueError(\"At least one weight must be non-zero\")\n        if not all([w &gt;= 0 for w in self.weights]):\n            raise ValueError(\"Weights must be non-negative\")\n\n        if self.sentence_segmenter is None:\n            language = self.long_form_answer_prompt.language\n            self.sentence_segmenter = get_segmenter(language=language, clean=False)\n\n    def init(self, run_config: RunConfig):\n        super().init(run_config)\n        if self.answer_similarity is None and self.weights[1] != 0:\n            self.answer_similarity = AnswerSimilarity(\n                llm=self.llm, embeddings=self.embeddings\n            )\n\n    def _compute_statement_presence(\n        self, prediction: AnswerCorrectnessClassification\n    ) -&gt; float:\n        tp = len(prediction.TP)\n        fp = len(prediction.FP)\n        fn = len(prediction.FN)\n        score = tp / (tp + 0.5 * (fp + fn)) if tp &gt; 0 else 0\n        return score\n\n    def _create_statements_prompt(self, question: str, text: str) -&gt; PromptValue:\n        assert self.sentence_segmenter is not None, \"sentence_segmenter is not set\"\n\n        sentences = self.sentence_segmenter.segment(text)\n        sentences = [\n            sentence for sentence in sentences if sentence.strip().endswith(\".\")\n        ]\n        sentences = \"\\n\".join([f\"{i}:{x}\" for i, x in enumerate(sentences)])\n        prompt_value = self.long_form_answer_prompt.format(\n            question=question, answer=text, sentences=sentences\n        )\n        return prompt_value\n\n    async def _single_turn_ascore(\n        self: t.Self, sample: SingleTurnSample, callbacks: Callbacks\n    ) -&gt; float:\n        row = sample.to_dict()\n        score = await self._ascore(row, callbacks)\n        return score\n\n    async def _ascore(self, row: t.Dict, callbacks: Callbacks) -&gt; float:\n        assert self.llm is not None, \"LLM must be set\"\n\n        question = row[\"user_input\"]\n        statements = {}\n        for item in [\"response\", \"reference\"]:\n            p_value = self._create_statements_prompt(question, row[item])\n            item_statement = await self.llm.generate(p_value, callbacks=callbacks)\n            statements[item] = await _statements_output_parser.aparse(\n                item_statement.generations[0][0].text,\n                p_value,\n                self.llm,\n                self.max_retries,\n            )\n            statements[item] = (\n                statements[item].dicts() if statements[item] is not None else []\n            )\n\n        if not all([val == [] for val in statements.values()]):\n            ground_truth = [\n                statement\n                for item in statements[\"reference\"]\n                for statement in item[\"simpler_statements\"]\n            ]\n            answer = [\n                statement\n                for item in statements[\"response\"]\n                for statement in item[\"simpler_statements\"]\n            ]\n            p_value = self.correctness_prompt.format(\n                question=question,\n                ground_truth=ground_truth,\n                answer=answer,\n            )\n            is_statement_present = await self.llm.generate(p_value, callbacks=callbacks)\n            result_text = is_statement_present.generations[0][0].text\n\n            answers = await _output_parser.aparse(\n                result_text, p_value, self.llm, self.max_retries\n            )\n            if answers is None:\n                return np.nan\n\n            f1_score = self._compute_statement_presence(answers)\n        else:\n            f1_score = 1.0\n\n        if self.weights[1] == 0:\n            similarity_score = 0.0\n        else:\n            assert self.answer_similarity is not None, \"AnswerSimilarity must be set\"\n\n            similarity_score = await self.answer_similarity.ascore(\n                row, callbacks=callbacks\n            )\n\n        score = np.average(\n            [f1_score, similarity_score],\n            weights=self.weights,\n        )\n\n        return float(score)\n\n    def adapt(self, language: str, cache_dir: t.Optional[str] = None) -&gt; None:\n        assert self.llm is not None, \"llm must be set to compute score\"\n\n        logger.info(f\"Adapting AnswerCorrectness metric to {language}\")\n        self.correctness_prompt = self.correctness_prompt.adapt(\n            language, self.llm, cache_dir\n        )\n\n        self.sentence_segmenter = get_segmenter(language=language, clean=False)\n\n    def save(self, cache_dir: t.Optional[str] = None) -&gt; None:\n        self.correctness_prompt.save(cache_dir)\n</code></pre>"},{"location":"references/metrics/__init__/#ragas.metrics.AspectCritic","title":"<code>AspectCritic</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetricWithLLM</code>, <code>SingleTurnMetric</code>, <code>MultiTurnMetric</code></p> <p>Judges the submission to give binary results using the criteria specified in the metric definition.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>name of the metrics</p> <code>definition</code> <code>str</code> <p>criteria to judge the submission, example \"Is the submission spreading fake information?\"</p> <code>strictness</code> <code>int</code> <p>The number of times self consistency checks is made. Final judgement is made using majority vote.</p> Source code in <code>src/ragas/metrics/_aspect_critic.py</code> <pre><code>@dataclass\nclass AspectCritic(MetricWithLLM, SingleTurnMetric, MultiTurnMetric):\n    \"\"\"\n    Judges the submission to give binary results using the criteria specified\n    in the metric definition.\n\n    Attributes\n    ----------\n    name: str\n        name of the metrics\n    definition: str\n        criteria to judge the submission, example \"Is the submission spreading\n        fake information?\"\n    strictness: int\n        The number of times self consistency checks is made. Final judgement is\n        made using majority vote.\n    \"\"\"\n\n    name: str = field(default=\"\", repr=True)  # type: ignore\n    _required_columns: t.Dict[MetricType, t.Set[str]] = field(\n        default_factory=lambda: {\n            MetricType.SINGLE_TURN: {\n                \"user_input\",\n                \"response\",\n            }\n        }\n    )\n    single_turn_prompt: PydanticPrompt = field(\n        default_factory=lambda: SingleTurnAspectCriticPrompt()\n    )\n    multi_turn_prompt: PydanticPrompt = field(\n        default_factory=lambda: MultiTurnAspectCriticPrompt()\n    )\n    definition: str = field(default=\"\", repr=True)\n    strictness: int = field(default=1, repr=False)\n    max_retries: int = 1\n\n    def __post_init__(self: t.Self):\n        if self.name == \"\":\n            raise ValueError(\"Expects a name\")\n        if self.definition == \"\":\n            raise ValueError(\"Expects definition\")\n\n        # ensure odd number of checks to avoid tie in majority vote.\n        self.strictness = (\n            self.strictness if self.strictness % 2 != 0 else self.strictness + 1\n        )\n\n    def _compute_score(\n        self, safe_loaded_responses: t.List[AspectCriticOutput]\n    ) -&gt; float:\n        if self.strictness &gt; 1:\n            score = Counter(\n                [item.verdict for item in safe_loaded_responses]\n            ).most_common(1)[0][0]\n        else:\n            score = safe_loaded_responses[0].verdict\n\n        return score\n\n    async def _single_turn_ascore(\n        self: t.Self, sample: SingleTurnSample, callbacks: Callbacks\n    ) -&gt; float:\n        row = sample.to_dict()\n        return await self._ascore(row, callbacks)\n\n    async def _ascore(self: t.Self, row: t.Dict, callbacks: Callbacks) -&gt; float:\n        assert self.llm is not None, \"set LLM before use\"\n\n        user_input, context, response = (\n            row[\"user_input\"],\n            row.get(\"retrieved_contexts\"),\n            row[\"response\"],\n        )\n\n        if context is not None:\n            if isinstance(context, list):\n                context = \"\\n\".join(context)\n            user_input = f\"Question: {user_input} Answer using context: {context}\"\n\n        prompt_input = AspectCriticInput(\n            user_input=user_input,\n            response=response,\n            criteria=self.definition,\n        )\n\n        response = await self.single_turn_prompt.generate(\n            data=prompt_input,\n            llm=self.llm,\n            callbacks=callbacks,\n        )\n\n        return self._compute_score([response])\n\n    async def _multi_turn_ascore(\n        self: t.Self, sample: MultiTurnSample, callbacks: Callbacks\n    ) -&gt; float:\n        assert self.llm is not None, \"LLM is not set\"\n        assert sample.reference is not None, \"Reference is not set\"\n\n        interaction = sample.pretty_repr()\n        reference = sample.reference\n        prompt_input = AspectCriticInput(\n            user_input=interaction,\n            response=reference,\n            criteria=self.definition,\n        )\n        response = await self.multi_turn_prompt.generate(\n            data=prompt_input,\n            llm=self.llm,\n            callbacks=callbacks,\n        )\n        return self._compute_score([response])\n</code></pre>"},{"location":"references/metrics/__init__/#ragas.metrics.ContextEntityRecall","title":"<code>ContextEntityRecall</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetricWithLLM</code>, <code>SingleTurnMetric</code></p> <p>Calculates recall based on entities present in ground truth and context. Let CN be the set of entities present in context, GN be the set of entities present in the ground truth.</p> <p>Then we define can the context entity recall as follows: Context Entity recall = | CN \u2229 GN | / | GN |</p> <p>If this quantity is 1, we can say that the retrieval mechanism has retrieved context which covers all entities present in the ground truth, thus being a useful retrieval. Thus this can be used to evaluate retrieval mechanisms in specific use cases where entities matter, for example, a tourism help chatbot.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <code>batch_size</code> <code>int</code> <p>Batch size for openai completion.</p> Source code in <code>src/ragas/metrics/_context_entities_recall.py</code> <pre><code>@dataclass\nclass ContextEntityRecall(MetricWithLLM, SingleTurnMetric):\n    \"\"\"\n    Calculates recall based on entities present in ground truth and context.\n    Let CN be the set of entities present in context,\n    GN be the set of entities present in the ground truth.\n\n    Then we define can the context entity recall as follows:\n    Context Entity recall = | CN \u2229 GN | / | GN |\n\n    If this quantity is 1, we can say that the retrieval mechanism has\n    retrieved context which covers all entities present in the ground truth,\n    thus being a useful retrieval. Thus this can be used to evaluate retrieval\n    mechanisms in specific use cases where entities matter, for example, a\n    tourism help chatbot.\n\n    Attributes\n    ----------\n    name : str\n    batch_size : int\n        Batch size for openai completion.\n    \"\"\"\n\n    name: str = \"context_entity_recall\"  # type: ignore\n    _required_columns: t.Dict[MetricType, t.Set[str]] = field(\n        default_factory=lambda: {\n            MetricType.SINGLE_TURN: {\"reference\", \"retrieved_contexts\"}\n        }\n    )\n    context_entity_recall_prompt: Prompt = field(\n        default_factory=lambda: TEXT_ENTITY_EXTRACTION\n    )\n    max_retries: int = 1\n\n    def _compute_score(\n        self, ground_truth_entities: t.Sequence[str], context_entities: t.Sequence[str]\n    ) -&gt; float:\n        num_entities_in_both = len(\n            set(context_entities).intersection(set(ground_truth_entities))\n        )\n        return num_entities_in_both / (len(ground_truth_entities) + 1e-8)\n\n    async def get_entities(\n        self,\n        text: str,\n        callbacks: Callbacks,\n    ) -&gt; t.Optional[ContextEntitiesResponse]:\n        assert self.llm is not None, \"LLM is not initialized\"\n        p_value = self.context_entity_recall_prompt.format(\n            text=text,\n        )\n        result = await self.llm.generate(\n            prompt=p_value,\n            callbacks=callbacks,\n        )\n\n        result_text = result.generations[0][0].text\n        answer = await _output_parser.aparse(\n            result_text, p_value, self.llm, self.max_retries\n        )\n        if answer is None:\n            return ContextEntitiesResponse(entities=[])\n\n        return answer\n\n    async def _single_turn_ascore(\n        self, sample: SingleTurnSample, callbacks: Callbacks\n    ) -&gt; float:\n        row = sample.to_dict()\n        return await self._ascore(row, callbacks)\n\n    async def _ascore(\n        self,\n        row: Dict,\n        callbacks: Callbacks,\n    ) -&gt; float:\n        ground_truth, contexts = row[\"reference\"], row[\"retrieved_contexts\"]\n        ground_truth = await self.get_entities(ground_truth, callbacks=callbacks)\n        contexts = await self.get_entities(\"\\n\".join(contexts), callbacks=callbacks)\n        if ground_truth is None or contexts is None:\n            return np.nan\n        return self._compute_score(ground_truth.entities, contexts.entities)\n\n    def save(self, cache_dir: str | None = None) -&gt; None:\n        return self.context_entity_recall_prompt.save(cache_dir)\n</code></pre>"},{"location":"references/metrics/__init__/#ragas.metrics.Faithfulness","title":"<code>Faithfulness</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetricWithLLM</code>, <code>SingleTurnMetric</code></p> Source code in <code>src/ragas/metrics/_faithfulness.py</code> <pre><code>@dataclass\nclass Faithfulness(MetricWithLLM, SingleTurnMetric):\n    name: str = \"faithfulness\"  # type: ignore\n    _required_columns: t.Dict[MetricType, t.Set[str]] = field(\n        default_factory=lambda: {\n            MetricType.SINGLE_TURN: {\n                \"user_input\",\n                \"response\",\n                \"retrieved_contexts\",\n            }\n        }\n    )\n    nli_statements_message: Prompt = field(\n        default_factory=lambda: NLI_STATEMENTS_MESSAGE\n    )\n    statement_prompt: Prompt = field(default_factory=lambda: LONG_FORM_ANSWER_PROMPT)\n    sentence_segmenter: t.Optional[HasSegmentMethod] = None\n    max_retries: int = 1\n    _reproducibility: int = 1\n\n    @property\n    def reproducibility(self):\n        return self._reproducibility\n\n    @reproducibility.setter\n    def reproducibility(self, value):\n        if value &lt; 1:\n            logger.warning(\"reproducibility cannot be less than 1, setting to 1\")\n            value = 1\n        elif value % 2 == 0:\n            logger.warning(\n                \"reproducibility level cannot be set to even number, setting to odd\"\n            )\n            value += 1\n        self._reproducibility = value\n\n    def __post_init__(self):\n        if self.sentence_segmenter is None:\n            language = self.nli_statements_message.language\n            self.sentence_segmenter = get_segmenter(language=language, clean=False)\n\n    def _create_nli_prompt(self, row: t.Dict, statements: t.List[str]) -&gt; PromptValue:\n        assert self.llm is not None, \"llm must be set to compute score\"\n\n        contexts = row[\"retrieved_contexts\"]\n        # check if the statements are support in the contexts\n        contexts_str: str = \"\\n\".join(contexts)\n        statements_str: str = json.dumps(statements, ensure_ascii=False)\n        prompt_value = self.nli_statements_message.format(\n            context=contexts_str, statements=statements_str\n        )\n        return prompt_value\n\n    def _create_statements_prompt(self, row: t.Dict) -&gt; PromptValue:\n        assert self.sentence_segmenter is not None, \"sentence_segmenter is not set\"\n\n        text, question = row[\"response\"], row[\"user_input\"]\n        sentences = self.sentence_segmenter.segment(text)\n        sentences = [\n            sentence for sentence in sentences if sentence.strip().endswith(\".\")\n        ]\n        sentences = \"\\n\".join([f\"{i}:{x}\" for i, x in enumerate(sentences)])\n        prompt_value = self.statement_prompt.format(\n            question=question, answer=text, sentences=sentences\n        )\n        return prompt_value\n\n    def _compute_score(self, answers: StatementFaithfulnessAnswers):\n        # check the verdicts and compute the score\n        faithful_statements = sum(\n            1 if answer.verdict else 0 for answer in answers.model_dump()\n        )\n        num_statements = len(answers.model_dump())\n        if num_statements:\n            score = faithful_statements / num_statements\n        else:\n            logger.warning(\"No statements were generated from the answer.\")\n            score = np.nan\n\n        return score\n\n    async def _single_turn_ascore(\n        self, sample: SingleTurnSample, callbacks: Callbacks\n    ) -&gt; float:\n        row = sample.to_dict()\n        return await self._ascore(row, callbacks)\n\n    async def _ascore(self: t.Self, row: t.Dict, callbacks: Callbacks) -&gt; float:\n        \"\"\"\n        returns the NLI score for each (q, c, a) pair\n        \"\"\"\n        assert self.llm is not None, \"LLM is not set\"\n\n        p_value = self._create_statements_prompt(row)\n        statements = await self.llm.generate(\n            p_value,\n            callbacks=callbacks,\n        )\n        statements = await _statements_output_parser.aparse(\n            statements.generations[0][0].text, p_value, self.llm, self.max_retries\n        )\n\n        if statements is None:\n            return np.nan\n\n        statements = [item[\"simpler_statements\"] for item in statements.model_dump()]\n        statements = [item for sublist in statements for item in sublist]\n\n        assert isinstance(statements, t.List), \"statements must be a list\"\n\n        p_value = self._create_nli_prompt(row, statements)\n        nli_result = await self.llm.generate(\n            p_value,\n            callbacks=callbacks,\n            n=self._reproducibility,\n        )\n\n        nli_result_text = [\n            nli_result.generations[0][i].text for i in range(self._reproducibility)\n        ]\n        faithfulness_list = [\n            await _faithfulness_output_parser.aparse(\n                text, p_value, self.llm, self.max_retries\n            )\n            for text in nli_result_text\n        ]\n\n        faithfulness_list = [\n            faith.model_dump() for faith in faithfulness_list if faith is not None\n        ]\n\n        if faithfulness_list:\n            faithfulness_list = ensembler.from_discrete(\n                faithfulness_list,\n                \"verdict\",\n            )\n\n            faithfulness_list = StatementFaithfulnessAnswers.parse_obj(\n                faithfulness_list\n            )\n        else:\n            return np.nan\n\n        return self._compute_score(faithfulness_list)\n\n    def adapt(self, language: str, cache_dir: t.Optional[str] = None) -&gt; None:\n        assert self.llm is not None, \"LLM is not set\"\n\n        logger.info(f\"Adapting Faithfulness metric to {language}\")\n\n        self.nli_statements_message = self.nli_statements_message.adapt(\n            language, self.llm, cache_dir\n        )\n        self.statement_prompt = self.statement_prompt.adapt(\n            language, self.llm, cache_dir\n        )\n\n        self.sentence_segmenter = get_segmenter(language=language, clean=False)\n\n    def save(self, cache_dir: t.Optional[str] = None) -&gt; None:\n        self.nli_statements_message.save(cache_dir)\n        self.statement_prompt.save(cache_dir)\n</code></pre>"},{"location":"references/metrics/__init__/#ragas.metrics.FaithfulnesswithHHEM","title":"<code>FaithfulnesswithHHEM</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Faithfulness</code></p> Source code in <code>src/ragas/metrics/_faithfulness.py</code> <pre><code>@dataclass\nclass FaithfulnesswithHHEM(Faithfulness):\n    name: str = \"faithfulness_with_hhem\"  # type: ignore\n    device: str = \"cpu\"\n    batch_size: int = 10\n\n    def __post_init__(self):\n        try:\n            from transformers import AutoModelForSequenceClassification\n        except ImportError:\n            raise ImportError(\n                \"Huggingface transformers must be installed to use this feature, try `pip install transformers`\"\n            )\n        self.nli_classifier = AutoModelForSequenceClassification.from_pretrained(\n            \"vectara/hallucination_evaluation_model\", trust_remote_code=True\n        )\n        self.nli_classifier.to(self.device)\n        super().__post_init__()\n\n    def _create_pairs(\n        self, row: t.Dict, statements: t.List[str]\n    ) -&gt; t.List[t.Tuple[str, str]]:\n        \"\"\"\n        create pairs of (question, answer) from the row\n        \"\"\"\n        premise = \"\\n\".join(row[\"retrieved_contexts\"])\n        pairs = [(premise, statement) for statement in statements]\n        return pairs\n\n    def _create_batch(\n        self, pairs: t.List[t.Tuple[str, str]]\n    ) -&gt; t.Generator[t.List[t.Tuple[str, str]], None, None]:\n        length_of_pairs = len(pairs)\n        for ndx in range(0, length_of_pairs, self.batch_size):\n            yield pairs[ndx : min(ndx + self.batch_size, length_of_pairs)]\n\n    async def _ascore(self: t.Self, row: t.Dict, callbacks: Callbacks) -&gt; float:\n        \"\"\"\n        returns the NLI score for each (q, c, a) pair\n        \"\"\"\n        assert self.llm is not None, \"LLM is not set\"\n\n        p_value = self._create_statements_prompt(row)\n        statements = await self.llm.generate(\n            p_value,\n            callbacks=callbacks,\n        )\n        statements = await _statements_output_parser.aparse(\n            statements.generations[0][0].text, p_value, self.llm, self.max_retries\n        )\n\n        if statements is None:\n            return np.nan\n\n        statements = [item[\"simpler_statements\"] for item in statements.model_dump()]\n        statements = [item for sublist in statements for item in sublist]\n\n        assert isinstance(statements, t.List), \"statements must be a list\"\n\n        scores = []\n        pairs = self._create_pairs(row, statements)\n        for input_pairs in self._create_batch(pairs):  # to avoid OOM\n            batch_scores = (\n                self.nli_classifier.predict(input_pairs).cpu().detach().round()\n            )\n            scores += batch_scores\n        return sum(scores) / len(scores)\n</code></pre>"},{"location":"references/metrics/__init__/#ragas.metrics.LLMContextRecall","title":"<code>LLMContextRecall</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetricWithLLM</code>, <code>SingleTurnMetric</code></p> <p>Estimates context recall by estimating TP and FN using annotated answer and retrieved context.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> Source code in <code>src/ragas/metrics/_context_recall.py</code> <pre><code>@dataclass\nclass LLMContextRecall(MetricWithLLM, SingleTurnMetric):\n    \"\"\"\n    Estimates context recall by estimating TP and FN using annotated answer and\n    retrieved context.\n\n    Attributes\n    ----------\n    name : str\n    \"\"\"\n\n    name: str = \"context_recall\"  # type: ignore\n    _required_columns: t.Dict[MetricType, t.Set[str]] = field(\n        default_factory=lambda: {\n            MetricType.SINGLE_TURN: {\n                \"user_input\",\n                \"retrieved_contexts\",\n                \"reference\",\n            }\n        }\n    )\n    context_recall_prompt: Prompt = field(default_factory=lambda: CONTEXT_RECALL_RA)\n    max_retries: int = 1\n    _reproducibility: int = 1\n\n    @property\n    def reproducibility(self):\n        return self._reproducibility\n\n    @reproducibility.setter\n    def reproducibility(self, value):\n        if value &lt; 1:\n            logger.warning(\"reproducibility cannot be less than 1, setting to 1\")\n            value = 1\n        elif value % 2 == 0:\n            logger.warning(\n                \"reproducibility level cannot be set to even number, setting to odd\"\n            )\n            value += 1\n        self._reproducibility = value\n\n    def __post_init__(self) -&gt; None:\n        if self.reproducibility &lt; 1:\n            logger.warning(\"reproducibility cannot be less than 1, setting to 1\")\n            self.reproducibility = 1\n\n    def _create_context_recall_prompt(self, row: t.Dict) -&gt; PromptValue:\n        qstn, ctx, gt = row[\"user_input\"], row[\"retrieved_contexts\"], row[\"reference\"]\n        ctx = \"\\n\".join(ctx) if isinstance(ctx, list) else ctx\n\n        return self.context_recall_prompt.format(question=qstn, context=ctx, answer=gt)\n\n    def _compute_score(self, response: t.Any) -&gt; float:\n        response = [1 if item.attributed else 0 for item in response.__root__]\n        denom = len(response)\n        numerator = sum(response)\n        score = numerator / denom if denom &gt; 0 else np.nan\n\n        if np.isnan(score):\n            logger.warning(\"The LLM did not return a valid classification.\")\n\n        return score\n\n    async def _single_turn_ascore(\n        self, sample: SingleTurnSample, callbacks: Callbacks\n    ) -&gt; float:\n        row = sample.to_dict()\n        return await self._ascore(row, callbacks)\n\n    async def _ascore(self, row: t.Dict, callbacks: Callbacks) -&gt; float:\n        assert self.llm is not None, \"set LLM before use\"\n        p_value = self._create_context_recall_prompt(row)\n        results = await self.llm.generate(\n            p_value,\n            callbacks=callbacks,\n            n=self.reproducibility,\n        )\n        results = [results.generations[0][i].text for i in range(self.reproducibility)]\n\n        answers = [\n            await _output_parser.aparse(text, p_value, self.llm, self.max_retries)\n            for text in results\n        ]\n\n        answers = [answer.dicts() for answer in answers if answer is not None]\n        if all(answer is None for answer in answers):\n            return np.nan\n\n        answers = ensembler.from_discrete(answers, \"attributed\")\n        answers = ContextRecallClassificationAnswers.parse_obj(answers)\n\n        return self._compute_score(answers)\n\n    def adapt(self, language: str, cache_dir: str | None = None) -&gt; None:\n        assert self.llm is not None, \"set LLM before use\"\n\n        logger.info(f\"Adapting Context Recall to {language}\")\n        self.context_recall_prompt = self.context_recall_prompt.adapt(\n            language, self.llm, cache_dir\n        )\n\n    def save(self, cache_dir: str | None = None) -&gt; None:\n        self.context_recall_prompt.save(cache_dir)\n</code></pre>"},{"location":"references/metrics/__init__/#ragas.metrics.NoiseSensitivity","title":"<code>NoiseSensitivity</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetricWithLLM</code>, <code>SingleTurnMetric</code></p> Source code in <code>src/ragas/metrics/_noise_sensitivity.py</code> <pre><code>@dataclass\nclass NoiseSensitivity(MetricWithLLM, SingleTurnMetric):\n    name: str = \"noise_sensitivity\"  # type: ignore\n    focus: t.Literal[\"relevant\", \"irrelevant\"] = \"relevant\"\n    _required_columns: t.Dict[MetricType, t.Set[str]] = field(\n        default_factory=lambda: {\n            MetricType.SINGLE_TURN: {\n                \"user_input\",\n                \"response\",\n                \"reference\",\n                \"retrieved_contexts\",\n            }\n        }\n    )\n    nli_statements_message: Prompt = field(\n        default_factory=lambda: NLI_STATEMENTS_MESSAGE\n    )\n    statement_prompt: Prompt = field(default_factory=lambda: LONG_FORM_ANSWER_PROMPT)\n    sentence_segmenter: t.Optional[HasSegmentMethod] = None\n    max_retries: int = 1\n    _reproducibility: int = 1\n\n    @property\n    def reproducibility(self):\n        return self._reproducibility\n\n    @reproducibility.setter\n    def reproducibility(self, value):\n        if value &lt; 1:\n            logger.warning(\"reproducibility cannot be less than 1, setting to 1\")\n            value = 1\n        elif value % 2 == 0:\n            logger.warning(\n                \"reproducibility level cannot be set to even number, setting to odd\"\n            )\n            value += 1\n        self._reproducibility = value\n\n    def __post_init__(self):\n        if self.sentence_segmenter is None:\n            language = self.nli_statements_message.language\n            self.sentence_segmenter = get_segmenter(language=language, clean=False)\n        if self.focus not in {\"relevant\", \"irrelevant\"}:\n            raise ValueError(\n                f\"Invalid argument passed for 'focus': {self.focus}. Must be 'relevant' or 'irrelevant'.\"\n            )\n        self.name = f\"{self.name}_{self.focus}\"  # type: ignore\n\n    def _create_nli_prompt(self, contexts: str, statements: t.List[str]) -&gt; PromptValue:\n        assert self.llm is not None, \"llm must be set to compute score\"\n\n        statements_str: str = json.dumps(statements, ensure_ascii=False)\n        prompt_value = self.nli_statements_message.format(\n            context=contexts, statements=statements_str\n        )\n        return prompt_value\n\n    def _create_statements_prompt(self, text: str, question: str) -&gt; PromptValue:\n        assert self.sentence_segmenter is not None, \"sentence_segmenter is not set\"\n        # contexts = row[\"contexts\"]\n        sentences = self.sentence_segmenter.segment(text)\n        sentences = [\n            sentence for sentence in sentences if sentence.strip().endswith(\".\")\n        ]\n        sentences = \"\\n\".join([f\"{i}:{x}\" for i, x in enumerate(sentences)])\n        prompt_value = self.statement_prompt.format(\n            question=question, answer=text, sentences=sentences\n        )\n        return prompt_value\n\n    async def _evaluate_statement_faithfulness(\n        self, statements, context: str, callbacks: Callbacks\n    ):\n        assert self.llm is not None, \"LLM is not set\"\n\n        p_value = self._create_nli_prompt(context, statements)\n        nli_result = await self.llm.generate(\n            p_value,\n            callbacks=callbacks,\n            n=self._reproducibility,\n        )\n\n        nli_result_text = [\n            nli_result.generations[0][i].text for i in range(self._reproducibility)\n        ]\n        faithfulness_list = [\n            await _faithfulness_output_parser.aparse(\n                text, p_value, self.llm, self.max_retries\n            )\n            for text in nli_result_text\n        ]\n\n        faithfulness_list = [\n            faith.dicts() for faith in faithfulness_list if faith is not None\n        ]\n\n        if faithfulness_list:\n            faithfulness_list = ensembler.from_discrete(\n                faithfulness_list,\n                \"verdict\",\n            )\n\n            faithfulness_list = StatementFaithfulnessAnswers.model_validate(\n                faithfulness_list\n            )\n\n            verdict_list = [\n                1 if statement.verdict else 0 for statement in faithfulness_list.dicts()\n            ]\n            return np.array(verdict_list)\n        else:\n            return np.nan\n\n    async def _decompose_answer_into_statements(\n        self, text: str, question: str, callbacks: Callbacks\n    ):\n        assert self.llm is not None, \"LLM is not set\"\n\n        p_value = self._create_statements_prompt(text, question)\n\n        if inspect.iscoroutinefunction(self.llm.generate):\n            statements_gen = await self.llm.generate(\n                p_value,\n                callbacks=callbacks,\n            )\n        else:\n            statements_gen = await self.llm.generate(\n                p_value,\n                callbacks=callbacks,\n            )\n\n        # Await the aparse method\n        statements = await _statements_output_parser.aparse(\n            statements_gen.generations[0][0].text,\n            p_value,\n            self.llm,\n            self.max_retries,  # type: ignore\n        )\n\n        if statements is None:\n            return np.nan\n\n        # Ensure statements is not a coroutine before calling dicts()\n        if inspect.iscoroutine(statements):\n            statements = await statements\n\n        # Add error handling and logging\n        if not hasattr(statements, \"dicts\"):\n            logging.error(f\"Unexpected type for statements: {type(statements)}\")\n            logging.error(f\"Statements content: {statements}\")\n            raise AttributeError(\n                f\"'statements' object of type {type(statements)} has no attribute 'dicts'\"\n            )\n\n        statements = [item[\"simpler_statements\"] for item in statements.dicts()]\n        statements = [item for sublist in statements for item in sublist]\n\n        return statements\n\n    def _compute_score(self, answers: t.Dict) -&gt; float:\n        # relevant retrievals\n        relevant_retrieved = np.max(\n            answers[\"retrieved2ground_truth\"], axis=0, keepdims=True\n        )\n        relevant_faithful = np.max(\n            relevant_retrieved &amp; answers[\"retrieved2answer\"], axis=1\n        )\n\n        # irrelevant retrievals\n        irrelevant_retrieved = ~np.max(\n            answers[\"retrieved2ground_truth\"], axis=0, keepdims=True\n        )\n        irrelevant_faithful = np.max(\n            irrelevant_retrieved &amp; answers[\"retrieved2answer\"], axis=1\n        )\n\n        # to keep them exclusive\n        irrelevant_faithful &amp;= ~relevant_faithful\n\n        incorrect = ~answers[\"ground_truth2answer\"]\n        noise_sensitivity_in_relevant = np.mean(relevant_faithful &amp; incorrect)\n        noise_sensitivity_in_irrelevant = np.mean(irrelevant_faithful &amp; incorrect)\n\n        if self.focus == \"irrelevant\":\n            return noise_sensitivity_in_irrelevant\n\n        return noise_sensitivity_in_relevant\n\n    async def _single_turn_ascore(\n        self, sample: SingleTurnSample, callbacks: Callbacks\n    ) -&gt; float:\n        row = sample.to_dict()\n        return await self._ascore(row, callbacks)\n\n    async def _ascore(self: t.Self, row: t.Dict, callbacks: Callbacks) -&gt; float:\n        \"\"\"\n        returns the NLI score for each (q, c, a) pair\n        \"\"\"\n        assert self.llm is not None, \"LLM is not set\"\n\n        gt_statements = await self._decompose_answer_into_statements(\n            row[\"reference\"], row[\"user_input\"], callbacks\n        )\n        ans_statements = await self._decompose_answer_into_statements(\n            row[\"response\"], row[\"user_input\"], callbacks\n        )\n        gt_verdictslist = []\n        ans_verdictslist = []\n\n        for ctx in row[\"retrieved_contexts\"]:\n            verdicts = await self._evaluate_statement_faithfulness(\n                gt_statements, ctx, callbacks\n            )\n            gt_verdictslist.append(verdicts)\n\n            verdicts = await self._evaluate_statement_faithfulness(\n                ans_statements, ctx, callbacks\n            )\n            ans_verdictslist.append(verdicts)\n\n        answers = {}\n        answers[\"retrieved2ground_truth\"] = np.array(gt_verdictslist).T\n        answers[\"retrieved2answer\"] = np.array(ans_verdictslist).T\n        answers[\"ground_truth2answer\"] = await self._evaluate_statement_faithfulness(\n            ans_statements, row[\"reference\"], callbacks\n        )\n        answers[\"ground_truth2answer\"] = np.array([answers[\"ground_truth2answer\"]])\n        answers = {k: v.astype(bool) for k, v in answers.items()}\n        return self._compute_score(answers)\n\n    def adapt(self, language: str, cache_dir: t.Optional[str] = None) -&gt; None:\n        assert self.llm is not None, \"LLM is not set\"\n\n        self.nli_statements_message = self.nli_statements_message.adapt(\n            language, self.llm, cache_dir\n        )\n        self.statement_prompt = self.statement_prompt.adapt(\n            language, self.llm, cache_dir\n        )\n\n        self.sentence_segmenter = get_segmenter(language=language, clean=False)\n\n    def save(self, cache_dir: t.Optional[str] = None) -&gt; None:\n        self.nli_statements_message.save(cache_dir)\n        self.statement_prompt.save(cache_dir)\n</code></pre>"},{"location":"references/metrics/__init__/#ragas.metrics.ResponseRelevancy","title":"<code>ResponseRelevancy</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetricWithLLM</code>, <code>MetricWithEmbeddings</code>, <code>SingleTurnMetric</code></p> <p>Scores the relevancy of the answer according to the given question. Answers with incomplete, redundant or unnecessary information is penalized. Score can range from 0 to 1 with 1 being the best.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>string</code> <p>The name of the metrics</p> <code>strictness</code> <code>int</code> <p>Here indicates the number questions generated per answer. Ideal range between 3 to 5.</p> <code>embeddings</code> <code>Embedding</code> <p>The langchain wrapper of Embedding object. E.g. HuggingFaceEmbeddings('BAAI/bge-base-en')</p> Source code in <code>src/ragas/metrics/_answer_relevance.py</code> <pre><code>@dataclass\nclass ResponseRelevancy(MetricWithLLM, MetricWithEmbeddings, SingleTurnMetric):\n    \"\"\"\n    Scores the relevancy of the answer according to the given question.\n    Answers with incomplete, redundant or unnecessary information is penalized.\n    Score can range from 0 to 1 with 1 being the best.\n\n    Attributes\n    ----------\n    name: string\n        The name of the metrics\n    strictness: int\n        Here indicates the number questions generated per answer.\n        Ideal range between 3 to 5.\n    embeddings: Embedding\n        The langchain wrapper of Embedding object.\n        E.g. HuggingFaceEmbeddings('BAAI/bge-base-en')\n    \"\"\"\n\n    name: str = \"answer_relevancy\"  # type: ignore\n    _required_columns: t.Dict[MetricType, t.Set[str]] = field(\n        default_factory=lambda: {\n            MetricType.SINGLE_TURN: {\n                \"user_input\",\n                \"response\",\n            }\n        }\n    )\n    question_generation: PydanticPrompt = RessponseRelevancePrompt()\n    strictness: int = 3\n\n    def calculate_similarity(self, question: str, generated_questions: list[str]):\n        assert self.embeddings is not None\n        question_vec = np.asarray(self.embeddings.embed_query(question)).reshape(1, -1)\n        gen_question_vec = np.asarray(\n            self.embeddings.embed_documents(generated_questions)\n        ).reshape(len(generated_questions), -1)\n        norm = np.linalg.norm(gen_question_vec, axis=1) * np.linalg.norm(\n            question_vec, axis=1\n        )\n        return (\n            np.dot(gen_question_vec, question_vec.T).reshape(\n                -1,\n            )\n            / norm\n        )\n\n    def _calculate_score(\n        self, answers: t.Sequence[ResponseRelevanceOutput], row: t.Dict\n    ) -&gt; float:\n        question = row[\"user_input\"]\n        gen_questions = [answer.question for answer in answers]\n        committal = np.any([answer.noncommittal for answer in answers])\n        if all(q == \"\" for q in gen_questions):\n            logger.warning(\n                \"Invalid JSON response. Expected dictionary with key 'question'\"\n            )\n            score = np.nan\n        else:\n            cosine_sim = self.calculate_similarity(question, gen_questions)\n            score = cosine_sim.mean() * int(not committal)\n\n        return score\n\n    async def _single_turn_ascore(\n        self, sample: SingleTurnSample, callbacks: Callbacks\n    ) -&gt; float:\n        row = sample.to_dict()\n        return await self._ascore(row, callbacks)\n\n    async def _ascore(self, row: t.Dict, callbacks: Callbacks) -&gt; float:\n        assert self.llm is not None, \"LLM is not set\"\n\n        prompt_input = ResponseRelevanceInput(response=row[\"response\"])\n        responses = []\n        for _ in range(self.strictness):\n            response = await self.question_generation.generate(\n                data=prompt_input,\n                llm=self.llm,\n                callbacks=callbacks,\n            )\n            responses.append(response)\n\n        return self._calculate_score(responses, row)\n</code></pre>"},{"location":"references/metrics/__init__/#ragas.metrics.SemanticSimilarity","title":"<code>SemanticSimilarity</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetricWithLLM</code>, <code>MetricWithEmbeddings</code>, <code>SingleTurnMetric</code></p> <p>Scores the semantic similarity of ground truth with generated answer. cross encoder score is used to quantify semantic similarity. SAS paper: https://arxiv.org/pdf/2108.06130.pdf</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <code>model_name</code> <p>The model to be used for calculating semantic similarity Defaults open-ai-embeddings select cross-encoder model for best results https://huggingface.co/spaces/mteb/leaderboard</p> <code>threshold</code> <code>Optional[float]</code> <p>The threshold if given used to map output to binary Default 0.5</p> Source code in <code>src/ragas/metrics/_answer_similarity.py</code> <pre><code>@dataclass\nclass SemanticSimilarity(MetricWithLLM, MetricWithEmbeddings, SingleTurnMetric):\n    \"\"\"\n    Scores the semantic similarity of ground truth with generated answer.\n    cross encoder score is used to quantify semantic similarity.\n    SAS paper: https://arxiv.org/pdf/2108.06130.pdf\n\n    Attributes\n    ----------\n    name : str\n    model_name:\n        The model to be used for calculating semantic similarity\n        Defaults open-ai-embeddings\n        select cross-encoder model for best results\n        https://huggingface.co/spaces/mteb/leaderboard\n    threshold:\n        The threshold if given used to map output to binary\n        Default 0.5\n    \"\"\"\n\n    name: str = \"answer_similarity\"  # type: ignore\n    _required_columns: t.Dict[MetricType, t.Set[str]] = field(\n        default_factory=lambda: {MetricType.SINGLE_TURN: {\"reference\", \"response\"}}\n    )\n    is_cross_encoder: bool = False\n    threshold: t.Optional[float] = None\n\n    def __post_init__(self: t.Self):\n        # only for cross encoder\n        if isinstance(self.embeddings, HuggingfaceEmbeddings):\n            self.is_cross_encoder = True if self.embeddings.is_cross_encoder else False\n            self.embeddings.encode_kwargs = {\n                **self.embeddings.encode_kwargs,\n            }\n\n    async def _single_turn_ascore(\n        self, sample: SingleTurnSample, callbacks: Callbacks\n    ) -&gt; float:\n        row = sample.to_dict()\n        return await self._ascore(row, callbacks)\n\n    async def _ascore(self: t.Self, row: t.Dict, callbacks: Callbacks) -&gt; float:\n        assert self.embeddings is not None, \"embeddings must be set\"\n\n        ground_truth = t.cast(str, row[\"reference\"])\n        answer = t.cast(str, row[\"response\"])\n\n        if self.is_cross_encoder and isinstance(self.embeddings, HuggingfaceEmbeddings):\n            raise NotImplementedError(\n                \"async score [ascore()] not implemented for HuggingFace embeddings\"\n            )\n        else:\n            embedding_1 = np.array(await self.embeddings.embed_text(ground_truth))\n            embedding_2 = np.array(await self.embeddings.embed_text(answer))\n            # Normalization factors of the above embeddings\n            norms_1 = np.linalg.norm(embedding_1, keepdims=True)\n            norms_2 = np.linalg.norm(embedding_2, keepdims=True)\n            embedding_1_normalized = embedding_1 / norms_1\n            embedding_2_normalized = embedding_2 / norms_2\n            similarity = embedding_1_normalized @ embedding_2_normalized.T\n            score = similarity.flatten()\n\n        assert isinstance(score, np.ndarray), \"Expects ndarray\"\n        if self.threshold:\n            score = score &gt;= self.threshold\n\n        return score.tolist()[0]\n</code></pre>"},{"location":"references/metrics/base/","title":"base","text":"<p>Q - question A - answer: generated_text from RAG pipeline C - contexts: context used for generation G - ground_truth: ground truth answer</p>"},{"location":"references/metrics/base/#ragas.metrics.base.Ensember","title":"<code>Ensember</code>","text":"<p>Combine multiple llm outputs for same input (n&gt;1) to a single output</p> Source code in <code>src/ragas/metrics/base.py</code> <pre><code>class Ensember:\n    \"\"\"\n    Combine multiple llm outputs for same input (n&gt;1) to a single output\n    \"\"\"\n\n    def from_discrete(self, inputs: list[list[t.Dict]], attribute: str):\n        \"\"\"\n        Simple majority voting for binary values, ie [0,0,1] -&gt; 0\n        inputs: list of list of dicts each containing verdict for a single input\n        \"\"\"\n\n        if not isinstance(inputs, list):\n            inputs = [inputs]\n\n        if not all(len(item) == len(inputs[0]) for item in inputs):\n            logger.warning(\"All inputs must have the same length\")\n            return inputs[0]\n\n        if not all(attribute in item for input in inputs for item in input):\n            logger.warning(f\"All inputs must have {attribute} attribute\")\n            return inputs[0]\n\n        if len(inputs) == 1:\n            return inputs[0]\n\n        verdict_agg = []\n        for i in range(len(inputs[0])):\n            item = inputs[0][i]\n            verdicts = [inputs[k][i][attribute] for k in range(len(inputs))]\n            verdict_counts = dict(Counter(verdicts).most_common())\n            item[attribute] = list(verdict_counts.keys())[0]\n            verdict_agg.append(item)\n\n        return verdict_agg\n</code></pre>"},{"location":"references/metrics/base/#ragas.metrics.base.Ensember.from_discrete","title":"<code>from_discrete(inputs, attribute)</code>","text":"<p>Simple majority voting for binary values, ie [0,0,1] -&gt; 0 inputs: list of list of dicts each containing verdict for a single input</p> Source code in <code>src/ragas/metrics/base.py</code> <pre><code>def from_discrete(self, inputs: list[list[t.Dict]], attribute: str):\n    \"\"\"\n    Simple majority voting for binary values, ie [0,0,1] -&gt; 0\n    inputs: list of list of dicts each containing verdict for a single input\n    \"\"\"\n\n    if not isinstance(inputs, list):\n        inputs = [inputs]\n\n    if not all(len(item) == len(inputs[0]) for item in inputs):\n        logger.warning(\"All inputs must have the same length\")\n        return inputs[0]\n\n    if not all(attribute in item for input in inputs for item in input):\n        logger.warning(f\"All inputs must have {attribute} attribute\")\n        return inputs[0]\n\n    if len(inputs) == 1:\n        return inputs[0]\n\n    verdict_agg = []\n    for i in range(len(inputs[0])):\n        item = inputs[0][i]\n        verdicts = [inputs[k][i][attribute] for k in range(len(inputs))]\n        verdict_counts = dict(Counter(verdicts).most_common())\n        item[attribute] = list(verdict_counts.keys())[0]\n        verdict_agg.append(item)\n\n    return verdict_agg\n</code></pre>"},{"location":"references/metrics/base/#ragas.metrics.base.Metric","title":"<code>Metric</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>src/ragas/metrics/base.py</code> <pre><code>@dataclass\nclass Metric(ABC):\n    _required_columns: t.Dict[MetricType, t.Set[str]] = field(default_factory=dict)\n\n    @property\n    @abstractmethod\n    def name(self) -&gt; str: ...\n\n    @property\n    def required_columns(self) -&gt; t.Dict[str, t.Set[str]]:\n        return {k.name: v for k, v in self._required_columns.items()}\n\n    @required_columns.setter\n    def required_columns(self, metric_type: MetricType, columns: t.Set[str]):\n        for column in columns:\n            if column not in VALID_COLUMNS:\n                raise ValueError(\n                    f\"Invalid column '{column}'. Must be one of {VALID_COLUMNS}\"\n                )\n        self._required_columns[metric_type] = columns\n\n    @abstractmethod\n    def init(self, run_config: RunConfig):\n        \"\"\"\n        This method will lazy initialize the model.\n        \"\"\"\n        ...\n\n    def adapt(self, language: str, cache_dir: t.Optional[str] = None) -&gt; None:\n        \"\"\"\n        Adapt the metric to a different language.\n        \"\"\"\n        raise NotImplementedError(\n            \"adapt() is not implemented for {} metric\".format(self.name)\n        )\n\n    def save(self, cache_dir: t.Optional[str] = None) -&gt; None:\n        \"\"\"\n        Save the metric to a path.\n        \"\"\"\n        raise NotImplementedError(\n            \"adapt() is not implemented for {} metric\".format(self.name)\n        )\n\n    @deprecated(\"0.2\", removal=\"0.3\", alternative=\"single_turn_ascore\")\n    def score(self: t.Self, row: t.Dict, callbacks: Callbacks = None) -&gt; float:\n        callbacks = callbacks or []\n        rm, group_cm = new_group(self.name, inputs=row, callbacks=callbacks)\n        try:\n            if is_event_loop_running():\n                try:\n                    import nest_asyncio\n\n                    nest_asyncio.apply()\n                except ImportError:\n                    raise ImportError(\n                        \"It seems like your running this in a jupyter-like environment. Please install nest_asyncio with `pip install nest_asyncio` to make it work.\"\n                    )\n            loop = asyncio.get_event_loop()\n            score = loop.run_until_complete(self._ascore(row=row, callbacks=group_cm))\n        except Exception as e:\n            if not group_cm.ended:\n                rm.on_chain_error(e)\n            raise e\n        else:\n            if not group_cm.ended:\n                rm.on_chain_end({\"output\": score})\n        return score\n\n    @deprecated(\"0.2\", removal=\"0.3\", alternative=\"single_turn_ascore\")\n    async def ascore(\n        self: t.Self,\n        row: t.Dict,\n        callbacks: Callbacks = None,\n        timeout: t.Optional[float] = None,\n    ) -&gt; float:\n        callbacks = callbacks or []\n        rm, group_cm = new_group(self.name, inputs=row, callbacks=callbacks)\n        try:\n            score = await asyncio.wait_for(\n                self._ascore(row=row, callbacks=group_cm),\n                timeout=timeout,\n            )\n        except Exception as e:\n            if not group_cm.ended:\n                rm.on_chain_error(e)\n            raise e\n        else:\n            if not group_cm.ended:\n                rm.on_chain_end({\"output\": score})\n        return score\n\n    @abstractmethod\n    async def _ascore(self, row: t.Dict, callbacks: Callbacks) -&gt; float: ...\n</code></pre>"},{"location":"references/metrics/base/#ragas.metrics.base.Metric.adapt","title":"<code>adapt(language, cache_dir=None)</code>","text":"<p>Adapt the metric to a different language.</p> Source code in <code>src/ragas/metrics/base.py</code> <pre><code>def adapt(self, language: str, cache_dir: t.Optional[str] = None) -&gt; None:\n    \"\"\"\n    Adapt the metric to a different language.\n    \"\"\"\n    raise NotImplementedError(\n        \"adapt() is not implemented for {} metric\".format(self.name)\n    )\n</code></pre>"},{"location":"references/metrics/base/#ragas.metrics.base.Metric.init","title":"<code>init(run_config)</code>  <code>abstractmethod</code>","text":"<p>This method will lazy initialize the model.</p> Source code in <code>src/ragas/metrics/base.py</code> <pre><code>@abstractmethod\ndef init(self, run_config: RunConfig):\n    \"\"\"\n    This method will lazy initialize the model.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"references/metrics/base/#ragas.metrics.base.Metric.save","title":"<code>save(cache_dir=None)</code>","text":"<p>Save the metric to a path.</p> Source code in <code>src/ragas/metrics/base.py</code> <pre><code>def save(self, cache_dir: t.Optional[str] = None) -&gt; None:\n    \"\"\"\n    Save the metric to a path.\n    \"\"\"\n    raise NotImplementedError(\n        \"adapt() is not implemented for {} metric\".format(self.name)\n    )\n</code></pre>"},{"location":"references/metrics/base/#ragas.metrics.base.MetricWithEmbeddings","title":"<code>MetricWithEmbeddings</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Metric</code></p> Source code in <code>src/ragas/metrics/base.py</code> <pre><code>@dataclass\nclass MetricWithEmbeddings(Metric):\n    embeddings: t.Optional[BaseRagasEmbeddings] = None\n\n    def init(self, run_config: RunConfig):\n        \"\"\"\n        Init any models in the metric, this is invoked before evaluate()\n        to load all the models\n        Also check if the api key is valid for OpenAI and AzureOpenAI\n        \"\"\"\n        if self.embeddings is None:\n            raise ValueError(\n                f\"Metric '{self.name}' has no valid embeddings provided (self.embeddings is None). Please initantiate a the metric with an embeddings to run.\"  # noqa\n            )\n        self.embeddings.set_run_config(run_config)\n</code></pre>"},{"location":"references/metrics/base/#ragas.metrics.base.MetricWithEmbeddings.init","title":"<code>init(run_config)</code>","text":"<p>Init any models in the metric, this is invoked before evaluate() to load all the models Also check if the api key is valid for OpenAI and AzureOpenAI</p> Source code in <code>src/ragas/metrics/base.py</code> <pre><code>def init(self, run_config: RunConfig):\n    \"\"\"\n    Init any models in the metric, this is invoked before evaluate()\n    to load all the models\n    Also check if the api key is valid for OpenAI and AzureOpenAI\n    \"\"\"\n    if self.embeddings is None:\n        raise ValueError(\n            f\"Metric '{self.name}' has no valid embeddings provided (self.embeddings is None). Please initantiate a the metric with an embeddings to run.\"  # noqa\n        )\n    self.embeddings.set_run_config(run_config)\n</code></pre>"},{"location":"references/metrics/base/#ragas.metrics.base.MetricWithLLM","title":"<code>MetricWithLLM</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Metric</code>, <code>PromptMixin</code></p> Source code in <code>src/ragas/metrics/base.py</code> <pre><code>@dataclass\nclass MetricWithLLM(Metric, PromptMixin):\n    llm: t.Optional[BaseRagasLLM] = None\n\n    def init(self, run_config: RunConfig):\n        \"\"\"\n        Init any models in the metric, this is invoked before evaluate()\n        to load all the models\n        Also check if the api key is valid for OpenAI and AzureOpenAI\n        \"\"\"\n        if self.llm is None:\n            raise ValueError(\n                f\"Metric '{self.name}' has no valid LLM provided (self.llm is None). Please initantiate a the metric with an LLM to run.\"  # noqa\n            )\n        self.llm.set_run_config(run_config)\n</code></pre>"},{"location":"references/metrics/base/#ragas.metrics.base.MetricWithLLM.init","title":"<code>init(run_config)</code>","text":"<p>Init any models in the metric, this is invoked before evaluate() to load all the models Also check if the api key is valid for OpenAI and AzureOpenAI</p> Source code in <code>src/ragas/metrics/base.py</code> <pre><code>def init(self, run_config: RunConfig):\n    \"\"\"\n    Init any models in the metric, this is invoked before evaluate()\n    to load all the models\n    Also check if the api key is valid for OpenAI and AzureOpenAI\n    \"\"\"\n    if self.llm is None:\n        raise ValueError(\n            f\"Metric '{self.name}' has no valid LLM provided (self.llm is None). Please initantiate a the metric with an LLM to run.\"  # noqa\n        )\n    self.llm.set_run_config(run_config)\n</code></pre>"},{"location":"references/metrics/base/#ragas.metrics.base.get_segmenter","title":"<code>get_segmenter(language='english', clean=False, char_span=False)</code>","text":"<p>Get a sentence segmenter for a given language</p> Source code in <code>src/ragas/metrics/base.py</code> <pre><code>def get_segmenter(\n    language: str = \"english\", clean: bool = False, char_span: bool = False\n):\n    \"\"\"\n    Get a sentence segmenter for a given language\n    \"\"\"\n    language = language.lower()\n    if language not in RAGAS_SUPPORTED_LANGUAGE_CODES:\n        raise ValueError(\n            f\"Language '{language}' not supported. Supported languages: {RAGAS_SUPPORTED_LANGUAGE_CODES.keys()}\"\n        )\n    return Segmenter(\n        language=RAGAS_SUPPORTED_LANGUAGE_CODES[language],\n        clean=clean,\n        char_span=char_span,\n    )\n</code></pre>"},{"location":"references/metrics/utils/","title":"utils","text":""},{"location":"references/metrics/utils/#ragas.metrics.utils.get_available_metrics","title":"<code>get_available_metrics(ds)</code>","text":"<p>Get the available metrics for the given dataset. E.g. if the dataset contains (\"question\", \"answer\", \"contexts\") columns, the available metrics are those that can be evaluated in [qa, qac, qc] mode.</p> Source code in <code>src/ragas/metrics/utils.py</code> <pre><code>def get_available_metrics(ds: EvaluationDataset) -&gt; list[Metric]:\n    \"\"\"\n    Get the available metrics for the given dataset.\n    E.g. if the dataset contains (\"question\", \"answer\", \"contexts\") columns,\n    the available metrics are those that can be evaluated in [qa, qac, qc] mode.\n    \"\"\"\n    available_metrics = []\n    for metric in ALL_METRICS:\n        try:\n            validate_required_columns(ds, [metric])\n            available_metrics.append(metric)\n        except ValueError:\n            pass\n\n    return available_metrics\n</code></pre>"},{"location":"references/prompt/__init__/","title":"prompt","text":""},{"location":"references/prompt/__init__/#ragas.prompt.BasePrompt","title":"<code>BasePrompt</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>src/ragas/prompt/base.py</code> <pre><code>class BasePrompt(ABC):\n    def __init__(self, name: t.Optional[str] = None, language: str = \"english\"):\n        if name is None:\n            self.name = camel_to_snake(self.__class__.__name__)\n\n        _check_if_language_is_supported(language)\n        self.language = language\n\n    @abstractmethod\n    async def generate(\n        self,\n        llm: BaseRagasLLM,\n        data: t.Any,\n        temperature: t.Optional[float] = None,\n        stop: t.Optional[t.List[str]] = None,\n        callbacks: Callbacks = [],\n    ) -&gt; t.Any:\n        \"\"\"\n        Generate a single completion from the prompt.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def generate_multiple(\n        self,\n        llm: BaseRagasLLM,\n        data: t.Any,\n        n: int = 1,\n        temperature: t.Optional[float] = None,\n        stop: t.Optional[t.List[str]] = None,\n        callbacks: Callbacks = [],\n    ) -&gt; t.Any:\n        \"\"\"\n        Generate multiple completions from the prompt.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"references/prompt/__init__/#ragas.prompt.BasePrompt.generate","title":"<code>generate(llm, data, temperature=None, stop=None, callbacks=[])</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Generate a single completion from the prompt.</p> Source code in <code>src/ragas/prompt/base.py</code> <pre><code>@abstractmethod\nasync def generate(\n    self,\n    llm: BaseRagasLLM,\n    data: t.Any,\n    temperature: t.Optional[float] = None,\n    stop: t.Optional[t.List[str]] = None,\n    callbacks: Callbacks = [],\n) -&gt; t.Any:\n    \"\"\"\n    Generate a single completion from the prompt.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/prompt/__init__/#ragas.prompt.BasePrompt.generate_multiple","title":"<code>generate_multiple(llm, data, n=1, temperature=None, stop=None, callbacks=[])</code>  <code>abstractmethod</code>","text":"<p>Generate multiple completions from the prompt.</p> Source code in <code>src/ragas/prompt/base.py</code> <pre><code>@abstractmethod\ndef generate_multiple(\n    self,\n    llm: BaseRagasLLM,\n    data: t.Any,\n    n: int = 1,\n    temperature: t.Optional[float] = None,\n    stop: t.Optional[t.List[str]] = None,\n    callbacks: Callbacks = [],\n) -&gt; t.Any:\n    \"\"\"\n    Generate multiple completions from the prompt.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/prompt/__init__/#ragas.prompt.PydanticPrompt","title":"<code>PydanticPrompt</code>","text":"<p>               Bases: <code>BasePrompt</code>, <code>Generic[InputModel, OutputModel]</code></p> Source code in <code>src/ragas/prompt/pydantic_prompt.py</code> <pre><code>class PydanticPrompt(BasePrompt, t.Generic[InputModel, OutputModel]):\n    input_model: t.Type[InputModel]\n    output_model: t.Type[OutputModel]\n    instruction: str\n    examples: t.List[t.Tuple[InputModel, OutputModel]] = []\n\n    def _generate_instruction(self) -&gt; str:\n        return self.instruction\n\n    def _generate_output_signature(self, indent: int = 4) -&gt; str:\n        return (\n            f\"Please return the output in a JSON format that complies with the \"\n            f\"following schema as specified in JSON Schema and OpenAPI specification:\\n\"\n            f\"{self.output_model.model_json_schema()}\"\n        )\n\n    def _generate_examples(self):\n        if self.examples:\n            example_strings = []\n            for e in self.examples:\n                input_data, output_data = e\n                example_strings.append(\n                    self.instruction\n                    + \"\\n\"\n                    + \"input: \"\n                    + input_data.model_dump_json(indent=4)\n                    + \"\\n\"\n                    + \"output: \"\n                    + output_data.model_dump_json(indent=4)\n                )\n\n            return (\n                \"These are some examples to show how to perform the above instruction\\n\"\n                + \"\\n\\n\".join(example_strings)\n            )\n        # if no examples are provided\n        else:\n            return \"\"\n\n    def to_string(self, data: InputModel) -&gt; str:\n        # this needs a check\n        return (\n            self._generate_instruction()\n            + \"\\n\"\n            + self._generate_output_signature()\n            + \"\\n\"\n            + self._generate_examples()\n            + \"\\nNow perform the above instruction with the following input\\n\"\n            + \"input: \"\n            + data.model_dump_json(indent=4)\n            + \"\\n\"\n            + \"output: \"\n        )\n\n    async def generate(\n        self,\n        llm: BaseRagasLLM,\n        data: InputModel,\n        temperature: t.Optional[float] = None,\n        stop: t.Optional[t.List[str]] = None,\n        callbacks: t.Optional[Callbacks] = None,\n    ) -&gt; OutputModel:\n        \"\"\"\n        Generate a single output using the provided language model and input data.\n\n        This method is a special case of `generate_multiple` where only one output is generated.\n\n        Parameters\n        ----------\n        llm : BaseRagasLLM\n            The language model to use for generation.\n        data : InputModel\n            The input data for generation.\n        temperature : float, optional\n            The temperature parameter for controlling randomness in generation.\n        stop : List[str], optional\n            A list of stop sequences to end generation.\n        callbacks : Callbacks, optional\n            Callback functions to be called during the generation process.\n\n        Returns\n        -------\n        OutputModel\n            The generated output.\n\n        Notes\n        -----\n        This method internally calls `generate_multiple` with `n=1` and returns the first (and only) result.\n        \"\"\"\n        callbacks = callbacks or []\n\n        # this is just a special case of generate_multiple\n        output_single = await self.generate_multiple(\n            llm=llm,\n            data=data,\n            n=1,\n            temperature=temperature,\n            stop=stop,\n            callbacks=callbacks,\n        )\n        return output_single[0]\n\n    async def generate_multiple(\n        self,\n        llm: BaseRagasLLM,\n        data: InputModel,\n        n: int = 1,\n        temperature: t.Optional[float] = None,\n        stop: t.Optional[t.List[str]] = None,\n        callbacks: t.Optional[Callbacks] = None,\n    ) -&gt; t.List[OutputModel]:\n        \"\"\"\n        Generate multiple outputs using the provided language model and input data.\n\n        Parameters\n        ----------\n        llm : BaseRagasLLM\n            The language model to use for generation.\n        data : InputModel\n            The input data for generation.\n        n : int, optional\n            The number of outputs to generate. Default is 1.\n        temperature : float, optional\n            The temperature parameter for controlling randomness in generation.\n        stop : List[str], optional\n            A list of stop sequences to end generation.\n        callbacks : Callbacks, optional\n            Callback functions to be called during the generation process.\n\n        Returns\n        -------\n        List[OutputModel]\n            A list of generated outputs.\n\n        Raises\n        ------\n        RagasOutputParserException\n            If there's an error parsing the output.\n        \"\"\"\n        callbacks = callbacks or []\n        processed_data = self.process_input(data)\n        prompt_rm, prompt_cb = new_group(\n            name=self.name,\n            inputs={\"data\": processed_data},\n            callbacks=callbacks,\n        )\n        prompt_value = PromptValue(prompt_str=self.to_string(processed_data))\n        resp = await llm.generate(\n            prompt_value,\n            n=n,\n            temperature=temperature,\n            stop=stop,\n            callbacks=prompt_cb,\n        )\n\n        output_models = []\n        parser = RagasOutputParser(pydantic_object=self.output_model)\n        for i in range(n):\n            output_string = resp.generations[0][i].text\n            try:\n                answer = await parser.parse_output_string(\n                    output_string=output_string,\n                    prompt_value=prompt_value,\n                    llm=llm,\n                    callbacks=prompt_cb,\n                    max_retries=3,\n                )\n                processed_output = self.process_output(answer, data)  # type: ignore\n                output_models.append(processed_output)\n            except RagasOutputParserException as e:\n                prompt_rm.on_chain_error(error=e)\n                logger.error(\"Prompt %s failed to parse output: %s\", self.name, e)\n                raise e\n\n        prompt_rm.on_chain_end({\"output\": output_models})\n        return output_models\n\n    def process_input(self, input: InputModel) -&gt; InputModel:\n        return input\n\n    def process_output(self, output: OutputModel, input: InputModel) -&gt; OutputModel:\n        return output\n\n    async def adapt(\n        self, target_language: str, llm: BaseRagasLLM\n    ) -&gt; \"PydanticPrompt[InputModel, OutputModel]\":\n        \"\"\"\n        Adapt the prompt to a new language.\n        \"\"\"\n\n        # throws ValueError if language is not supported\n        _check_if_language_is_supported(target_language)\n\n        strings = get_all_strings(self.examples)\n        translated_strings = await translate_statements_prompt.generate(\n            llm=llm,\n            data=ToTranslate(target_language=target_language, statements=strings),\n        )\n\n        translated_examples = update_strings(\n            obj=self.examples,\n            old_strings=strings,\n            new_strings=translated_strings.statements,\n        )\n\n        new_prompt = copy.deepcopy(self)\n        new_prompt.examples = translated_examples\n        new_prompt.language = target_language\n        return new_prompt\n</code></pre>"},{"location":"references/prompt/__init__/#ragas.prompt.PydanticPrompt.adapt","title":"<code>adapt(target_language, llm)</code>  <code>async</code>","text":"<p>Adapt the prompt to a new language.</p> Source code in <code>src/ragas/prompt/pydantic_prompt.py</code> <pre><code>async def adapt(\n    self, target_language: str, llm: BaseRagasLLM\n) -&gt; \"PydanticPrompt[InputModel, OutputModel]\":\n    \"\"\"\n    Adapt the prompt to a new language.\n    \"\"\"\n\n    # throws ValueError if language is not supported\n    _check_if_language_is_supported(target_language)\n\n    strings = get_all_strings(self.examples)\n    translated_strings = await translate_statements_prompt.generate(\n        llm=llm,\n        data=ToTranslate(target_language=target_language, statements=strings),\n    )\n\n    translated_examples = update_strings(\n        obj=self.examples,\n        old_strings=strings,\n        new_strings=translated_strings.statements,\n    )\n\n    new_prompt = copy.deepcopy(self)\n    new_prompt.examples = translated_examples\n    new_prompt.language = target_language\n    return new_prompt\n</code></pre>"},{"location":"references/prompt/__init__/#ragas.prompt.PydanticPrompt.generate","title":"<code>generate(llm, data, temperature=None, stop=None, callbacks=None)</code>  <code>async</code>","text":"<p>Generate a single output using the provided language model and input data.</p> <p>This method is a special case of <code>generate_multiple</code> where only one output is generated.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>BaseRagasLLM</code> <p>The language model to use for generation.</p> required <code>data</code> <code>InputModel</code> <p>The input data for generation.</p> required <code>temperature</code> <code>float</code> <p>The temperature parameter for controlling randomness in generation.</p> <code>None</code> <code>stop</code> <code>List[str]</code> <p>A list of stop sequences to end generation.</p> <code>None</code> <code>callbacks</code> <code>Callbacks</code> <p>Callback functions to be called during the generation process.</p> <code>None</code> <p>Returns:</p> Type Description <code>OutputModel</code> <p>The generated output.</p> Notes <p>This method internally calls <code>generate_multiple</code> with <code>n=1</code> and returns the first (and only) result.</p> Source code in <code>src/ragas/prompt/pydantic_prompt.py</code> <pre><code>async def generate(\n    self,\n    llm: BaseRagasLLM,\n    data: InputModel,\n    temperature: t.Optional[float] = None,\n    stop: t.Optional[t.List[str]] = None,\n    callbacks: t.Optional[Callbacks] = None,\n) -&gt; OutputModel:\n    \"\"\"\n    Generate a single output using the provided language model and input data.\n\n    This method is a special case of `generate_multiple` where only one output is generated.\n\n    Parameters\n    ----------\n    llm : BaseRagasLLM\n        The language model to use for generation.\n    data : InputModel\n        The input data for generation.\n    temperature : float, optional\n        The temperature parameter for controlling randomness in generation.\n    stop : List[str], optional\n        A list of stop sequences to end generation.\n    callbacks : Callbacks, optional\n        Callback functions to be called during the generation process.\n\n    Returns\n    -------\n    OutputModel\n        The generated output.\n\n    Notes\n    -----\n    This method internally calls `generate_multiple` with `n=1` and returns the first (and only) result.\n    \"\"\"\n    callbacks = callbacks or []\n\n    # this is just a special case of generate_multiple\n    output_single = await self.generate_multiple(\n        llm=llm,\n        data=data,\n        n=1,\n        temperature=temperature,\n        stop=stop,\n        callbacks=callbacks,\n    )\n    return output_single[0]\n</code></pre>"},{"location":"references/prompt/__init__/#ragas.prompt.PydanticPrompt.generate_multiple","title":"<code>generate_multiple(llm, data, n=1, temperature=None, stop=None, callbacks=None)</code>  <code>async</code>","text":"<p>Generate multiple outputs using the provided language model and input data.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>BaseRagasLLM</code> <p>The language model to use for generation.</p> required <code>data</code> <code>InputModel</code> <p>The input data for generation.</p> required <code>n</code> <code>int</code> <p>The number of outputs to generate. Default is 1.</p> <code>1</code> <code>temperature</code> <code>float</code> <p>The temperature parameter for controlling randomness in generation.</p> <code>None</code> <code>stop</code> <code>List[str]</code> <p>A list of stop sequences to end generation.</p> <code>None</code> <code>callbacks</code> <code>Callbacks</code> <p>Callback functions to be called during the generation process.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[OutputModel]</code> <p>A list of generated outputs.</p> <p>Raises:</p> Type Description <code>RagasOutputParserException</code> <p>If there's an error parsing the output.</p> Source code in <code>src/ragas/prompt/pydantic_prompt.py</code> <pre><code>async def generate_multiple(\n    self,\n    llm: BaseRagasLLM,\n    data: InputModel,\n    n: int = 1,\n    temperature: t.Optional[float] = None,\n    stop: t.Optional[t.List[str]] = None,\n    callbacks: t.Optional[Callbacks] = None,\n) -&gt; t.List[OutputModel]:\n    \"\"\"\n    Generate multiple outputs using the provided language model and input data.\n\n    Parameters\n    ----------\n    llm : BaseRagasLLM\n        The language model to use for generation.\n    data : InputModel\n        The input data for generation.\n    n : int, optional\n        The number of outputs to generate. Default is 1.\n    temperature : float, optional\n        The temperature parameter for controlling randomness in generation.\n    stop : List[str], optional\n        A list of stop sequences to end generation.\n    callbacks : Callbacks, optional\n        Callback functions to be called during the generation process.\n\n    Returns\n    -------\n    List[OutputModel]\n        A list of generated outputs.\n\n    Raises\n    ------\n    RagasOutputParserException\n        If there's an error parsing the output.\n    \"\"\"\n    callbacks = callbacks or []\n    processed_data = self.process_input(data)\n    prompt_rm, prompt_cb = new_group(\n        name=self.name,\n        inputs={\"data\": processed_data},\n        callbacks=callbacks,\n    )\n    prompt_value = PromptValue(prompt_str=self.to_string(processed_data))\n    resp = await llm.generate(\n        prompt_value,\n        n=n,\n        temperature=temperature,\n        stop=stop,\n        callbacks=prompt_cb,\n    )\n\n    output_models = []\n    parser = RagasOutputParser(pydantic_object=self.output_model)\n    for i in range(n):\n        output_string = resp.generations[0][i].text\n        try:\n            answer = await parser.parse_output_string(\n                output_string=output_string,\n                prompt_value=prompt_value,\n                llm=llm,\n                callbacks=prompt_cb,\n                max_retries=3,\n            )\n            processed_output = self.process_output(answer, data)  # type: ignore\n            output_models.append(processed_output)\n        except RagasOutputParserException as e:\n            prompt_rm.on_chain_error(error=e)\n            logger.error(\"Prompt %s failed to parse output: %s\", self.name, e)\n            raise e\n\n    prompt_rm.on_chain_end({\"output\": output_models})\n    return output_models\n</code></pre>"},{"location":"references/prompt/__init__/#ragas.prompt.StringPrompt","title":"<code>StringPrompt</code>","text":"<p>               Bases: <code>BasePrompt</code></p> <p>A simple prompt that can be formatted with additional data using f-string syntax.</p> <p>This prompt is a simpler alternative to PydanticPrompt for those who prefer a more flexible approach without the need for a Pydantic model.</p> <p>Parameters:</p> Name Type Description Default <code>instruction</code> <code>str</code> <p>The instruction string that can be formatted with additional data.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ragas.prompt import string_prompt\n&gt;&gt;&gt; await prompt.generate(llm=llm, data={\"category\": \"commerce\"})\n</code></pre> Source code in <code>src/ragas/prompt/base.py</code> <pre><code>class StringPrompt(BasePrompt):\n    \"\"\"\n    A simple prompt that can be formatted with additional data using f-string syntax.\n\n    This prompt is a simpler alternative to PydanticPrompt for those who prefer a more\n    flexible approach without the need for a Pydantic model.\n\n    Parameters\n    ----------\n    instruction : str\n        The instruction string that can be formatted with additional data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from ragas.prompt import string_prompt\n    &gt;&gt;&gt; await prompt.generate(llm=llm, data={\"category\": \"commerce\"})\n    \"\"\"\n\n    async def generate(\n        self,\n        llm: BaseRagasLLM,\n        data: str,\n        temperature: t.Optional[float] = None,\n        stop: t.Optional[t.List[str]] = None,\n        callbacks: Callbacks = [],\n    ) -&gt; str:\n        \"\"\"\n        Generate text based on the instruction and provided data.\n\n        Parameters\n        ----------\n        llm : BaseRagasLLM\n            The language model to use for text generation.\n        data : Optional[Dict[str, Any]], optional\n            The data to format the instruction with, by default None.\n        n : int, optional\n            The number of completions to generate, by default 1.\n        temperature : Optional[float], optional\n            The temperature for text generation, by default None.\n        stop : Optional[List[str]], optional\n            The stop sequences for text generation, by default None.\n        callbacks : Callbacks, optional\n            The callbacks to use during text generation, by default [].\n\n        Returns\n        -------\n        str\n            The generated text.\n        \"\"\"\n        llm_result = await llm.agenerate_text(\n            PromptValue(prompt_str=data),\n            n=1,\n            temperature=temperature,\n            stop=stop,\n            callbacks=callbacks,\n        )\n        return llm_result.generations[0][0].text\n\n    async def generate_multiple(\n        self,\n        llm: BaseRagasLLM,\n        data: str,\n        n: int = 1,\n        temperature: t.Optional[float] = None,\n        stop: t.Optional[t.List[str]] = None,\n        callbacks: Callbacks = [],\n    ) -&gt; t.List[str]:\n        return [\n            await self.generate(llm, data, temperature, stop, callbacks)\n            for _ in range(n)\n        ]\n</code></pre>"},{"location":"references/prompt/__init__/#ragas.prompt.StringPrompt.generate","title":"<code>generate(llm, data, temperature=None, stop=None, callbacks=[])</code>  <code>async</code>","text":"<p>Generate text based on the instruction and provided data.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>BaseRagasLLM</code> <p>The language model to use for text generation.</p> required <code>data</code> <code>Optional[Dict[str, Any]]</code> <p>The data to format the instruction with, by default None.</p> required <code>n</code> <code>int</code> <p>The number of completions to generate, by default 1.</p> required <code>temperature</code> <code>Optional[float]</code> <p>The temperature for text generation, by default None.</p> <code>None</code> <code>stop</code> <code>Optional[List[str]]</code> <p>The stop sequences for text generation, by default None.</p> <code>None</code> <code>callbacks</code> <code>Callbacks</code> <p>The callbacks to use during text generation, by default [].</p> <code>[]</code> <p>Returns:</p> Type Description <code>str</code> <p>The generated text.</p> Source code in <code>src/ragas/prompt/base.py</code> <pre><code>async def generate(\n    self,\n    llm: BaseRagasLLM,\n    data: str,\n    temperature: t.Optional[float] = None,\n    stop: t.Optional[t.List[str]] = None,\n    callbacks: Callbacks = [],\n) -&gt; str:\n    \"\"\"\n    Generate text based on the instruction and provided data.\n\n    Parameters\n    ----------\n    llm : BaseRagasLLM\n        The language model to use for text generation.\n    data : Optional[Dict[str, Any]], optional\n        The data to format the instruction with, by default None.\n    n : int, optional\n        The number of completions to generate, by default 1.\n    temperature : Optional[float], optional\n        The temperature for text generation, by default None.\n    stop : Optional[List[str]], optional\n        The stop sequences for text generation, by default None.\n    callbacks : Callbacks, optional\n        The callbacks to use during text generation, by default [].\n\n    Returns\n    -------\n    str\n        The generated text.\n    \"\"\"\n    llm_result = await llm.agenerate_text(\n        PromptValue(prompt_str=data),\n        n=1,\n        temperature=temperature,\n        stop=stop,\n        callbacks=callbacks,\n    )\n    return llm_result.generations[0][0].text\n</code></pre>"},{"location":"references/prompt/base/","title":"base","text":""},{"location":"references/prompt/base/#ragas.prompt.base.BasePrompt","title":"<code>BasePrompt</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>src/ragas/prompt/base.py</code> <pre><code>class BasePrompt(ABC):\n    def __init__(self, name: t.Optional[str] = None, language: str = \"english\"):\n        if name is None:\n            self.name = camel_to_snake(self.__class__.__name__)\n\n        _check_if_language_is_supported(language)\n        self.language = language\n\n    @abstractmethod\n    async def generate(\n        self,\n        llm: BaseRagasLLM,\n        data: t.Any,\n        temperature: t.Optional[float] = None,\n        stop: t.Optional[t.List[str]] = None,\n        callbacks: Callbacks = [],\n    ) -&gt; t.Any:\n        \"\"\"\n        Generate a single completion from the prompt.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def generate_multiple(\n        self,\n        llm: BaseRagasLLM,\n        data: t.Any,\n        n: int = 1,\n        temperature: t.Optional[float] = None,\n        stop: t.Optional[t.List[str]] = None,\n        callbacks: Callbacks = [],\n    ) -&gt; t.Any:\n        \"\"\"\n        Generate multiple completions from the prompt.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"references/prompt/base/#ragas.prompt.base.BasePrompt.generate","title":"<code>generate(llm, data, temperature=None, stop=None, callbacks=[])</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Generate a single completion from the prompt.</p> Source code in <code>src/ragas/prompt/base.py</code> <pre><code>@abstractmethod\nasync def generate(\n    self,\n    llm: BaseRagasLLM,\n    data: t.Any,\n    temperature: t.Optional[float] = None,\n    stop: t.Optional[t.List[str]] = None,\n    callbacks: Callbacks = [],\n) -&gt; t.Any:\n    \"\"\"\n    Generate a single completion from the prompt.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/prompt/base/#ragas.prompt.base.BasePrompt.generate_multiple","title":"<code>generate_multiple(llm, data, n=1, temperature=None, stop=None, callbacks=[])</code>  <code>abstractmethod</code>","text":"<p>Generate multiple completions from the prompt.</p> Source code in <code>src/ragas/prompt/base.py</code> <pre><code>@abstractmethod\ndef generate_multiple(\n    self,\n    llm: BaseRagasLLM,\n    data: t.Any,\n    n: int = 1,\n    temperature: t.Optional[float] = None,\n    stop: t.Optional[t.List[str]] = None,\n    callbacks: Callbacks = [],\n) -&gt; t.Any:\n    \"\"\"\n    Generate multiple completions from the prompt.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/prompt/base/#ragas.prompt.base.StringPrompt","title":"<code>StringPrompt</code>","text":"<p>               Bases: <code>BasePrompt</code></p> <p>A simple prompt that can be formatted with additional data using f-string syntax.</p> <p>This prompt is a simpler alternative to PydanticPrompt for those who prefer a more flexible approach without the need for a Pydantic model.</p> <p>Parameters:</p> Name Type Description Default <code>instruction</code> <code>str</code> <p>The instruction string that can be formatted with additional data.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ragas.prompt import string_prompt\n&gt;&gt;&gt; await prompt.generate(llm=llm, data={\"category\": \"commerce\"})\n</code></pre> Source code in <code>src/ragas/prompt/base.py</code> <pre><code>class StringPrompt(BasePrompt):\n    \"\"\"\n    A simple prompt that can be formatted with additional data using f-string syntax.\n\n    This prompt is a simpler alternative to PydanticPrompt for those who prefer a more\n    flexible approach without the need for a Pydantic model.\n\n    Parameters\n    ----------\n    instruction : str\n        The instruction string that can be formatted with additional data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from ragas.prompt import string_prompt\n    &gt;&gt;&gt; await prompt.generate(llm=llm, data={\"category\": \"commerce\"})\n    \"\"\"\n\n    async def generate(\n        self,\n        llm: BaseRagasLLM,\n        data: str,\n        temperature: t.Optional[float] = None,\n        stop: t.Optional[t.List[str]] = None,\n        callbacks: Callbacks = [],\n    ) -&gt; str:\n        \"\"\"\n        Generate text based on the instruction and provided data.\n\n        Parameters\n        ----------\n        llm : BaseRagasLLM\n            The language model to use for text generation.\n        data : Optional[Dict[str, Any]], optional\n            The data to format the instruction with, by default None.\n        n : int, optional\n            The number of completions to generate, by default 1.\n        temperature : Optional[float], optional\n            The temperature for text generation, by default None.\n        stop : Optional[List[str]], optional\n            The stop sequences for text generation, by default None.\n        callbacks : Callbacks, optional\n            The callbacks to use during text generation, by default [].\n\n        Returns\n        -------\n        str\n            The generated text.\n        \"\"\"\n        llm_result = await llm.agenerate_text(\n            PromptValue(prompt_str=data),\n            n=1,\n            temperature=temperature,\n            stop=stop,\n            callbacks=callbacks,\n        )\n        return llm_result.generations[0][0].text\n\n    async def generate_multiple(\n        self,\n        llm: BaseRagasLLM,\n        data: str,\n        n: int = 1,\n        temperature: t.Optional[float] = None,\n        stop: t.Optional[t.List[str]] = None,\n        callbacks: Callbacks = [],\n    ) -&gt; t.List[str]:\n        return [\n            await self.generate(llm, data, temperature, stop, callbacks)\n            for _ in range(n)\n        ]\n</code></pre>"},{"location":"references/prompt/base/#ragas.prompt.base.StringPrompt.generate","title":"<code>generate(llm, data, temperature=None, stop=None, callbacks=[])</code>  <code>async</code>","text":"<p>Generate text based on the instruction and provided data.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>BaseRagasLLM</code> <p>The language model to use for text generation.</p> required <code>data</code> <code>Optional[Dict[str, Any]]</code> <p>The data to format the instruction with, by default None.</p> required <code>n</code> <code>int</code> <p>The number of completions to generate, by default 1.</p> required <code>temperature</code> <code>Optional[float]</code> <p>The temperature for text generation, by default None.</p> <code>None</code> <code>stop</code> <code>Optional[List[str]]</code> <p>The stop sequences for text generation, by default None.</p> <code>None</code> <code>callbacks</code> <code>Callbacks</code> <p>The callbacks to use during text generation, by default [].</p> <code>[]</code> <p>Returns:</p> Type Description <code>str</code> <p>The generated text.</p> Source code in <code>src/ragas/prompt/base.py</code> <pre><code>async def generate(\n    self,\n    llm: BaseRagasLLM,\n    data: str,\n    temperature: t.Optional[float] = None,\n    stop: t.Optional[t.List[str]] = None,\n    callbacks: Callbacks = [],\n) -&gt; str:\n    \"\"\"\n    Generate text based on the instruction and provided data.\n\n    Parameters\n    ----------\n    llm : BaseRagasLLM\n        The language model to use for text generation.\n    data : Optional[Dict[str, Any]], optional\n        The data to format the instruction with, by default None.\n    n : int, optional\n        The number of completions to generate, by default 1.\n    temperature : Optional[float], optional\n        The temperature for text generation, by default None.\n    stop : Optional[List[str]], optional\n        The stop sequences for text generation, by default None.\n    callbacks : Callbacks, optional\n        The callbacks to use during text generation, by default [].\n\n    Returns\n    -------\n    str\n        The generated text.\n    \"\"\"\n    llm_result = await llm.agenerate_text(\n        PromptValue(prompt_str=data),\n        n=1,\n        temperature=temperature,\n        stop=stop,\n        callbacks=callbacks,\n    )\n    return llm_result.generations[0][0].text\n</code></pre>"},{"location":"references/prompt/mixin/","title":"mixin","text":""},{"location":"references/prompt/pydantic_prompt/","title":"pydantic_prompt","text":""},{"location":"references/prompt/pydantic_prompt/#ragas.prompt.pydantic_prompt.PydanticPrompt","title":"<code>PydanticPrompt</code>","text":"<p>               Bases: <code>BasePrompt</code>, <code>Generic[InputModel, OutputModel]</code></p> Source code in <code>src/ragas/prompt/pydantic_prompt.py</code> <pre><code>class PydanticPrompt(BasePrompt, t.Generic[InputModel, OutputModel]):\n    input_model: t.Type[InputModel]\n    output_model: t.Type[OutputModel]\n    instruction: str\n    examples: t.List[t.Tuple[InputModel, OutputModel]] = []\n\n    def _generate_instruction(self) -&gt; str:\n        return self.instruction\n\n    def _generate_output_signature(self, indent: int = 4) -&gt; str:\n        return (\n            f\"Please return the output in a JSON format that complies with the \"\n            f\"following schema as specified in JSON Schema and OpenAPI specification:\\n\"\n            f\"{self.output_model.model_json_schema()}\"\n        )\n\n    def _generate_examples(self):\n        if self.examples:\n            example_strings = []\n            for e in self.examples:\n                input_data, output_data = e\n                example_strings.append(\n                    self.instruction\n                    + \"\\n\"\n                    + \"input: \"\n                    + input_data.model_dump_json(indent=4)\n                    + \"\\n\"\n                    + \"output: \"\n                    + output_data.model_dump_json(indent=4)\n                )\n\n            return (\n                \"These are some examples to show how to perform the above instruction\\n\"\n                + \"\\n\\n\".join(example_strings)\n            )\n        # if no examples are provided\n        else:\n            return \"\"\n\n    def to_string(self, data: InputModel) -&gt; str:\n        # this needs a check\n        return (\n            self._generate_instruction()\n            + \"\\n\"\n            + self._generate_output_signature()\n            + \"\\n\"\n            + self._generate_examples()\n            + \"\\nNow perform the above instruction with the following input\\n\"\n            + \"input: \"\n            + data.model_dump_json(indent=4)\n            + \"\\n\"\n            + \"output: \"\n        )\n\n    async def generate(\n        self,\n        llm: BaseRagasLLM,\n        data: InputModel,\n        temperature: t.Optional[float] = None,\n        stop: t.Optional[t.List[str]] = None,\n        callbacks: t.Optional[Callbacks] = None,\n    ) -&gt; OutputModel:\n        \"\"\"\n        Generate a single output using the provided language model and input data.\n\n        This method is a special case of `generate_multiple` where only one output is generated.\n\n        Parameters\n        ----------\n        llm : BaseRagasLLM\n            The language model to use for generation.\n        data : InputModel\n            The input data for generation.\n        temperature : float, optional\n            The temperature parameter for controlling randomness in generation.\n        stop : List[str], optional\n            A list of stop sequences to end generation.\n        callbacks : Callbacks, optional\n            Callback functions to be called during the generation process.\n\n        Returns\n        -------\n        OutputModel\n            The generated output.\n\n        Notes\n        -----\n        This method internally calls `generate_multiple` with `n=1` and returns the first (and only) result.\n        \"\"\"\n        callbacks = callbacks or []\n\n        # this is just a special case of generate_multiple\n        output_single = await self.generate_multiple(\n            llm=llm,\n            data=data,\n            n=1,\n            temperature=temperature,\n            stop=stop,\n            callbacks=callbacks,\n        )\n        return output_single[0]\n\n    async def generate_multiple(\n        self,\n        llm: BaseRagasLLM,\n        data: InputModel,\n        n: int = 1,\n        temperature: t.Optional[float] = None,\n        stop: t.Optional[t.List[str]] = None,\n        callbacks: t.Optional[Callbacks] = None,\n    ) -&gt; t.List[OutputModel]:\n        \"\"\"\n        Generate multiple outputs using the provided language model and input data.\n\n        Parameters\n        ----------\n        llm : BaseRagasLLM\n            The language model to use for generation.\n        data : InputModel\n            The input data for generation.\n        n : int, optional\n            The number of outputs to generate. Default is 1.\n        temperature : float, optional\n            The temperature parameter for controlling randomness in generation.\n        stop : List[str], optional\n            A list of stop sequences to end generation.\n        callbacks : Callbacks, optional\n            Callback functions to be called during the generation process.\n\n        Returns\n        -------\n        List[OutputModel]\n            A list of generated outputs.\n\n        Raises\n        ------\n        RagasOutputParserException\n            If there's an error parsing the output.\n        \"\"\"\n        callbacks = callbacks or []\n        processed_data = self.process_input(data)\n        prompt_rm, prompt_cb = new_group(\n            name=self.name,\n            inputs={\"data\": processed_data},\n            callbacks=callbacks,\n        )\n        prompt_value = PromptValue(prompt_str=self.to_string(processed_data))\n        resp = await llm.generate(\n            prompt_value,\n            n=n,\n            temperature=temperature,\n            stop=stop,\n            callbacks=prompt_cb,\n        )\n\n        output_models = []\n        parser = RagasOutputParser(pydantic_object=self.output_model)\n        for i in range(n):\n            output_string = resp.generations[0][i].text\n            try:\n                answer = await parser.parse_output_string(\n                    output_string=output_string,\n                    prompt_value=prompt_value,\n                    llm=llm,\n                    callbacks=prompt_cb,\n                    max_retries=3,\n                )\n                processed_output = self.process_output(answer, data)  # type: ignore\n                output_models.append(processed_output)\n            except RagasOutputParserException as e:\n                prompt_rm.on_chain_error(error=e)\n                logger.error(\"Prompt %s failed to parse output: %s\", self.name, e)\n                raise e\n\n        prompt_rm.on_chain_end({\"output\": output_models})\n        return output_models\n\n    def process_input(self, input: InputModel) -&gt; InputModel:\n        return input\n\n    def process_output(self, output: OutputModel, input: InputModel) -&gt; OutputModel:\n        return output\n\n    async def adapt(\n        self, target_language: str, llm: BaseRagasLLM\n    ) -&gt; \"PydanticPrompt[InputModel, OutputModel]\":\n        \"\"\"\n        Adapt the prompt to a new language.\n        \"\"\"\n\n        # throws ValueError if language is not supported\n        _check_if_language_is_supported(target_language)\n\n        strings = get_all_strings(self.examples)\n        translated_strings = await translate_statements_prompt.generate(\n            llm=llm,\n            data=ToTranslate(target_language=target_language, statements=strings),\n        )\n\n        translated_examples = update_strings(\n            obj=self.examples,\n            old_strings=strings,\n            new_strings=translated_strings.statements,\n        )\n\n        new_prompt = copy.deepcopy(self)\n        new_prompt.examples = translated_examples\n        new_prompt.language = target_language\n        return new_prompt\n</code></pre>"},{"location":"references/prompt/pydantic_prompt/#ragas.prompt.pydantic_prompt.PydanticPrompt.adapt","title":"<code>adapt(target_language, llm)</code>  <code>async</code>","text":"<p>Adapt the prompt to a new language.</p> Source code in <code>src/ragas/prompt/pydantic_prompt.py</code> <pre><code>async def adapt(\n    self, target_language: str, llm: BaseRagasLLM\n) -&gt; \"PydanticPrompt[InputModel, OutputModel]\":\n    \"\"\"\n    Adapt the prompt to a new language.\n    \"\"\"\n\n    # throws ValueError if language is not supported\n    _check_if_language_is_supported(target_language)\n\n    strings = get_all_strings(self.examples)\n    translated_strings = await translate_statements_prompt.generate(\n        llm=llm,\n        data=ToTranslate(target_language=target_language, statements=strings),\n    )\n\n    translated_examples = update_strings(\n        obj=self.examples,\n        old_strings=strings,\n        new_strings=translated_strings.statements,\n    )\n\n    new_prompt = copy.deepcopy(self)\n    new_prompt.examples = translated_examples\n    new_prompt.language = target_language\n    return new_prompt\n</code></pre>"},{"location":"references/prompt/pydantic_prompt/#ragas.prompt.pydantic_prompt.PydanticPrompt.generate","title":"<code>generate(llm, data, temperature=None, stop=None, callbacks=None)</code>  <code>async</code>","text":"<p>Generate a single output using the provided language model and input data.</p> <p>This method is a special case of <code>generate_multiple</code> where only one output is generated.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>BaseRagasLLM</code> <p>The language model to use for generation.</p> required <code>data</code> <code>InputModel</code> <p>The input data for generation.</p> required <code>temperature</code> <code>float</code> <p>The temperature parameter for controlling randomness in generation.</p> <code>None</code> <code>stop</code> <code>List[str]</code> <p>A list of stop sequences to end generation.</p> <code>None</code> <code>callbacks</code> <code>Callbacks</code> <p>Callback functions to be called during the generation process.</p> <code>None</code> <p>Returns:</p> Type Description <code>OutputModel</code> <p>The generated output.</p> Notes <p>This method internally calls <code>generate_multiple</code> with <code>n=1</code> and returns the first (and only) result.</p> Source code in <code>src/ragas/prompt/pydantic_prompt.py</code> <pre><code>async def generate(\n    self,\n    llm: BaseRagasLLM,\n    data: InputModel,\n    temperature: t.Optional[float] = None,\n    stop: t.Optional[t.List[str]] = None,\n    callbacks: t.Optional[Callbacks] = None,\n) -&gt; OutputModel:\n    \"\"\"\n    Generate a single output using the provided language model and input data.\n\n    This method is a special case of `generate_multiple` where only one output is generated.\n\n    Parameters\n    ----------\n    llm : BaseRagasLLM\n        The language model to use for generation.\n    data : InputModel\n        The input data for generation.\n    temperature : float, optional\n        The temperature parameter for controlling randomness in generation.\n    stop : List[str], optional\n        A list of stop sequences to end generation.\n    callbacks : Callbacks, optional\n        Callback functions to be called during the generation process.\n\n    Returns\n    -------\n    OutputModel\n        The generated output.\n\n    Notes\n    -----\n    This method internally calls `generate_multiple` with `n=1` and returns the first (and only) result.\n    \"\"\"\n    callbacks = callbacks or []\n\n    # this is just a special case of generate_multiple\n    output_single = await self.generate_multiple(\n        llm=llm,\n        data=data,\n        n=1,\n        temperature=temperature,\n        stop=stop,\n        callbacks=callbacks,\n    )\n    return output_single[0]\n</code></pre>"},{"location":"references/prompt/pydantic_prompt/#ragas.prompt.pydantic_prompt.PydanticPrompt.generate_multiple","title":"<code>generate_multiple(llm, data, n=1, temperature=None, stop=None, callbacks=None)</code>  <code>async</code>","text":"<p>Generate multiple outputs using the provided language model and input data.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>BaseRagasLLM</code> <p>The language model to use for generation.</p> required <code>data</code> <code>InputModel</code> <p>The input data for generation.</p> required <code>n</code> <code>int</code> <p>The number of outputs to generate. Default is 1.</p> <code>1</code> <code>temperature</code> <code>float</code> <p>The temperature parameter for controlling randomness in generation.</p> <code>None</code> <code>stop</code> <code>List[str]</code> <p>A list of stop sequences to end generation.</p> <code>None</code> <code>callbacks</code> <code>Callbacks</code> <p>Callback functions to be called during the generation process.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[OutputModel]</code> <p>A list of generated outputs.</p> <p>Raises:</p> Type Description <code>RagasOutputParserException</code> <p>If there's an error parsing the output.</p> Source code in <code>src/ragas/prompt/pydantic_prompt.py</code> <pre><code>async def generate_multiple(\n    self,\n    llm: BaseRagasLLM,\n    data: InputModel,\n    n: int = 1,\n    temperature: t.Optional[float] = None,\n    stop: t.Optional[t.List[str]] = None,\n    callbacks: t.Optional[Callbacks] = None,\n) -&gt; t.List[OutputModel]:\n    \"\"\"\n    Generate multiple outputs using the provided language model and input data.\n\n    Parameters\n    ----------\n    llm : BaseRagasLLM\n        The language model to use for generation.\n    data : InputModel\n        The input data for generation.\n    n : int, optional\n        The number of outputs to generate. Default is 1.\n    temperature : float, optional\n        The temperature parameter for controlling randomness in generation.\n    stop : List[str], optional\n        A list of stop sequences to end generation.\n    callbacks : Callbacks, optional\n        Callback functions to be called during the generation process.\n\n    Returns\n    -------\n    List[OutputModel]\n        A list of generated outputs.\n\n    Raises\n    ------\n    RagasOutputParserException\n        If there's an error parsing the output.\n    \"\"\"\n    callbacks = callbacks or []\n    processed_data = self.process_input(data)\n    prompt_rm, prompt_cb = new_group(\n        name=self.name,\n        inputs={\"data\": processed_data},\n        callbacks=callbacks,\n    )\n    prompt_value = PromptValue(prompt_str=self.to_string(processed_data))\n    resp = await llm.generate(\n        prompt_value,\n        n=n,\n        temperature=temperature,\n        stop=stop,\n        callbacks=prompt_cb,\n    )\n\n    output_models = []\n    parser = RagasOutputParser(pydantic_object=self.output_model)\n    for i in range(n):\n        output_string = resp.generations[0][i].text\n        try:\n            answer = await parser.parse_output_string(\n                output_string=output_string,\n                prompt_value=prompt_value,\n                llm=llm,\n                callbacks=prompt_cb,\n                max_retries=3,\n            )\n            processed_output = self.process_output(answer, data)  # type: ignore\n            output_models.append(processed_output)\n        except RagasOutputParserException as e:\n            prompt_rm.on_chain_error(error=e)\n            logger.error(\"Prompt %s failed to parse output: %s\", self.name, e)\n            raise e\n\n    prompt_rm.on_chain_end({\"output\": output_models})\n    return output_models\n</code></pre>"},{"location":"references/prompt/utils/","title":"utils","text":""},{"location":"references/prompt/utils/#ragas.prompt.utils.get_all_strings","title":"<code>get_all_strings(obj)</code>","text":"<p>Get all strings in the objects.</p> Source code in <code>src/ragas/prompt/utils.py</code> <pre><code>def get_all_strings(obj: t.Any) -&gt; list[str]:\n    \"\"\"\n    Get all strings in the objects.\n    \"\"\"\n    strings = []\n\n    if isinstance(obj, str):\n        strings.append(obj)\n    elif isinstance(obj, BaseModel):\n        for field_value in obj.model_dump().values():\n            strings.extend(get_all_strings(field_value))\n    elif isinstance(obj, (list, tuple)):\n        for item in obj:\n            strings.extend(get_all_strings(item))\n    elif isinstance(obj, dict):\n        for value in obj.values():\n            strings.extend(get_all_strings(value))\n\n    return strings\n</code></pre>"},{"location":"references/prompt/utils/#ragas.prompt.utils.update_strings","title":"<code>update_strings(obj, old_strings, new_strings)</code>","text":"<p>Replace strings in the object with new strings. Example Usage: <pre><code>old_strings = [\"old1\", \"old2\", \"old3\"]\nnew_strings = [\"new1\", \"new2\", \"new3\"]\nobj = {\"a\": \"old1\", \"b\": \"old2\", \"c\": [\"old1\", \"old2\", \"old3\"], \"d\": {\"e\": \"old2\"}}\nupdate_strings(obj, old_strings, new_strings)\n</code></pre></p> Source code in <code>src/ragas/prompt/utils.py</code> <pre><code>def update_strings(obj: t.Any, old_strings: list[str], new_strings: list[str]) -&gt; t.Any:\n    \"\"\"\n    Replace strings in the object with new strings.\n    Example Usage:\n    ```\n    old_strings = [\"old1\", \"old2\", \"old3\"]\n    new_strings = [\"new1\", \"new2\", \"new3\"]\n    obj = {\"a\": \"old1\", \"b\": \"old2\", \"c\": [\"old1\", \"old2\", \"old3\"], \"d\": {\"e\": \"old2\"}}\n    update_strings(obj, old_strings, new_strings)\n    ```\n    \"\"\"\n    if len(old_strings) != len(new_strings):\n        raise ValueError(\"The number of old and new strings must be the same\")\n\n    def replace_string(s: str) -&gt; str:\n        for old, new in zip(old_strings, new_strings):\n            if s == old:\n                return new\n        return s\n\n    if isinstance(obj, str):\n        return replace_string(obj)\n    elif isinstance(obj, BaseModel):\n        new_obj = copy.deepcopy(obj)\n        for field in new_obj.model_fields:\n            setattr(\n                new_obj,\n                field,\n                update_strings(getattr(new_obj, field), old_strings, new_strings),\n            )\n        return new_obj\n    elif isinstance(obj, list):\n        return [update_strings(item, old_strings, new_strings) for item in obj]\n    elif isinstance(obj, tuple):\n        return tuple(update_strings(item, old_strings, new_strings) for item in obj)\n    elif isinstance(obj, dict):\n        return {k: update_strings(v, old_strings, new_strings) for k, v in obj.items()}\n\n    return copy.deepcopy(obj)\n</code></pre>"},{"location":"references/testset/__init__/","title":"testset","text":""},{"location":"references/testset/__init__/#ragas.testset.TestsetGenerator","title":"<code>TestsetGenerator</code>  <code>dataclass</code>","text":"Source code in <code>src/ragas/testset/synthesizers/generate.py</code> <pre><code>@dataclass\nclass TestsetGenerator:\n    llm: BaseRagasLLM\n    knowledge_graph: KnowledgeGraph = field(default_factory=KnowledgeGraph)\n\n    @classmethod\n    def from_langchain(\n        cls,\n        llm: LangchainLLM,\n        knowledge_graph: t.Optional[KnowledgeGraph] = None,\n    ):\n        knowledge_graph = knowledge_graph or KnowledgeGraph()\n        return cls(LangchainLLMWrapper(llm), knowledge_graph)\n\n    def generate_with_langchain_docs(\n        self,\n        documents: t.Sequence[LCDocument],\n        test_size: int,\n        transforms: t.Optional[Transforms] = None,\n        query_distribution: t.Optional[QueryDistribution] = None,\n        run_config: t.Optional[RunConfig] = None,\n        callbacks: t.Optional[Callbacks] = None,\n        with_debugging_logs=False,\n        raise_exceptions: bool = True,\n    ) -&gt; Testset:\n        transforms = transforms or default_transforms()\n\n        # convert the documents to Ragas nodes\n        nodes = []\n        for doc in documents:\n            node = Node(\n                type=NodeType.DOCUMENT,\n                properties={\n                    \"page_content\": doc.page_content,\n                    \"document_metadata\": doc.metadata,\n                },\n            )\n            nodes.append(node)\n\n        kg = KnowledgeGraph(nodes=nodes)\n\n        # apply transforms and update the knowledge graph\n        apply_transforms(kg, transforms)\n        self.knowledge_graph = kg\n\n        return self.generate(\n            test_size=test_size,\n            query_distribution=query_distribution,\n            run_config=run_config,\n            callbacks=callbacks,\n            with_debugging_logs=with_debugging_logs,\n            raise_exceptions=raise_exceptions,\n        )\n\n    def generate(\n        self,\n        test_size: int,\n        query_distribution: t.Optional[QueryDistribution] = None,\n        run_config: t.Optional[RunConfig] = None,\n        callbacks: t.Optional[Callbacks] = None,\n        with_debugging_logs=False,\n        raise_exceptions: bool = True,\n    ) -&gt; Testset:\n        \"\"\"\n        Generate an evaluation dataset based on given scenarios and parameters.\n\n        Parameters\n        ----------\n        test_size : int\n            The number of samples to generate.\n        query_distribution : Optional[QueryDistribution], optional\n            A list of tuples containing scenario simulators and their probabilities.\n            If None, default simulators will be used.\n        callbacks : Optional[Callbacks], optional\n            Langchain style callbacks to use for the generation process. You can use\n            this to log the generation process or add other metadata.\n        run_config : Optional[RunConfig], optional\n            Configuration for running the generation process.\n        with_debugging_logs : bool, default False\n            If True, enable debug logging for various components.\n        raise_exceptions : bool, default True\n            If True, raise exceptions during the generation process.\n\n        Returns\n        -------\n        Testset\n            A dataset containing the generated TestsetSamples.\n\n        Notes\n        -----\n        This function performs the following steps:\n        1. Set up scenarios and debug logging if required.\n        2. Generate scenarios using an Executor.\n        3. Calculate split values for different scenario types.\n        4. Generate samples for each scenario.\n        5. Compile the results into an EvaluationDataset.\n        \"\"\"\n        query_distribution = query_distribution or default_query_distribution(self.llm)\n        callbacks = callbacks or []\n\n        # new group for Testset Generation\n        testset_generation_rm, testset_generation_grp = new_group(\n            name=RAGAS_TESTSET_GENERATION_GROUP_NAME,\n            inputs={\"test_size\": test_size},\n            callbacks=callbacks,\n        )\n\n        if with_debugging_logs:\n            # TODO: Edit this before pre-release\n            from ragas.utils import patch_logger\n\n            patch_logger(\"ragas.experimental.testset.synthesizers\", logging.DEBUG)\n            patch_logger(\"ragas.experimental.testset.graph\", logging.DEBUG)\n            patch_logger(\"ragas.experimental.testset.transforms\", logging.DEBUG)\n\n        splits, _ = calculate_split_values(\n            [prob for _, prob in query_distribution], test_size\n        )\n        # new group for Generation of Scenarios\n        scenario_generation_rm, scenario_generation_grp = new_group(\n            name=\"Scenario Generation\",\n            inputs={\"splits\": splits},\n            callbacks=testset_generation_grp,\n        )\n\n        # generate scenarios\n        exec = Executor(\n            \"Generating Scenarios\",\n            raise_exceptions=raise_exceptions,\n            run_config=run_config,\n            keep_progress_bar=False,\n        )\n        # generate samples\n        splits, _ = calculate_split_values(\n            [prob for _, prob in query_distribution], test_size\n        )\n        for i, (scenario, _) in enumerate(query_distribution):\n            exec.submit(scenario.generate_scenarios, splits[i], self.knowledge_graph)\n\n        scenario_sample_list: t.List[t.List[BaseScenario]] = exec.results()\n        scenario_generation_rm.on_chain_end(\n            outputs={\"scenario_sample_list\": scenario_sample_list}\n        )\n\n        # new group for Generation of Samples\n        sample_generation_rm, sample_generation_grp = new_group(\n            name=\"Sample Generation\",\n            inputs={\"scenario_sample_list\": scenario_sample_list},\n            callbacks=testset_generation_grp,\n        )\n        exec = Executor(\n            \"Generating Samples\",\n            raise_exceptions=raise_exceptions,\n            run_config=run_config,\n            keep_progress_bar=True,\n        )\n        additional_testset_info: t.List[t.Dict] = []\n        for i, (synthesizer, _) in enumerate(query_distribution):\n            for sample in scenario_sample_list[i]:\n                exec.submit(synthesizer.generate_sample, sample)\n                # fill out the additional info for the TestsetSample\n                additional_testset_info.append(\n                    {\n                        \"synthesizer_name\": synthesizer.name,\n                    }\n                )\n\n        eval_samples = exec.results()\n        sample_generation_rm.on_chain_end(outputs={\"eval_samples\": eval_samples})\n\n        # build the testset\n        testsets = []\n        for sample, additional_info in zip(eval_samples, additional_testset_info):\n            testsets.append(TestsetSample(eval_sample=sample, **additional_info))\n        testset = Testset(samples=testsets)\n        testset_generation_rm.on_chain_end({\"testset\": testset})\n\n        # tracking how many samples were generated\n        track(\n            TestsetGenerationEvent(\n                event_type=\"testset_generation\",\n                evolution_names=[\n                    e.__class__.__name__.lower() for e, _ in query_distribution\n                ],\n                evolution_percentages=[p for _, p in query_distribution],\n                num_rows=test_size,\n                language=\"english\",\n            )\n        )\n        return testset\n</code></pre>"},{"location":"references/testset/__init__/#ragas.testset.TestsetGenerator.generate","title":"<code>generate(test_size, query_distribution=None, run_config=None, callbacks=None, with_debugging_logs=False, raise_exceptions=True)</code>","text":"<p>Generate an evaluation dataset based on given scenarios and parameters.</p> <p>Parameters:</p> Name Type Description Default <code>test_size</code> <code>int</code> <p>The number of samples to generate.</p> required <code>query_distribution</code> <code>Optional[QueryDistribution]</code> <p>A list of tuples containing scenario simulators and their probabilities. If None, default simulators will be used.</p> <code>None</code> <code>callbacks</code> <code>Optional[Callbacks]</code> <p>Langchain style callbacks to use for the generation process. You can use this to log the generation process or add other metadata.</p> <code>None</code> <code>run_config</code> <code>Optional[RunConfig]</code> <p>Configuration for running the generation process.</p> <code>None</code> <code>with_debugging_logs</code> <code>bool</code> <p>If True, enable debug logging for various components.</p> <code>False</code> <code>raise_exceptions</code> <code>bool</code> <p>If True, raise exceptions during the generation process.</p> <code>True</code> <p>Returns:</p> Type Description <code>Testset</code> <p>A dataset containing the generated TestsetSamples.</p> Notes <p>This function performs the following steps: 1. Set up scenarios and debug logging if required. 2. Generate scenarios using an Executor. 3. Calculate split values for different scenario types. 4. Generate samples for each scenario. 5. Compile the results into an EvaluationDataset.</p> Source code in <code>src/ragas/testset/synthesizers/generate.py</code> <pre><code>def generate(\n    self,\n    test_size: int,\n    query_distribution: t.Optional[QueryDistribution] = None,\n    run_config: t.Optional[RunConfig] = None,\n    callbacks: t.Optional[Callbacks] = None,\n    with_debugging_logs=False,\n    raise_exceptions: bool = True,\n) -&gt; Testset:\n    \"\"\"\n    Generate an evaluation dataset based on given scenarios and parameters.\n\n    Parameters\n    ----------\n    test_size : int\n        The number of samples to generate.\n    query_distribution : Optional[QueryDistribution], optional\n        A list of tuples containing scenario simulators and their probabilities.\n        If None, default simulators will be used.\n    callbacks : Optional[Callbacks], optional\n        Langchain style callbacks to use for the generation process. You can use\n        this to log the generation process or add other metadata.\n    run_config : Optional[RunConfig], optional\n        Configuration for running the generation process.\n    with_debugging_logs : bool, default False\n        If True, enable debug logging for various components.\n    raise_exceptions : bool, default True\n        If True, raise exceptions during the generation process.\n\n    Returns\n    -------\n    Testset\n        A dataset containing the generated TestsetSamples.\n\n    Notes\n    -----\n    This function performs the following steps:\n    1. Set up scenarios and debug logging if required.\n    2. Generate scenarios using an Executor.\n    3. Calculate split values for different scenario types.\n    4. Generate samples for each scenario.\n    5. Compile the results into an EvaluationDataset.\n    \"\"\"\n    query_distribution = query_distribution or default_query_distribution(self.llm)\n    callbacks = callbacks or []\n\n    # new group for Testset Generation\n    testset_generation_rm, testset_generation_grp = new_group(\n        name=RAGAS_TESTSET_GENERATION_GROUP_NAME,\n        inputs={\"test_size\": test_size},\n        callbacks=callbacks,\n    )\n\n    if with_debugging_logs:\n        # TODO: Edit this before pre-release\n        from ragas.utils import patch_logger\n\n        patch_logger(\"ragas.experimental.testset.synthesizers\", logging.DEBUG)\n        patch_logger(\"ragas.experimental.testset.graph\", logging.DEBUG)\n        patch_logger(\"ragas.experimental.testset.transforms\", logging.DEBUG)\n\n    splits, _ = calculate_split_values(\n        [prob for _, prob in query_distribution], test_size\n    )\n    # new group for Generation of Scenarios\n    scenario_generation_rm, scenario_generation_grp = new_group(\n        name=\"Scenario Generation\",\n        inputs={\"splits\": splits},\n        callbacks=testset_generation_grp,\n    )\n\n    # generate scenarios\n    exec = Executor(\n        \"Generating Scenarios\",\n        raise_exceptions=raise_exceptions,\n        run_config=run_config,\n        keep_progress_bar=False,\n    )\n    # generate samples\n    splits, _ = calculate_split_values(\n        [prob for _, prob in query_distribution], test_size\n    )\n    for i, (scenario, _) in enumerate(query_distribution):\n        exec.submit(scenario.generate_scenarios, splits[i], self.knowledge_graph)\n\n    scenario_sample_list: t.List[t.List[BaseScenario]] = exec.results()\n    scenario_generation_rm.on_chain_end(\n        outputs={\"scenario_sample_list\": scenario_sample_list}\n    )\n\n    # new group for Generation of Samples\n    sample_generation_rm, sample_generation_grp = new_group(\n        name=\"Sample Generation\",\n        inputs={\"scenario_sample_list\": scenario_sample_list},\n        callbacks=testset_generation_grp,\n    )\n    exec = Executor(\n        \"Generating Samples\",\n        raise_exceptions=raise_exceptions,\n        run_config=run_config,\n        keep_progress_bar=True,\n    )\n    additional_testset_info: t.List[t.Dict] = []\n    for i, (synthesizer, _) in enumerate(query_distribution):\n        for sample in scenario_sample_list[i]:\n            exec.submit(synthesizer.generate_sample, sample)\n            # fill out the additional info for the TestsetSample\n            additional_testset_info.append(\n                {\n                    \"synthesizer_name\": synthesizer.name,\n                }\n            )\n\n    eval_samples = exec.results()\n    sample_generation_rm.on_chain_end(outputs={\"eval_samples\": eval_samples})\n\n    # build the testset\n    testsets = []\n    for sample, additional_info in zip(eval_samples, additional_testset_info):\n        testsets.append(TestsetSample(eval_sample=sample, **additional_info))\n    testset = Testset(samples=testsets)\n    testset_generation_rm.on_chain_end({\"testset\": testset})\n\n    # tracking how many samples were generated\n    track(\n        TestsetGenerationEvent(\n            event_type=\"testset_generation\",\n            evolution_names=[\n                e.__class__.__name__.lower() for e, _ in query_distribution\n            ],\n            evolution_percentages=[p for _, p in query_distribution],\n            num_rows=test_size,\n            language=\"english\",\n        )\n    )\n    return testset\n</code></pre>"},{"location":"references/testset/graph/","title":"graph","text":""},{"location":"references/testset/synthesizers/__init__/","title":"synthesizers","text":""},{"location":"references/testset/synthesizers/abstract_query/","title":"abstract_query","text":""},{"location":"references/testset/synthesizers/base/","title":"base","text":""},{"location":"references/testset/synthesizers/base_query/","title":"base_query","text":""},{"location":"references/testset/synthesizers/generate/","title":"generate","text":""},{"location":"references/testset/synthesizers/generate/#ragas.testset.synthesizers.generate.TestsetGenerator","title":"<code>TestsetGenerator</code>  <code>dataclass</code>","text":"Source code in <code>src/ragas/testset/synthesizers/generate.py</code> <pre><code>@dataclass\nclass TestsetGenerator:\n    llm: BaseRagasLLM\n    knowledge_graph: KnowledgeGraph = field(default_factory=KnowledgeGraph)\n\n    @classmethod\n    def from_langchain(\n        cls,\n        llm: LangchainLLM,\n        knowledge_graph: t.Optional[KnowledgeGraph] = None,\n    ):\n        knowledge_graph = knowledge_graph or KnowledgeGraph()\n        return cls(LangchainLLMWrapper(llm), knowledge_graph)\n\n    def generate_with_langchain_docs(\n        self,\n        documents: t.Sequence[LCDocument],\n        test_size: int,\n        transforms: t.Optional[Transforms] = None,\n        query_distribution: t.Optional[QueryDistribution] = None,\n        run_config: t.Optional[RunConfig] = None,\n        callbacks: t.Optional[Callbacks] = None,\n        with_debugging_logs=False,\n        raise_exceptions: bool = True,\n    ) -&gt; Testset:\n        transforms = transforms or default_transforms()\n\n        # convert the documents to Ragas nodes\n        nodes = []\n        for doc in documents:\n            node = Node(\n                type=NodeType.DOCUMENT,\n                properties={\n                    \"page_content\": doc.page_content,\n                    \"document_metadata\": doc.metadata,\n                },\n            )\n            nodes.append(node)\n\n        kg = KnowledgeGraph(nodes=nodes)\n\n        # apply transforms and update the knowledge graph\n        apply_transforms(kg, transforms)\n        self.knowledge_graph = kg\n\n        return self.generate(\n            test_size=test_size,\n            query_distribution=query_distribution,\n            run_config=run_config,\n            callbacks=callbacks,\n            with_debugging_logs=with_debugging_logs,\n            raise_exceptions=raise_exceptions,\n        )\n\n    def generate(\n        self,\n        test_size: int,\n        query_distribution: t.Optional[QueryDistribution] = None,\n        run_config: t.Optional[RunConfig] = None,\n        callbacks: t.Optional[Callbacks] = None,\n        with_debugging_logs=False,\n        raise_exceptions: bool = True,\n    ) -&gt; Testset:\n        \"\"\"\n        Generate an evaluation dataset based on given scenarios and parameters.\n\n        Parameters\n        ----------\n        test_size : int\n            The number of samples to generate.\n        query_distribution : Optional[QueryDistribution], optional\n            A list of tuples containing scenario simulators and their probabilities.\n            If None, default simulators will be used.\n        callbacks : Optional[Callbacks], optional\n            Langchain style callbacks to use for the generation process. You can use\n            this to log the generation process or add other metadata.\n        run_config : Optional[RunConfig], optional\n            Configuration for running the generation process.\n        with_debugging_logs : bool, default False\n            If True, enable debug logging for various components.\n        raise_exceptions : bool, default True\n            If True, raise exceptions during the generation process.\n\n        Returns\n        -------\n        Testset\n            A dataset containing the generated TestsetSamples.\n\n        Notes\n        -----\n        This function performs the following steps:\n        1. Set up scenarios and debug logging if required.\n        2. Generate scenarios using an Executor.\n        3. Calculate split values for different scenario types.\n        4. Generate samples for each scenario.\n        5. Compile the results into an EvaluationDataset.\n        \"\"\"\n        query_distribution = query_distribution or default_query_distribution(self.llm)\n        callbacks = callbacks or []\n\n        # new group for Testset Generation\n        testset_generation_rm, testset_generation_grp = new_group(\n            name=RAGAS_TESTSET_GENERATION_GROUP_NAME,\n            inputs={\"test_size\": test_size},\n            callbacks=callbacks,\n        )\n\n        if with_debugging_logs:\n            # TODO: Edit this before pre-release\n            from ragas.utils import patch_logger\n\n            patch_logger(\"ragas.experimental.testset.synthesizers\", logging.DEBUG)\n            patch_logger(\"ragas.experimental.testset.graph\", logging.DEBUG)\n            patch_logger(\"ragas.experimental.testset.transforms\", logging.DEBUG)\n\n        splits, _ = calculate_split_values(\n            [prob for _, prob in query_distribution], test_size\n        )\n        # new group for Generation of Scenarios\n        scenario_generation_rm, scenario_generation_grp = new_group(\n            name=\"Scenario Generation\",\n            inputs={\"splits\": splits},\n            callbacks=testset_generation_grp,\n        )\n\n        # generate scenarios\n        exec = Executor(\n            \"Generating Scenarios\",\n            raise_exceptions=raise_exceptions,\n            run_config=run_config,\n            keep_progress_bar=False,\n        )\n        # generate samples\n        splits, _ = calculate_split_values(\n            [prob for _, prob in query_distribution], test_size\n        )\n        for i, (scenario, _) in enumerate(query_distribution):\n            exec.submit(scenario.generate_scenarios, splits[i], self.knowledge_graph)\n\n        scenario_sample_list: t.List[t.List[BaseScenario]] = exec.results()\n        scenario_generation_rm.on_chain_end(\n            outputs={\"scenario_sample_list\": scenario_sample_list}\n        )\n\n        # new group for Generation of Samples\n        sample_generation_rm, sample_generation_grp = new_group(\n            name=\"Sample Generation\",\n            inputs={\"scenario_sample_list\": scenario_sample_list},\n            callbacks=testset_generation_grp,\n        )\n        exec = Executor(\n            \"Generating Samples\",\n            raise_exceptions=raise_exceptions,\n            run_config=run_config,\n            keep_progress_bar=True,\n        )\n        additional_testset_info: t.List[t.Dict] = []\n        for i, (synthesizer, _) in enumerate(query_distribution):\n            for sample in scenario_sample_list[i]:\n                exec.submit(synthesizer.generate_sample, sample)\n                # fill out the additional info for the TestsetSample\n                additional_testset_info.append(\n                    {\n                        \"synthesizer_name\": synthesizer.name,\n                    }\n                )\n\n        eval_samples = exec.results()\n        sample_generation_rm.on_chain_end(outputs={\"eval_samples\": eval_samples})\n\n        # build the testset\n        testsets = []\n        for sample, additional_info in zip(eval_samples, additional_testset_info):\n            testsets.append(TestsetSample(eval_sample=sample, **additional_info))\n        testset = Testset(samples=testsets)\n        testset_generation_rm.on_chain_end({\"testset\": testset})\n\n        # tracking how many samples were generated\n        track(\n            TestsetGenerationEvent(\n                event_type=\"testset_generation\",\n                evolution_names=[\n                    e.__class__.__name__.lower() for e, _ in query_distribution\n                ],\n                evolution_percentages=[p for _, p in query_distribution],\n                num_rows=test_size,\n                language=\"english\",\n            )\n        )\n        return testset\n</code></pre>"},{"location":"references/testset/synthesizers/generate/#ragas.testset.synthesizers.generate.TestsetGenerator.generate","title":"<code>generate(test_size, query_distribution=None, run_config=None, callbacks=None, with_debugging_logs=False, raise_exceptions=True)</code>","text":"<p>Generate an evaluation dataset based on given scenarios and parameters.</p> <p>Parameters:</p> Name Type Description Default <code>test_size</code> <code>int</code> <p>The number of samples to generate.</p> required <code>query_distribution</code> <code>Optional[QueryDistribution]</code> <p>A list of tuples containing scenario simulators and their probabilities. If None, default simulators will be used.</p> <code>None</code> <code>callbacks</code> <code>Optional[Callbacks]</code> <p>Langchain style callbacks to use for the generation process. You can use this to log the generation process or add other metadata.</p> <code>None</code> <code>run_config</code> <code>Optional[RunConfig]</code> <p>Configuration for running the generation process.</p> <code>None</code> <code>with_debugging_logs</code> <code>bool</code> <p>If True, enable debug logging for various components.</p> <code>False</code> <code>raise_exceptions</code> <code>bool</code> <p>If True, raise exceptions during the generation process.</p> <code>True</code> <p>Returns:</p> Type Description <code>Testset</code> <p>A dataset containing the generated TestsetSamples.</p> Notes <p>This function performs the following steps: 1. Set up scenarios and debug logging if required. 2. Generate scenarios using an Executor. 3. Calculate split values for different scenario types. 4. Generate samples for each scenario. 5. Compile the results into an EvaluationDataset.</p> Source code in <code>src/ragas/testset/synthesizers/generate.py</code> <pre><code>def generate(\n    self,\n    test_size: int,\n    query_distribution: t.Optional[QueryDistribution] = None,\n    run_config: t.Optional[RunConfig] = None,\n    callbacks: t.Optional[Callbacks] = None,\n    with_debugging_logs=False,\n    raise_exceptions: bool = True,\n) -&gt; Testset:\n    \"\"\"\n    Generate an evaluation dataset based on given scenarios and parameters.\n\n    Parameters\n    ----------\n    test_size : int\n        The number of samples to generate.\n    query_distribution : Optional[QueryDistribution], optional\n        A list of tuples containing scenario simulators and their probabilities.\n        If None, default simulators will be used.\n    callbacks : Optional[Callbacks], optional\n        Langchain style callbacks to use for the generation process. You can use\n        this to log the generation process or add other metadata.\n    run_config : Optional[RunConfig], optional\n        Configuration for running the generation process.\n    with_debugging_logs : bool, default False\n        If True, enable debug logging for various components.\n    raise_exceptions : bool, default True\n        If True, raise exceptions during the generation process.\n\n    Returns\n    -------\n    Testset\n        A dataset containing the generated TestsetSamples.\n\n    Notes\n    -----\n    This function performs the following steps:\n    1. Set up scenarios and debug logging if required.\n    2. Generate scenarios using an Executor.\n    3. Calculate split values for different scenario types.\n    4. Generate samples for each scenario.\n    5. Compile the results into an EvaluationDataset.\n    \"\"\"\n    query_distribution = query_distribution or default_query_distribution(self.llm)\n    callbacks = callbacks or []\n\n    # new group for Testset Generation\n    testset_generation_rm, testset_generation_grp = new_group(\n        name=RAGAS_TESTSET_GENERATION_GROUP_NAME,\n        inputs={\"test_size\": test_size},\n        callbacks=callbacks,\n    )\n\n    if with_debugging_logs:\n        # TODO: Edit this before pre-release\n        from ragas.utils import patch_logger\n\n        patch_logger(\"ragas.experimental.testset.synthesizers\", logging.DEBUG)\n        patch_logger(\"ragas.experimental.testset.graph\", logging.DEBUG)\n        patch_logger(\"ragas.experimental.testset.transforms\", logging.DEBUG)\n\n    splits, _ = calculate_split_values(\n        [prob for _, prob in query_distribution], test_size\n    )\n    # new group for Generation of Scenarios\n    scenario_generation_rm, scenario_generation_grp = new_group(\n        name=\"Scenario Generation\",\n        inputs={\"splits\": splits},\n        callbacks=testset_generation_grp,\n    )\n\n    # generate scenarios\n    exec = Executor(\n        \"Generating Scenarios\",\n        raise_exceptions=raise_exceptions,\n        run_config=run_config,\n        keep_progress_bar=False,\n    )\n    # generate samples\n    splits, _ = calculate_split_values(\n        [prob for _, prob in query_distribution], test_size\n    )\n    for i, (scenario, _) in enumerate(query_distribution):\n        exec.submit(scenario.generate_scenarios, splits[i], self.knowledge_graph)\n\n    scenario_sample_list: t.List[t.List[BaseScenario]] = exec.results()\n    scenario_generation_rm.on_chain_end(\n        outputs={\"scenario_sample_list\": scenario_sample_list}\n    )\n\n    # new group for Generation of Samples\n    sample_generation_rm, sample_generation_grp = new_group(\n        name=\"Sample Generation\",\n        inputs={\"scenario_sample_list\": scenario_sample_list},\n        callbacks=testset_generation_grp,\n    )\n    exec = Executor(\n        \"Generating Samples\",\n        raise_exceptions=raise_exceptions,\n        run_config=run_config,\n        keep_progress_bar=True,\n    )\n    additional_testset_info: t.List[t.Dict] = []\n    for i, (synthesizer, _) in enumerate(query_distribution):\n        for sample in scenario_sample_list[i]:\n            exec.submit(synthesizer.generate_sample, sample)\n            # fill out the additional info for the TestsetSample\n            additional_testset_info.append(\n                {\n                    \"synthesizer_name\": synthesizer.name,\n                }\n            )\n\n    eval_samples = exec.results()\n    sample_generation_rm.on_chain_end(outputs={\"eval_samples\": eval_samples})\n\n    # build the testset\n    testsets = []\n    for sample, additional_info in zip(eval_samples, additional_testset_info):\n        testsets.append(TestsetSample(eval_sample=sample, **additional_info))\n    testset = Testset(samples=testsets)\n    testset_generation_rm.on_chain_end({\"testset\": testset})\n\n    # tracking how many samples were generated\n    track(\n        TestsetGenerationEvent(\n            event_type=\"testset_generation\",\n            evolution_names=[\n                e.__class__.__name__.lower() for e, _ in query_distribution\n            ],\n            evolution_percentages=[p for _, p in query_distribution],\n            num_rows=test_size,\n            language=\"english\",\n        )\n    )\n    return testset\n</code></pre>"},{"location":"references/testset/synthesizers/prompts/","title":"prompts","text":""},{"location":"references/testset/synthesizers/prompts/#ragas.testset.synthesizers.prompts.SpecificQuery","title":"<code>SpecificQuery</code>","text":"<p>               Bases: <code>PydanticPrompt[SpecificQuestionInput, StringIO]</code></p> Source code in <code>src/ragas/testset/synthesizers/prompts.py</code> <pre><code>class SpecificQuery(PydanticPrompt[SpecificQuestionInput, StringIO]):\n    input_model = SpecificQuestionInput\n    output_model = StringIO\n    instruction = \"Given the title of a text and a text chunk, along with a keyphrase from the chunk, generate a specific question related to the keyphrase.\\n\\n\"\n    \"1. Read the title and the text chunk.\\n\"\n    \"2. Identify the context of the keyphrase within the text chunk.\\n\"\n    \"3. Formulate a question that directly relates to the keyphrase and its context within the chunk.\\n\"\n    \"4. Ensure the question is clear, specific, and relevant to the keyphrase.\"\n    examples = [\n        (\n            SpecificQuestionInput(\n                title=\"The Impact of Artificial Intelligence on Modern Healthcare\",\n                keyphrase=\"personalized treatment plans\",\n                text=\"Artificial intelligence (AI) is revolutionizing healthcare by improving diagnostic accuracy and enabling personalized treatment plans. AI algorithms analyze vast amounts of medical data to identify patterns and predict patient outcomes, which enhances the decision-making process for healthcare professionals.\",\n            ),\n            StringIO(\n                text=\"How does AI contribute to the development of personalized treatment plans in healthcare?\"\n            ),\n        )\n    ]\n</code></pre>"},{"location":"references/testset/synthesizers/prompts/#ragas.testset.synthesizers.prompts.SpecificQuery.instruction","title":"<code>instruction = 'Given the title of a text and a text chunk, along with a keyphrase from the chunk, generate a specific question related to the keyphrase.\\n\\n'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<ol> <li>Read the title and the text chunk.</li> </ol>"},{"location":"references/testset/synthesizers/specific_query/","title":"specific_query","text":""},{"location":"references/testset/synthesizers/testset_schema/","title":"testset_schema","text":""},{"location":"references/testset/synthesizers/utils/","title":"utils","text":""},{"location":"references/testset/transforms/__init__/","title":"transforms","text":""},{"location":"references/testset/transforms/__init__/#ragas.testset.transforms.BaseGraphTransformation","title":"<code>BaseGraphTransformation</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for graph transformations on a KnowledgeGraph.</p> Source code in <code>src/ragas/testset/transforms/base.py</code> <pre><code>@dataclass\nclass BaseGraphTransformation(ABC):\n    \"\"\"\n    Abstract base class for graph transformations on a KnowledgeGraph.\n    \"\"\"\n\n    name: str = \"\"\n\n    def __post_init__(self):\n        if not self.name:\n            self.name = self.__class__.__name__\n\n    @abstractmethod\n    async def transform(self, kg: KnowledgeGraph) -&gt; t.Any:\n        \"\"\"\n        Abstract method to transform the KnowledgeGraph. Transformations should be\n        idempotent, meaning that applying the transformation multiple times should\n        yield the same result as applying it once.\n\n        Parameters\n        ----------\n        kg : KnowledgeGraph\n            The knowledge graph to be transformed.\n\n        Returns\n        -------\n        t.Any\n            The transformed knowledge graph.\n        \"\"\"\n        pass\n\n    def filter(self, kg: KnowledgeGraph) -&gt; KnowledgeGraph:\n        \"\"\"\n        Filters the KnowledgeGraph and returns the filtered graph.\n\n        Parameters\n        ----------\n        kg : KnowledgeGraph\n            The knowledge graph to be filtered.\n\n        Returns\n        -------\n        KnowledgeGraph\n            The filtered knowledge graph.\n        \"\"\"\n        return kg\n\n    @abstractmethod\n    def generate_execution_plan(self, kg: KnowledgeGraph) -&gt; t.List[t.Coroutine]:\n        \"\"\"\n        Generates a list of coroutines to be executed in sequence by the Executor. This\n        coroutine will, upon execution, write the transformation into the KnowledgeGraph.\n\n        Parameters\n        ----------\n        kg : KnowledgeGraph\n            The knowledge graph to be transformed.\n\n        Returns\n        -------\n        t.List[t.Coroutine]\n            A list of coroutines to be executed in parallel.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"references/testset/transforms/__init__/#ragas.testset.transforms.BaseGraphTransformation.filter","title":"<code>filter(kg)</code>","text":"<p>Filters the KnowledgeGraph and returns the filtered graph.</p> <p>Parameters:</p> Name Type Description Default <code>kg</code> <code>KnowledgeGraph</code> <p>The knowledge graph to be filtered.</p> required <p>Returns:</p> Type Description <code>KnowledgeGraph</code> <p>The filtered knowledge graph.</p> Source code in <code>src/ragas/testset/transforms/base.py</code> <pre><code>def filter(self, kg: KnowledgeGraph) -&gt; KnowledgeGraph:\n    \"\"\"\n    Filters the KnowledgeGraph and returns the filtered graph.\n\n    Parameters\n    ----------\n    kg : KnowledgeGraph\n        The knowledge graph to be filtered.\n\n    Returns\n    -------\n    KnowledgeGraph\n        The filtered knowledge graph.\n    \"\"\"\n    return kg\n</code></pre>"},{"location":"references/testset/transforms/__init__/#ragas.testset.transforms.BaseGraphTransformation.generate_execution_plan","title":"<code>generate_execution_plan(kg)</code>  <code>abstractmethod</code>","text":"<p>Generates a list of coroutines to be executed in sequence by the Executor. This coroutine will, upon execution, write the transformation into the KnowledgeGraph.</p> <p>Parameters:</p> Name Type Description Default <code>kg</code> <code>KnowledgeGraph</code> <p>The knowledge graph to be transformed.</p> required <p>Returns:</p> Type Description <code>List[Coroutine]</code> <p>A list of coroutines to be executed in parallel.</p> Source code in <code>src/ragas/testset/transforms/base.py</code> <pre><code>@abstractmethod\ndef generate_execution_plan(self, kg: KnowledgeGraph) -&gt; t.List[t.Coroutine]:\n    \"\"\"\n    Generates a list of coroutines to be executed in sequence by the Executor. This\n    coroutine will, upon execution, write the transformation into the KnowledgeGraph.\n\n    Parameters\n    ----------\n    kg : KnowledgeGraph\n        The knowledge graph to be transformed.\n\n    Returns\n    -------\n    t.List[t.Coroutine]\n        A list of coroutines to be executed in parallel.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/testset/transforms/__init__/#ragas.testset.transforms.BaseGraphTransformation.transform","title":"<code>transform(kg)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Abstract method to transform the KnowledgeGraph. Transformations should be idempotent, meaning that applying the transformation multiple times should yield the same result as applying it once.</p> <p>Parameters:</p> Name Type Description Default <code>kg</code> <code>KnowledgeGraph</code> <p>The knowledge graph to be transformed.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The transformed knowledge graph.</p> Source code in <code>src/ragas/testset/transforms/base.py</code> <pre><code>@abstractmethod\nasync def transform(self, kg: KnowledgeGraph) -&gt; t.Any:\n    \"\"\"\n    Abstract method to transform the KnowledgeGraph. Transformations should be\n    idempotent, meaning that applying the transformation multiple times should\n    yield the same result as applying it once.\n\n    Parameters\n    ----------\n    kg : KnowledgeGraph\n        The knowledge graph to be transformed.\n\n    Returns\n    -------\n    t.Any\n        The transformed knowledge graph.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/testset/transforms/__init__/#ragas.testset.transforms.apply_transforms","title":"<code>apply_transforms(kg, transforms, run_config=RunConfig())</code>","text":"<p>Apply a list of transformations to a knowledge graph in place.</p> Source code in <code>src/ragas/testset/transforms/engine.py</code> <pre><code>def apply_transforms(\n    kg: KnowledgeGraph,\n    transforms: Transforms,\n    run_config: RunConfig = RunConfig(),\n):\n    \"\"\"\n    Apply a list of transformations to a knowledge graph in place.\n    \"\"\"\n    # apply nest_asyncio to fix the event loop issue in jupyter\n    apply_nest_asyncio()\n\n    # if single transformation, wrap it in a list\n    if isinstance(transforms, BaseGraphTransformation):\n        transforms = [transforms]\n\n    # apply the transformations\n    # if Sequences, apply each transformation sequentially\n    if isinstance(transforms, t.List):\n        for transform in transforms:\n            asyncio.run(\n                run_coroutines(\n                    transform.generate_execution_plan(kg),\n                    get_desc(transform),\n                    run_config.max_workers,\n                )\n            )\n    # if Parallel, collect inside it and run it all\n    elif isinstance(transforms, Parallel):\n        asyncio.run(\n            run_coroutines(\n                transforms.generate_execution_plan(kg),\n                get_desc(transforms),\n                run_config.max_workers,\n            )\n        )\n    else:\n        raise ValueError(\n            f\"Invalid transforms type: {type(transforms)}. Expects a list of BaseGraphTransformations or a Parallel instance.\"\n        )\n</code></pre>"},{"location":"references/testset/transforms/base/","title":"base","text":""},{"location":"references/testset/transforms/base/#ragas.testset.transforms.base.BaseGraphTransformation","title":"<code>BaseGraphTransformation</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for graph transformations on a KnowledgeGraph.</p> Source code in <code>src/ragas/testset/transforms/base.py</code> <pre><code>@dataclass\nclass BaseGraphTransformation(ABC):\n    \"\"\"\n    Abstract base class for graph transformations on a KnowledgeGraph.\n    \"\"\"\n\n    name: str = \"\"\n\n    def __post_init__(self):\n        if not self.name:\n            self.name = self.__class__.__name__\n\n    @abstractmethod\n    async def transform(self, kg: KnowledgeGraph) -&gt; t.Any:\n        \"\"\"\n        Abstract method to transform the KnowledgeGraph. Transformations should be\n        idempotent, meaning that applying the transformation multiple times should\n        yield the same result as applying it once.\n\n        Parameters\n        ----------\n        kg : KnowledgeGraph\n            The knowledge graph to be transformed.\n\n        Returns\n        -------\n        t.Any\n            The transformed knowledge graph.\n        \"\"\"\n        pass\n\n    def filter(self, kg: KnowledgeGraph) -&gt; KnowledgeGraph:\n        \"\"\"\n        Filters the KnowledgeGraph and returns the filtered graph.\n\n        Parameters\n        ----------\n        kg : KnowledgeGraph\n            The knowledge graph to be filtered.\n\n        Returns\n        -------\n        KnowledgeGraph\n            The filtered knowledge graph.\n        \"\"\"\n        return kg\n\n    @abstractmethod\n    def generate_execution_plan(self, kg: KnowledgeGraph) -&gt; t.List[t.Coroutine]:\n        \"\"\"\n        Generates a list of coroutines to be executed in sequence by the Executor. This\n        coroutine will, upon execution, write the transformation into the KnowledgeGraph.\n\n        Parameters\n        ----------\n        kg : KnowledgeGraph\n            The knowledge graph to be transformed.\n\n        Returns\n        -------\n        t.List[t.Coroutine]\n            A list of coroutines to be executed in parallel.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"references/testset/transforms/base/#ragas.testset.transforms.base.BaseGraphTransformation.filter","title":"<code>filter(kg)</code>","text":"<p>Filters the KnowledgeGraph and returns the filtered graph.</p> <p>Parameters:</p> Name Type Description Default <code>kg</code> <code>KnowledgeGraph</code> <p>The knowledge graph to be filtered.</p> required <p>Returns:</p> Type Description <code>KnowledgeGraph</code> <p>The filtered knowledge graph.</p> Source code in <code>src/ragas/testset/transforms/base.py</code> <pre><code>def filter(self, kg: KnowledgeGraph) -&gt; KnowledgeGraph:\n    \"\"\"\n    Filters the KnowledgeGraph and returns the filtered graph.\n\n    Parameters\n    ----------\n    kg : KnowledgeGraph\n        The knowledge graph to be filtered.\n\n    Returns\n    -------\n    KnowledgeGraph\n        The filtered knowledge graph.\n    \"\"\"\n    return kg\n</code></pre>"},{"location":"references/testset/transforms/base/#ragas.testset.transforms.base.BaseGraphTransformation.generate_execution_plan","title":"<code>generate_execution_plan(kg)</code>  <code>abstractmethod</code>","text":"<p>Generates a list of coroutines to be executed in sequence by the Executor. This coroutine will, upon execution, write the transformation into the KnowledgeGraph.</p> <p>Parameters:</p> Name Type Description Default <code>kg</code> <code>KnowledgeGraph</code> <p>The knowledge graph to be transformed.</p> required <p>Returns:</p> Type Description <code>List[Coroutine]</code> <p>A list of coroutines to be executed in parallel.</p> Source code in <code>src/ragas/testset/transforms/base.py</code> <pre><code>@abstractmethod\ndef generate_execution_plan(self, kg: KnowledgeGraph) -&gt; t.List[t.Coroutine]:\n    \"\"\"\n    Generates a list of coroutines to be executed in sequence by the Executor. This\n    coroutine will, upon execution, write the transformation into the KnowledgeGraph.\n\n    Parameters\n    ----------\n    kg : KnowledgeGraph\n        The knowledge graph to be transformed.\n\n    Returns\n    -------\n    t.List[t.Coroutine]\n        A list of coroutines to be executed in parallel.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/testset/transforms/base/#ragas.testset.transforms.base.BaseGraphTransformation.transform","title":"<code>transform(kg)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Abstract method to transform the KnowledgeGraph. Transformations should be idempotent, meaning that applying the transformation multiple times should yield the same result as applying it once.</p> <p>Parameters:</p> Name Type Description Default <code>kg</code> <code>KnowledgeGraph</code> <p>The knowledge graph to be transformed.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The transformed knowledge graph.</p> Source code in <code>src/ragas/testset/transforms/base.py</code> <pre><code>@abstractmethod\nasync def transform(self, kg: KnowledgeGraph) -&gt; t.Any:\n    \"\"\"\n    Abstract method to transform the KnowledgeGraph. Transformations should be\n    idempotent, meaning that applying the transformation multiple times should\n    yield the same result as applying it once.\n\n    Parameters\n    ----------\n    kg : KnowledgeGraph\n        The knowledge graph to be transformed.\n\n    Returns\n    -------\n    t.Any\n        The transformed knowledge graph.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/testset/transforms/base/#ragas.testset.transforms.base.Extractor","title":"<code>Extractor</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseGraphTransformation</code></p> <p>Abstract base class for extractors that transform a KnowledgeGraph by extracting specific properties from its nodes.</p> <p>Methods:</p> Name Description <code>transform</code> <p>Transforms the KnowledgeGraph by extracting properties from its nodes.</p> <code>extract</code> <p>Abstract method to extract a specific property from a node.</p> Source code in <code>src/ragas/testset/transforms/base.py</code> <pre><code>@dataclass\nclass Extractor(BaseGraphTransformation):\n    \"\"\"\n    Abstract base class for extractors that transform a KnowledgeGraph by extracting\n    specific properties from its nodes.\n\n    Methods\n    -------\n    transform(kg: KnowledgeGraph) -&gt; t.List[t.Tuple[Node, t.Tuple[str, t.Any]]]\n        Transforms the KnowledgeGraph by extracting properties from its nodes.\n\n    extract(node: Node) -&gt; t.Tuple[str, t.Any]\n        Abstract method to extract a specific property from a node.\n    \"\"\"\n\n    filter_nodes: t.Callable[[Node], bool] = field(\n        default_factory=lambda: default_filter\n    )\n\n    async def transform(\n        self, kg: KnowledgeGraph\n    ) -&gt; t.List[t.Tuple[Node, t.Tuple[str, t.Any]]]:\n        \"\"\"\n        Transforms the KnowledgeGraph by extracting properties from its nodes. Uses\n        the `filter` method to filter the graph and the `extract` method to extract\n        properties from each node.\n\n        Parameters\n        ----------\n        kg : KnowledgeGraph\n            The knowledge graph to be transformed.\n\n        Returns\n        -------\n        t.List[t.Tuple[Node, t.Tuple[str, t.Any]]]\n            A list of tuples where each tuple contains a node and the extracted\n            property.\n\n        Examples\n        --------\n        &gt;&gt;&gt; kg = KnowledgeGraph(nodes=[Node(id=1, properties={\"name\": \"Node1\"}), Node(id=2, properties={\"name\": \"Node2\"})])\n        &gt;&gt;&gt; extractor = SomeConcreteExtractor()\n        &gt;&gt;&gt; extractor.transform(kg)\n        [(Node(id=1, properties={\"name\": \"Node1\"}), (\"property_name\", \"extracted_value\")),\n         (Node(id=2, properties={\"name\": \"Node2\"}), (\"property_name\", \"extracted_value\"))]\n        \"\"\"\n        filtered = self.filter(kg)\n        return [(node, await self.extract(node)) for node in filtered.nodes]\n\n    @abstractmethod\n    async def extract(self, node: Node) -&gt; t.Tuple[str, t.Any]:\n        \"\"\"\n        Abstract method to extract a specific property from a node.\n\n        Parameters\n        ----------\n        node : Node\n            The node from which to extract the property.\n\n        Returns\n        -------\n        t.Tuple[str, t.Any]\n            A tuple containing the property name and the extracted value.\n        \"\"\"\n        pass\n\n    def generate_execution_plan(self, kg: KnowledgeGraph) -&gt; t.List[t.Coroutine]:\n        \"\"\"\n        Generates a list of coroutines to be executed in parallel by the Executor.\n\n        Parameters\n        ----------\n        kg : KnowledgeGraph\n            The knowledge graph to be transformed.\n\n        Returns\n        -------\n        t.List[t.Coroutine]\n            A list of coroutines to be executed in parallel.\n        \"\"\"\n\n        async def apply_extract(node: Node):\n            property_name, property_value = await self.extract(node)\n            if node.get_property(property_name) is None:\n                node.add_property(property_name, property_value)\n            else:\n                logger.warning(\n                    \"Property '%s' already exists in node '%.6s'. Skipping!\",\n                    property_name,\n                    node.id,\n                )\n\n        filtered = self.filter(kg)\n        return [apply_extract(node) for node in filtered.nodes]\n\n    def filter(self, kg: KnowledgeGraph) -&gt; KnowledgeGraph:\n        return KnowledgeGraph(\n            nodes=[node for node in kg.nodes if self.filter_nodes(node)],\n            relationships=[\n                rel\n                for rel in kg.relationships\n                if rel.source in kg.nodes and rel.target in kg.nodes\n            ],\n        )\n</code></pre>"},{"location":"references/testset/transforms/base/#ragas.testset.transforms.base.Extractor.extract","title":"<code>extract(node)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Abstract method to extract a specific property from a node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The node from which to extract the property.</p> required <p>Returns:</p> Type Description <code>Tuple[str, Any]</code> <p>A tuple containing the property name and the extracted value.</p> Source code in <code>src/ragas/testset/transforms/base.py</code> <pre><code>@abstractmethod\nasync def extract(self, node: Node) -&gt; t.Tuple[str, t.Any]:\n    \"\"\"\n    Abstract method to extract a specific property from a node.\n\n    Parameters\n    ----------\n    node : Node\n        The node from which to extract the property.\n\n    Returns\n    -------\n    t.Tuple[str, t.Any]\n        A tuple containing the property name and the extracted value.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/testset/transforms/base/#ragas.testset.transforms.base.Extractor.generate_execution_plan","title":"<code>generate_execution_plan(kg)</code>","text":"<p>Generates a list of coroutines to be executed in parallel by the Executor.</p> <p>Parameters:</p> Name Type Description Default <code>kg</code> <code>KnowledgeGraph</code> <p>The knowledge graph to be transformed.</p> required <p>Returns:</p> Type Description <code>List[Coroutine]</code> <p>A list of coroutines to be executed in parallel.</p> Source code in <code>src/ragas/testset/transforms/base.py</code> <pre><code>def generate_execution_plan(self, kg: KnowledgeGraph) -&gt; t.List[t.Coroutine]:\n    \"\"\"\n    Generates a list of coroutines to be executed in parallel by the Executor.\n\n    Parameters\n    ----------\n    kg : KnowledgeGraph\n        The knowledge graph to be transformed.\n\n    Returns\n    -------\n    t.List[t.Coroutine]\n        A list of coroutines to be executed in parallel.\n    \"\"\"\n\n    async def apply_extract(node: Node):\n        property_name, property_value = await self.extract(node)\n        if node.get_property(property_name) is None:\n            node.add_property(property_name, property_value)\n        else:\n            logger.warning(\n                \"Property '%s' already exists in node '%.6s'. Skipping!\",\n                property_name,\n                node.id,\n            )\n\n    filtered = self.filter(kg)\n    return [apply_extract(node) for node in filtered.nodes]\n</code></pre>"},{"location":"references/testset/transforms/base/#ragas.testset.transforms.base.Extractor.transform","title":"<code>transform(kg)</code>  <code>async</code>","text":"<p>Transforms the KnowledgeGraph by extracting properties from its nodes. Uses the <code>filter</code> method to filter the graph and the <code>extract</code> method to extract properties from each node.</p> <p>Parameters:</p> Name Type Description Default <code>kg</code> <code>KnowledgeGraph</code> <p>The knowledge graph to be transformed.</p> required <p>Returns:</p> Type Description <code>List[Tuple[Node, Tuple[str, Any]]]</code> <p>A list of tuples where each tuple contains a node and the extracted property.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; kg = KnowledgeGraph(nodes=[Node(id=1, properties={\"name\": \"Node1\"}), Node(id=2, properties={\"name\": \"Node2\"})])\n&gt;&gt;&gt; extractor = SomeConcreteExtractor()\n&gt;&gt;&gt; extractor.transform(kg)\n[(Node(id=1, properties={\"name\": \"Node1\"}), (\"property_name\", \"extracted_value\")),\n (Node(id=2, properties={\"name\": \"Node2\"}), (\"property_name\", \"extracted_value\"))]\n</code></pre> Source code in <code>src/ragas/testset/transforms/base.py</code> <pre><code>async def transform(\n    self, kg: KnowledgeGraph\n) -&gt; t.List[t.Tuple[Node, t.Tuple[str, t.Any]]]:\n    \"\"\"\n    Transforms the KnowledgeGraph by extracting properties from its nodes. Uses\n    the `filter` method to filter the graph and the `extract` method to extract\n    properties from each node.\n\n    Parameters\n    ----------\n    kg : KnowledgeGraph\n        The knowledge graph to be transformed.\n\n    Returns\n    -------\n    t.List[t.Tuple[Node, t.Tuple[str, t.Any]]]\n        A list of tuples where each tuple contains a node and the extracted\n        property.\n\n    Examples\n    --------\n    &gt;&gt;&gt; kg = KnowledgeGraph(nodes=[Node(id=1, properties={\"name\": \"Node1\"}), Node(id=2, properties={\"name\": \"Node2\"})])\n    &gt;&gt;&gt; extractor = SomeConcreteExtractor()\n    &gt;&gt;&gt; extractor.transform(kg)\n    [(Node(id=1, properties={\"name\": \"Node1\"}), (\"property_name\", \"extracted_value\")),\n     (Node(id=2, properties={\"name\": \"Node2\"}), (\"property_name\", \"extracted_value\"))]\n    \"\"\"\n    filtered = self.filter(kg)\n    return [(node, await self.extract(node)) for node in filtered.nodes]\n</code></pre>"},{"location":"references/testset/transforms/base/#ragas.testset.transforms.base.RelationshipBuilder","title":"<code>RelationshipBuilder</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseGraphTransformation</code></p> <p>Abstract base class for building relationships in a KnowledgeGraph.</p> <p>Methods:</p> Name Description <code>transform</code> <p>Transforms the KnowledgeGraph by building relationships.</p> Source code in <code>src/ragas/testset/transforms/base.py</code> <pre><code>class RelationshipBuilder(BaseGraphTransformation):\n    \"\"\"\n    Abstract base class for building relationships in a KnowledgeGraph.\n\n    Methods\n    -------\n    transform(kg: KnowledgeGraph) -&gt; t.List[Relationship]\n        Transforms the KnowledgeGraph by building relationships.\n    \"\"\"\n\n    @abstractmethod\n    async def transform(self, kg: KnowledgeGraph) -&gt; t.List[Relationship]:\n        \"\"\"\n        Transforms the KnowledgeGraph by building relationships.\n\n        Parameters\n        ----------\n        kg : KnowledgeGraph\n            The knowledge graph to be transformed.\n\n        Returns\n        -------\n        t.List[Relationship]\n            A list of new relationships.\n        \"\"\"\n        pass\n\n    def generate_execution_plan(self, kg: KnowledgeGraph) -&gt; t.List[t.Coroutine]:\n        \"\"\"\n        Generates a list of coroutines to be executed in parallel by the Executor.\n\n        Parameters\n        ----------\n        kg : KnowledgeGraph\n            The knowledge graph to be transformed.\n\n        Returns\n        -------\n        t.List[t.Coroutine]\n            A list of coroutines to be executed in parallel.\n        \"\"\"\n\n        async def apply_build_relationships(\n            filtered_kg: KnowledgeGraph, original_kg: KnowledgeGraph\n        ):\n            relationships = await self.transform(filtered_kg)\n            original_kg.relationships.extend(relationships)\n\n        filtered_kg = self.filter(kg)\n        return [apply_build_relationships(filtered_kg=filtered_kg, original_kg=kg)]\n</code></pre>"},{"location":"references/testset/transforms/base/#ragas.testset.transforms.base.RelationshipBuilder.generate_execution_plan","title":"<code>generate_execution_plan(kg)</code>","text":"<p>Generates a list of coroutines to be executed in parallel by the Executor.</p> <p>Parameters:</p> Name Type Description Default <code>kg</code> <code>KnowledgeGraph</code> <p>The knowledge graph to be transformed.</p> required <p>Returns:</p> Type Description <code>List[Coroutine]</code> <p>A list of coroutines to be executed in parallel.</p> Source code in <code>src/ragas/testset/transforms/base.py</code> <pre><code>def generate_execution_plan(self, kg: KnowledgeGraph) -&gt; t.List[t.Coroutine]:\n    \"\"\"\n    Generates a list of coroutines to be executed in parallel by the Executor.\n\n    Parameters\n    ----------\n    kg : KnowledgeGraph\n        The knowledge graph to be transformed.\n\n    Returns\n    -------\n    t.List[t.Coroutine]\n        A list of coroutines to be executed in parallel.\n    \"\"\"\n\n    async def apply_build_relationships(\n        filtered_kg: KnowledgeGraph, original_kg: KnowledgeGraph\n    ):\n        relationships = await self.transform(filtered_kg)\n        original_kg.relationships.extend(relationships)\n\n    filtered_kg = self.filter(kg)\n    return [apply_build_relationships(filtered_kg=filtered_kg, original_kg=kg)]\n</code></pre>"},{"location":"references/testset/transforms/base/#ragas.testset.transforms.base.RelationshipBuilder.transform","title":"<code>transform(kg)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Transforms the KnowledgeGraph by building relationships.</p> <p>Parameters:</p> Name Type Description Default <code>kg</code> <code>KnowledgeGraph</code> <p>The knowledge graph to be transformed.</p> required <p>Returns:</p> Type Description <code>List[Relationship]</code> <p>A list of new relationships.</p> Source code in <code>src/ragas/testset/transforms/base.py</code> <pre><code>@abstractmethod\nasync def transform(self, kg: KnowledgeGraph) -&gt; t.List[Relationship]:\n    \"\"\"\n    Transforms the KnowledgeGraph by building relationships.\n\n    Parameters\n    ----------\n    kg : KnowledgeGraph\n        The knowledge graph to be transformed.\n\n    Returns\n    -------\n    t.List[Relationship]\n        A list of new relationships.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/testset/transforms/base/#ragas.testset.transforms.base.Splitter","title":"<code>Splitter</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseGraphTransformation</code></p> <p>Abstract base class for splitters that transform a KnowledgeGraph by splitting its nodes into smaller chunks.</p> <p>Methods:</p> Name Description <code>transform</code> <p>Transforms the KnowledgeGraph by splitting its nodes into smaller chunks.</p> <code>split</code> <p>Abstract method to split a node into smaller chunks.</p> Source code in <code>src/ragas/testset/transforms/base.py</code> <pre><code>class Splitter(BaseGraphTransformation):\n    \"\"\"\n    Abstract base class for splitters that transform a KnowledgeGraph by splitting\n    its nodes into smaller chunks.\n\n    Methods\n    -------\n    transform(kg: KnowledgeGraph) -&gt; t.Tuple[t.List[Node], t.List[Relationship]]\n        Transforms the KnowledgeGraph by splitting its nodes into smaller chunks.\n\n    split(node: Node) -&gt; t.Tuple[t.List[Node], t.List[Relationship]]\n        Abstract method to split a node into smaller chunks.\n    \"\"\"\n\n    async def transform(\n        self, kg: KnowledgeGraph\n    ) -&gt; t.Tuple[t.List[Node], t.List[Relationship]]:\n        \"\"\"\n        Transforms the KnowledgeGraph by splitting its nodes into smaller chunks.\n\n        Parameters\n        ----------\n        kg : KnowledgeGraph\n            The knowledge graph to be transformed.\n\n        Returns\n        -------\n        t.Tuple[t.List[Node], t.List[Relationship]]\n            A tuple containing a list of new nodes and a list of new relationships.\n        \"\"\"\n        filtered = self.filter(kg)\n\n        all_nodes = []\n        all_relationships = []\n        for node in filtered.nodes:\n            nodes, relationships = await self.split(node)\n            all_nodes.extend(nodes)\n            all_relationships.extend(relationships)\n\n        return all_nodes, all_relationships\n\n    @abstractmethod\n    async def split(self, node: Node) -&gt; t.Tuple[t.List[Node], t.List[Relationship]]:\n        \"\"\"\n        Abstract method to split a node into smaller chunks.\n\n        Parameters\n        ----------\n        node : Node\n            The node to be split.\n\n        Returns\n        -------\n        t.Tuple[t.List[Node], t.List[Relationship]]\n            A tuple containing a list of new nodes and a list of new relationships.\n        \"\"\"\n        pass\n\n    def generate_execution_plan(self, kg: KnowledgeGraph) -&gt; t.List[t.Coroutine]:\n        \"\"\"\n        Generates a list of coroutines to be executed in parallel by the Executor.\n\n        Parameters\n        ----------\n        kg : KnowledgeGraph\n            The knowledge graph to be transformed.\n\n        Returns\n        -------\n        t.List[t.Coroutine]\n            A list of coroutines to be executed in parallel.\n        \"\"\"\n\n        async def apply_split(node: Node):\n            nodes, relationships = await self.split(node)\n            kg.nodes.extend(nodes)\n            kg.relationships.extend(relationships)\n\n        filtered = self.filter(kg)\n        return [apply_split(node) for node in filtered.nodes]\n</code></pre>"},{"location":"references/testset/transforms/base/#ragas.testset.transforms.base.Splitter.generate_execution_plan","title":"<code>generate_execution_plan(kg)</code>","text":"<p>Generates a list of coroutines to be executed in parallel by the Executor.</p> <p>Parameters:</p> Name Type Description Default <code>kg</code> <code>KnowledgeGraph</code> <p>The knowledge graph to be transformed.</p> required <p>Returns:</p> Type Description <code>List[Coroutine]</code> <p>A list of coroutines to be executed in parallel.</p> Source code in <code>src/ragas/testset/transforms/base.py</code> <pre><code>def generate_execution_plan(self, kg: KnowledgeGraph) -&gt; t.List[t.Coroutine]:\n    \"\"\"\n    Generates a list of coroutines to be executed in parallel by the Executor.\n\n    Parameters\n    ----------\n    kg : KnowledgeGraph\n        The knowledge graph to be transformed.\n\n    Returns\n    -------\n    t.List[t.Coroutine]\n        A list of coroutines to be executed in parallel.\n    \"\"\"\n\n    async def apply_split(node: Node):\n        nodes, relationships = await self.split(node)\n        kg.nodes.extend(nodes)\n        kg.relationships.extend(relationships)\n\n    filtered = self.filter(kg)\n    return [apply_split(node) for node in filtered.nodes]\n</code></pre>"},{"location":"references/testset/transforms/base/#ragas.testset.transforms.base.Splitter.split","title":"<code>split(node)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Abstract method to split a node into smaller chunks.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The node to be split.</p> required <p>Returns:</p> Type Description <code>Tuple[List[Node], List[Relationship]]</code> <p>A tuple containing a list of new nodes and a list of new relationships.</p> Source code in <code>src/ragas/testset/transforms/base.py</code> <pre><code>@abstractmethod\nasync def split(self, node: Node) -&gt; t.Tuple[t.List[Node], t.List[Relationship]]:\n    \"\"\"\n    Abstract method to split a node into smaller chunks.\n\n    Parameters\n    ----------\n    node : Node\n        The node to be split.\n\n    Returns\n    -------\n    t.Tuple[t.List[Node], t.List[Relationship]]\n        A tuple containing a list of new nodes and a list of new relationships.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/testset/transforms/base/#ragas.testset.transforms.base.Splitter.transform","title":"<code>transform(kg)</code>  <code>async</code>","text":"<p>Transforms the KnowledgeGraph by splitting its nodes into smaller chunks.</p> <p>Parameters:</p> Name Type Description Default <code>kg</code> <code>KnowledgeGraph</code> <p>The knowledge graph to be transformed.</p> required <p>Returns:</p> Type Description <code>Tuple[List[Node], List[Relationship]]</code> <p>A tuple containing a list of new nodes and a list of new relationships.</p> Source code in <code>src/ragas/testset/transforms/base.py</code> <pre><code>async def transform(\n    self, kg: KnowledgeGraph\n) -&gt; t.Tuple[t.List[Node], t.List[Relationship]]:\n    \"\"\"\n    Transforms the KnowledgeGraph by splitting its nodes into smaller chunks.\n\n    Parameters\n    ----------\n    kg : KnowledgeGraph\n        The knowledge graph to be transformed.\n\n    Returns\n    -------\n    t.Tuple[t.List[Node], t.List[Relationship]]\n        A tuple containing a list of new nodes and a list of new relationships.\n    \"\"\"\n    filtered = self.filter(kg)\n\n    all_nodes = []\n    all_relationships = []\n    for node in filtered.nodes:\n        nodes, relationships = await self.split(node)\n        all_nodes.extend(nodes)\n        all_relationships.extend(relationships)\n\n    return all_nodes, all_relationships\n</code></pre>"},{"location":"references/testset/transforms/engine/","title":"engine","text":""},{"location":"references/testset/transforms/engine/#ragas.testset.transforms.engine.apply_transforms","title":"<code>apply_transforms(kg, transforms, run_config=RunConfig())</code>","text":"<p>Apply a list of transformations to a knowledge graph in place.</p> Source code in <code>src/ragas/testset/transforms/engine.py</code> <pre><code>def apply_transforms(\n    kg: KnowledgeGraph,\n    transforms: Transforms,\n    run_config: RunConfig = RunConfig(),\n):\n    \"\"\"\n    Apply a list of transformations to a knowledge graph in place.\n    \"\"\"\n    # apply nest_asyncio to fix the event loop issue in jupyter\n    apply_nest_asyncio()\n\n    # if single transformation, wrap it in a list\n    if isinstance(transforms, BaseGraphTransformation):\n        transforms = [transforms]\n\n    # apply the transformations\n    # if Sequences, apply each transformation sequentially\n    if isinstance(transforms, t.List):\n        for transform in transforms:\n            asyncio.run(\n                run_coroutines(\n                    transform.generate_execution_plan(kg),\n                    get_desc(transform),\n                    run_config.max_workers,\n                )\n            )\n    # if Parallel, collect inside it and run it all\n    elif isinstance(transforms, Parallel):\n        asyncio.run(\n            run_coroutines(\n                transforms.generate_execution_plan(kg),\n                get_desc(transforms),\n                run_config.max_workers,\n            )\n        )\n    else:\n        raise ValueError(\n            f\"Invalid transforms type: {type(transforms)}. Expects a list of BaseGraphTransformations or a Parallel instance.\"\n        )\n</code></pre>"},{"location":"references/testset/transforms/engine/#ragas.testset.transforms.engine.run_coroutines","title":"<code>run_coroutines(coroutines, desc, max_workers)</code>  <code>async</code>","text":"<p>Run a list of coroutines in parallel.</p> Source code in <code>src/ragas/testset/transforms/engine.py</code> <pre><code>async def run_coroutines(coroutines: t.List[t.Coroutine], desc: str, max_workers: int):\n    \"\"\"\n    Run a list of coroutines in parallel.\n    \"\"\"\n    for future in tqdm(\n        await as_completed(coroutines, max_workers=max_workers),\n        desc=desc,\n        total=len(coroutines),\n        # whether you want to keep the progress bar after completion\n        leave=False,\n    ):\n        try:\n            await future\n        except Exception as e:\n            logger.error(f\"unable to apply transformation: {e}\")\n</code></pre>"},{"location":"references/testset/transforms/extractors/__init__/","title":"extractors","text":""},{"location":"references/testset/transforms/extractors/embeddings/","title":"embeddings","text":""},{"location":"references/testset/transforms/extractors/llm_based/","title":"llm_based","text":""},{"location":"references/testset/transforms/extractors/regex_based/","title":"regex_based","text":""},{"location":"references/testset/transforms/relationship_builders/__init__/","title":"relationship_builders","text":""},{"location":"references/testset/transforms/relationship_builders/cosine/","title":"cosine","text":""},{"location":"references/testset/transforms/relationship_builders/cosine/#ragas.testset.transforms.relationship_builders.cosine.SummaryCosineSimilarityBuilder","title":"<code>SummaryCosineSimilarityBuilder</code>  <code>dataclass</code>","text":"<p>               Bases: <code>CosineSimilarityBuilder</code></p> Source code in <code>src/ragas/testset/transforms/relationship_builders/cosine.py</code> <pre><code>@dataclass\nclass SummaryCosineSimilarityBuilder(CosineSimilarityBuilder):\n    property_name: str = \"summary_embedding\"\n    new_property_name: str = \"summary_cosine_similarity\"\n    threshold: float = 0.1\n\n    def filter(self, kg: KnowledgeGraph) -&gt; KnowledgeGraph:\n        \"\"\"\n        Filters the knowledge graph to only include nodes with a summary embedding.\n        \"\"\"\n        nodes = []\n        for node in kg.nodes:\n            if node.type == NodeType.DOCUMENT:\n                emb = node.get_property(self.property_name)\n                if emb is None:\n                    raise ValueError(f\"Node {node.id} has no {self.property_name}\")\n                nodes.append(node)\n        return KnowledgeGraph(nodes=nodes)\n\n    async def transform(self, kg: KnowledgeGraph) -&gt; t.List[Relationship]:\n        embeddings = [\n            node.get_property(self.property_name)\n            for node in kg.nodes\n            if node.get_property(self.property_name) is not None\n        ]\n        if not embeddings:\n            raise ValueError(f\"No nodes have a valid {self.property_name}\")\n        similar_pairs = self._find_similar_embedding_pairs(\n            np.array(embeddings), self.threshold\n        )\n        return [\n            Relationship(\n                source=kg.nodes[i],\n                target=kg.nodes[j],\n                type=\"summary_cosine_similarity\",\n                properties={self.new_property_name: similarity_float},\n                bidirectional=True,\n            )\n            for i, j, similarity_float in similar_pairs\n        ]\n</code></pre>"},{"location":"references/testset/transforms/relationship_builders/cosine/#ragas.testset.transforms.relationship_builders.cosine.SummaryCosineSimilarityBuilder.filter","title":"<code>filter(kg)</code>","text":"<p>Filters the knowledge graph to only include nodes with a summary embedding.</p> Source code in <code>src/ragas/testset/transforms/relationship_builders/cosine.py</code> <pre><code>def filter(self, kg: KnowledgeGraph) -&gt; KnowledgeGraph:\n    \"\"\"\n    Filters the knowledge graph to only include nodes with a summary embedding.\n    \"\"\"\n    nodes = []\n    for node in kg.nodes:\n        if node.type == NodeType.DOCUMENT:\n            emb = node.get_property(self.property_name)\n            if emb is None:\n                raise ValueError(f\"Node {node.id} has no {self.property_name}\")\n            nodes.append(node)\n    return KnowledgeGraph(nodes=nodes)\n</code></pre>"},{"location":"references/testset/transforms/splitters/__init__/","title":"splitters","text":""},{"location":"references/testset/transforms/splitters/headline/","title":"headline","text":""}]}